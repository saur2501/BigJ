CS - Postman New Update
    New Features Galore!
        Workspace overview: Get a quick overview of all the activity happening inside a workspace. Manage your workspace by adding helpful description, modifying visibility and inviting team members to collaborate.

        Collection Runner: Collection runner now opens in a tab instead of a new window, helping you switch seamlessly between workflows. Find the new runner on the bottom right corner.

        Monitors: View and work with all of your monitors without having to leave your application. You can now view run details, activity logs, and watch your monitor runs start, execute, and finish in real-time.

        Mock servers: View and edit mock servers in a tab.

        Trash: Accidentally deleted a collection? Fret not. You can recover collections from Trash within the app. Find trash on the bottom right corner.

        Forking and Pull requests: Contributing to collections has never been easier. You can fork a collection from any workspace to create a copy for your reference or development. Moreover, you can request reviews using pull requests from the app.

        Merge checks: Enforce rules on how collections should be updated by setting merge conditions. Click on “Manage roles” in the collection dropdown to edit these. This feature is available on the Business and Enterprise tiers.

        Supercharged profiles: Enrich and customize your user profile and team profile with profile and cover pictures, descriptions and links to your presence on the internet.

        Private API Network: The Private API Network is now available right inside the postman app in the Home section.

        Inline comments in API schema: You can now collaborate and review API schemas more effectively by leaving line-by-line comments.

        Documentation: Postman’s documentation is now closely integrated with requests and collections. You can now easily edit documentation while defining requests and collections and also refer to the documentation while sending a request. Complete documentation view is also available within the app for personal, team and public workspaces.

        Scratch Pad: This release will let you use a space called Scratch Pad, where you can work while not being connected to Postman Servers. All your work will be stored locally and will not be synced.

        API watching: You can watch APIs in any workspace and be alerted about changes made to their specification.
    Postman v8.0
        Postman v8 gives you a consistent, better API development experience. It has universal search across all of Postman, including the Public and Private API Networks, plus you can work on public workspaces directly through the desktop app. This means that the Postman desktop app and Postman on the web are now equally empowering—which one you use is a matter of preference for your workflows.

        Here’s what’s new in this version -

        1. Universal navigation
        1.1 Home
        Introducing the all new Home section in the top navigation. Get an overview of everything happening in your team - activity by team members, new alerts, notifications and a lot more. From here, you can quickly navigate to workspaces, API network, integrations or Postman learning center - wherever you want to go, home is the place to start.



        1.2 Workspaces
        We have a brand new workspace selector which lets you quickly switch between workspaces which were used recently. You can now search through different types of workspaces without having to switch tabs. Just type the name of the workspace you want to go to.

        And oh! We now have public workspaces These are just like any other workspace but visible to everyone on the internet and easily accessible via Postman API network. Create a new public workspace and share your APIs with millions of Postman users around the world.

        1.3 Reports
        Available on the Enterprise and Business tiers, reports help you understand how your organization is building and consuming APIs. Get insights about requests, collection runs and test failures. Administrators can also use reports to analyze how Postman is being used across the organization.

        1.4 Explore
        One stop solution to explore all APIs published in the Postman API network. Browse through different teams and their public workspaces to discover new APIs. Read on to know more about this.

        1.5 Universal Search
        Universal search makes it incredibly easy to search and discover any private or public API element. It looks through all API elements in your account, the private API network, and the Public API Network to help you get started in a few clicks.



        2. A unified interface for web and native
        2.1 Panes
        The main working area is now divided into Panes, which help you customize the Postman interface to match your preferred work environment. You can expand and adjust relevant panes so that the right information is accessible readily.



        2.2 Updated sidebar
        Postman v8 lets you access collections, environments, mock servers, monitors and request history right from the sidebar. You can easily author, search and modify any of these elements without leaving the app. The whole sidebar acts like a pane which can be resized and also collapsed based on your liking.

        2.3 Context bars
        Context bars provide information relevant to the element you’re viewing in a workspace. Here, you can access documentation for an element, see pull requests, add comments or just copy the ID for CI/CD integration.



        3. All the world’s APIs are just a click away
        Postman v8 not only simplifies your API development experience but also makes exploring APIs a piece of cake. Browse through hundreds of third party APIs in the “Explore” section. Here, you can look at collections published by different teams and navigate through them just like you do in your own workspace.

        Interested in an API? Want to send a request? It is just a few clicks away. Just fork the collection into your workspace and you’re good to go.

        But wait! Even you can be a part of the Postman API network. Simply create a public workspace and share your collections in it - they are automatically available to millions of Postman users.

        Like to contribute to someone else’s API? That’s easy too! Fork the collection into your workspace, make the changes and raise a pull request.
Wiki page
    State of the art - specifications and arch refs.
        can give UML if necessary.
    Requirement 
        Epic Discussion - Customer needs
        Concrete SRS - How we intend to meet the needs. Endpoints descriptions, STD - State Transition Diagrams.
    Architectural 
        Alternatives - discussions with pros and cons.
        Finalized one and why
        Details that follow - tables needed, etc.
        Additional resources needed - new endpoints to be created.
JIRA
    spaces
        Pages - Page Groups. 
        Page hierarchy - every page created with a parent - source or context of creation.
        Page - operations.
The application Confluence would like to have read and write access to your data on sapjira.wdf.sap.corp. The application will be allowed to use your credentials to authenticate (Devgun, Saurabh) as you in the future.
LIB - IQ - In worst case, the presentation may unorganized - which is not the case on average but there is bound to be many useful statements in a session or presentation.
    it's a valid point - let me think aobut it and get back to you for further discussions.
    first cut, backdoor entry, 
    these calls are self explanatory.
    effort, feedbacks -> good job, work, nice presentation.
Checked it conclusively that spark apis need to redone from the start - from middle they will always show wrong picture.
    if you have made any difference in the code redo all the apis.
Intellij stuffs
Scrum
    I was going thru the code yesterday to identify where to make changes. Have written few doubts.
    Hari - DoD - finished and shared security report. Deployment descriptor files updated with sth related with jobs in spaces.
    Sweta, Prabhakar - Space X tenant X App X calls - which calls failing. New script - add to nightly build. Raised ticket with Ops, TM.
        JMeter Script CRUD. Scope AWS, Azure. GDPR.
        pull in functionality into existing scripts. And new scripts.
Schema
    Tags - equipmentId, indicatorGroupId, modelId, templateId.
Scrum - I am blocked by this BCP and its repercussions - present things as they are.
Scrum
    code smells, UT, merged PR, today e2e test; Master POM merged - e1 except csw, abstract and migration.
    Derivation sonar quality - addressed to ops - PR can be merged with that.
    valid jobs running in EU10 - scripts to check needed jobs are running in eu10 and canary spaces. With prabhakar.
        CF issues - SSO works fine. Login with thing modeler it fails. Multi factor authentication failing - else would check with Ops team.
    Abhishek Security KT - exploring tools. Handling internal errors implementation in derivation just started.
    Hari - jobs in different spaces - PR and merged. Remaining 2 tenant migration - this week security report preparation.
        SetSDL meeting so discussing with them - Srini connected with that team for security task - what meeting.
        First assessment - improvement plan. Requirement.
        US10 abstract model - login to cf failing - Pavan or Abhijeet sync. client I can help you.
Hello World
React is mastering presentation layer
Spring is mastering application layer.
Scrum updates - L0 and drill down to L1 DFD until more is required.
Scrum - Need devp access for staging space in order to get kafka keys from the service instance. If not, their performance user - needs dev access and only Severin can do it.
    He is offline since morning must be one leave.
    Spark skeleton is ready - need to add casssandra and redis to it
    Kafka I have added but not working - haven't got time to work on it.
Scrum updates
    Performance
        Markdown formatting
        Added more details to wiki
        Added file in the same place
        Can I close it
    Spark Connection Health Check
        Added env parts for hana
        For spark - went thru the code
        Will start in scalable manner.
Scrum
    Performance
        validated a successful run but couldn't check spark metrics - could not restart cuz permission asked thru BCP.
        Added wiki link to the story
        more ideas on eventual analytics with the digital twin of spark and extending to azure integration - discussed.
    Spark Connector
        Tried deploying a sample spring app and it failed.
        Was trying spring security but not necessary
        continue
    Security Training is remaining from blr team side.
        Export control about ECC stuffs.
    Re-ingestion part tested in sandbox - blocked in pipeline - not resolved - so calling it blocked in jira.
        will jmeter script shared with colleagues
    finished testing hybrid issue for thing model. Take to staging. Testing scenarios in there for both models.
        some time in tickets. Spark jobs coming and going - NGP created. Chisel directly into NS2.
        sth failing cuz not able to log to IOTS instance.
        Contact that guy.
    DoD yesterday - 4 BCP taken care, 1 for derivation, 1 for PdMS in stakeholder space; some consultation for other team.
    abstract model hook, modeling incorporating sth, ...
        chance to sync regarding master pom.
    Security expert handover SVM - check, audit and assess vulnerabilities b4 release.
        spark lib size limit.
        diff of maven dependency trees - marked as provided. Transitive dependencies are causing the problem.
        reduce the jar size thru that.
        Master pom - have pipeline dependency added. Check-in to master pom then dependents should get triggered automatically.
        trigger build from di-utils can be removed - all apps will trigger automatically from master pom - so we remove our masters from there.
        Security audit - avoid doing things in the midnight.
        Derivation task - could not find time to work on it. Discuss with Suraj what's pending for handover.
    VH BCP time taken - analysis goes like this - submit to jobsubmitter passing credentials to tenant. Some of the tenants
        404 on managed store. Object store creation - onboarding - object store not created for some of tenants - core store.
        404 continuously - goes on trying endlessly. Doesn't know that object store doesn't exist.
        discussed with Imran and couple of options. Roll out that object store creation is not instant so gotta accommodate that scenario.
        worklist in 404.
        Migration activity - setting mirror maker with cluster with help of Thomas team. Replication of topics happening.
            automatic provisioning happening. Right offset should be there.
        Evaluation for mirror maker or manual things.
    Rules runtime is part of foundation1 - putting it in required section of app - it failed.
        removed and added to correct section. 1 part of rule is always available in Noah space.
        working with abstract model - some cases failing. will get stuck with jar size as well.
    Hari - migration for v2 to v3. 1 tenant in progress. Security report prep for app - for 2 it's already done.
        pipeline status fortify if possible.
        PR approved and ops coordination for merging.
    Review comments for fix.
    Derivation script updating going on.
    Explored some tools yesterday.
    2 scripts failing - Sweta aligned - eth goes fine after configs - derivation script for data purge. Created my own app to test it.
        data ingestion monitoring. expose api for her today.
        master pom adoption. Script fails in test suite - in commit space works fine.
Review meeting
    Raise blockers in the jira.
    kafka topic unsubscribed and not used to be deleted.
        monitoring/kafka topic cleanup - /api/jobsubmitter/exclude
    Next sprint - reingestion capabilities and master pom adoption besides.
    Update on kafka migration.
        part of standard cluster migration.
        non-mirror maker base and with mirror.
They share and pass on the culture to next generation but not huge pass on from 1 gen to next.
{
    "id": "dcacbd50-4117-4140-925b-17bbff325a17",
    "name": "app1",
    "tenantId": "f02fb6cd-43db-4604-856b-c2dacbab6804",
    "type": "java",
    "lastModified": "2021-02-05T08:04:37.297Z",
    "parameters": {
        "mainClass": "org.First"
    },
    "config": {}
}

curl --location --request POST 'https://spark-service.cf.sap.hana.ondemand.com/v1/f02fb6cd-43db-4604-856b-c2dacbab6804/apps' --header 'SLUG: app1' --header 'Content-Type: application/json' --header 'Authorization: Basic c2Jzc18xMTNfb3R0MmVjY2FfLW42YzctZ3Nrc2s4bXM9OmFhX3RQVW9EVzNyYVhaTWRscXMzNjROLzZMdStROD0=' --data-raw '{
    "type": "java",
    "parameters": {
      "mainClass": "org.First"
    }
}'

curl -X POST -u "user:passwd" -H "SLUG: app1"
  -H "Content-Type: application/json"
  -d '{
    "type": "java",
    "parameters": {
      "mainClass": "org.First"
    }
  }' https://spark-service.cf.sap.hana.ondemand.com/v1/f02fb6cd-43db-4604-856b-c2dacbab6804/apps
curl --location --request DELETE 'https://spark-service.cf.sap.hana.ondemand.com/v1/f02fb6cd-43db-4604-856b-c2dacbab6804/apps/app1' --header 'SLUG: lib1' --header 'Content-Type: application/json' --header 'Authorization: Basic c2Jzc18xMTNfb3R0MmVjY2FfLW42YzctZ3Nrc2s4bXM9OmFhX3RQVW9EVzNyYVhaTWRscXMzNjROLzZMdStROD0=' --data-raw '{
    "type": "java",
    "parameters": {
      "mainClass": "org.First"
    }
}'
curl --location --request GET 'https://spark-service.cf.sap.hana.ondemand.com/v1/f02fb6cd-43db-4604-856b-c2dacbab6804/apps' --header 'SLUG: app1' --header 'Content-Type: application/json' --header 'Authorization: Basic c2Jzc18xMTNfb3R0MmVjY2FfLW42YzctZ3Nrc2s4bXM9OmFhX3RQVW9EVzNyYVhaTWRscXMzNjROLzZMdStROD0=' --data-raw '{
    "type": "java",
    "parameters": {
      "mainClass": "org.First"
    }
}' - all names like iot-gdpr-unified-dsc-app, iot-iot_sandbox-unified-app, der-deriv251-unified-app, coldstore-export-sap-app.
You can send binary data thru postman - https://stackoverflow.com/questions/50395010/spring-postman-content-type-application-octet-stream-not-supported
    form file, data binary content, binary string, etc.
{
    "id": "4b3791c1-9265-4262-8204-58dbadf0c3f6",
    "name": "lib1",
    "tenantId": "f02fb6cd-43db-4604-856b-c2dacbab6804",
    "appId": "f630d1c1-2b4c-4dd1-b5f8-50ef23eac01e",
    "externalId": "l~f02fb6cd-43db-4604-856b-c2dacbab6804~f630d1c1-2b4c-4dd1-b5f8-50ef23eac01e~lib1",
    "version": "b301005eb7ebe9418378585be10cfa28b5600608",
    "contentLength": 3115,
    "lastModified": "2021-02-05T08:31:00.882Z"
}
curl --location --request POST 'https://spark-service.cf.sap.hana.ondemand.com/v1/f02fb6cd-43db-4604-856b-c2dacbab6804/contexts' --header 'Content-Type: application/json' --header 'SLUG: context1' --header 'Authorization: Basic c2Jzc18xMTNfb3R0MmVjY2FfLW42YzctZ3Nrc2s4bXM9OmFhX3RQVW9EVzNyYVhaTWRscXMzNjROLzZMdStROD0=' --data-raw '{
    "parameters": {
      "context-factory": "spark.jobserver.context.StreamingContextFactory"
    }
}'
{
    "id": "87e92e1a-262a-474d-b34c-cfb16f4e2f39",
    "name": "context1",
    "tenantId": "f02fb6cd-43db-4604-856b-c2dacbab6804",
    "externalId": "c~f02fb6cd-43db-4604-856b-c2dacbab6804~87e92e1a-262a-474d-b34c-cfb16f4e2f39",
    "status": "RUNNING",
    "cores": 2,
    "memory": "512mb",
    "lastModified": "2021-02-05T08:37:52.324Z",
    "parameters": {
        "context-factory": "spark.jobserver.context.StreamingContextFactory",
        "spark.cores.max": 2,
        "spark.executor.memory": "512mb",
        "launcher.spark.driver.memory": "1g"
    }
}
spark.jobserver.context.SessionContextFactory
{
    "id": "[400-SUBMITS-UNCLEX]",
    "description": "Unexpected Classpath Exception",
    "message": "The specified main class (org.App) is not found on the classpath of context context1",
    "timestamp": "2021-02-05T08:45:57.482Z",
    "requestId": "a92dc65c-169b-4ab5-7ae9-d1c871b4878b"
}
I changed file name but not class name of scala and hence it was not able to detect the main class path - cuz file with old class name didn't even exist.
{
    "id": "[400-SUBMITS-SULOFA]",
    "description": "Submit Loading Failed",
    "message": "Necessary class or method is not provided for the app app1. Inspect the Spark cluster log files.",
    "timestamp": "2021-02-05T08:52:10.048Z",
    "requestId": "ca830f48-8b93-4339-6d19-96e5cd8e446f"
}
need to implement runJob Method and extend a class.
appName not a valid name.
gotta extend SparkJob
    add pom dependencies of sparkJob-api and its extension.
    gotta override the methods.
https://github.com/spark-jobserver/spark-jobserver
Cloud Foundary access from UI - Basically SCP SPC - link given by Suraj but it's SCP only.
