<?xml version="1.0" encoding="UTF-8"?>
<Analytics> aka Data Science
    <Intro>
        Love for data. Meditation on data- staring at data.
        IT has grown beyond automation to Data Driven Softwares.
            80s ppl would automate paper based system
                apply for leaves thru email instead of paper.
                but decision making was still based on offline data.
                Eg- Enterprise architecture- all data of enterprise sits with the system.
                    7 asks you what enterprise are you working on? What protocol stack you work?
                    Zachman architecture can explain these questions.
            Data driven Decision making- when business processes use data present with the system (not offline)
                Eg - in software, business logic is not hard coded but uses information from data to help you decide.
                Eg - in business world, the company decisions should be corroborated with [past ]data.
        REL Updates in inbox ed morning from OLAP vs OLTPs.
            like using dashboards (Sparsh in Infy)
    </Intro>
    <ProducingData>
        About
            Analytics performed on transactional systems (useful insights on the fly)
            Producing Data- All software development 
            Operational Environment - Business Strategy (Output from OLAP --Processes--> Master Data Transactions (Input for OLAP))
        <AnalyticsInOLTP>
            Using advanced SQL
            Made for OLTP and not enough operators for analysis.
            Determine the navigational access on UML - the series of edge that need to be joined.
            <Eg>
                Retail POS (point of sale) Database
                    Customer- id
                        (1:n) Bill- TotalBill, Date
                            contains BillItem- Qty, TotalPrice
                                1:1 Product- unitPrice
                                    m:n Category- cId
                            Store- id
                    Q1- Compare sales of cosmetics across stores
                        storeName       SalesValue

                        Determine the navigational access on UML.
                        Soln - Join bill, billItem, Store and roll up along all but store with summation over total price.
                    Q2- Puja season- which items are sold the most
                        Solution
                            filter on billdates
                            join bill, billitem, product
                            group by product
                            aggregation- count
                        Problem if date was not captured as part of transactions- I don't have data to answer is (insufficient data)
                    Q3- how many non-local at every store
                        find tables involved.
                        These are not easy to answer- eCity, vs electronic city vs electronics city.
                            There is a need felt for clean data
                    <LMSPost>
                        How does the sale of cosmetics compare across the stores?
                        Ans : SELECT store_id, sum(bi.tot_price) AS sales_value
                        FROM store s, bill b, bill_item bi, product p, category c
                        WHERE s.store_id=b.store_id AND b.bill_id=bi.bill_id AND p.product_id=bi.product_id AND p.c_id=c.c_id
                        AND c.category=’cosmetics’
                        GROUP BY(s.store_id)
                        ORDER BY(s.store_id);

                        Q2: What products get sold most during puja season?
                        Ans: SELECT p.product_id, p.pname, MAX(prod_count)
                        FROM (SELECT p.product_id, p.pname, COUNT(bi.product_id) as prod_count
                         FROM bill b, bill_item bi, product p
                        WHERE b.bill_id=bi.bill_id AND p.product_id=bi.product_id
                        AND date BETWEEN ‘2016-10-01’ AND ‘2016-10-31’
                        GROUP BY(bi.product_id));

                        Q3: How many “non-local” customers come to each store? What do they buy?
                        Ans:
                        (a) SELECT count(c.cust_id), s.store_id
                        FROM customer c, store s, bill b
                        WHERE c.cust_id=b.cust_id AND b.store_id=s.store_id
                        AND c.location!=s.location
                        GROUP BY(s.store_id)
                        ORDER BY(s.store_id);

                        (b) SELECT p.product_id, p.pname
                        FROM  customer c, store s, bill b, bill_item bi, product p
                        WHERE c.cust_id=b.cust_id AND s.store_id=b.store_id AND b.bill_id=bi.bill_id AND p.product_id=bi.product_id
                        AND c.location!=s.location;
                    </LMSPost>
            </Eg>
            <Difficulties> in mgmt, in devp func, in devp non-func (speed)
                Difficult to write (complex queries) - cuz the DB design is not tuned for analysis.
                Hard to maintain from SE point of view - gap between SE Development team and Business People
                    hardcoded - new query (not just data-wise) then new effort
                    Less Flexible- Query can't modify.
                Inefficient - 5 joins in Q1 (very heavy)
            </Difficulties>
            <TopN> Strategy - given a row - find how many are greater or equal to its value - that's rank.
                select t1.x Value, count(*) Rank 
                from T t1, T t2
                where t2.x <= t1.x
                group by t1.x
                Having Rank < 3;

                Could have used orderby and limit- but limit is not standard across vendors.
                Simple old quesion - 2nd highest - find max from rows which are lesser than the highest value.
            </TopN>
            <OuterJoins>
                OLAP uses them widely over inner join (OLTP uses inner)
                Left outer join- all rows on left are bound to be there- no match returns NULL there.
                Full outer join- adds additional rows (unmatched ones) from both sides.
            </OuterJoins>
            <ConditionalSummation> Strategy - grouping lenient but counting conditional thru summation of those that qualify
                count(A) vs count(*)- nulls are ignored in former.
                Display the number of employees earning less than 50000 in each department
                    select dno, count(*) cnt
                    from employee
                    where salary < 50000
                    group by dno;
                vs
                    select dno, 
                        sum(case when salary < 50000 then 1 else 0 end) cnt from employee 
                    group by dno;
            </ConditionalSummation>
            <CrossTabs> Strategy - grouping lenient but conditional summation on columns (making 2 columns from 1)
                aka Pivots
                Display the salary of employees grouped by department and gender
                    select dno, sex, sum(salary)
                    from employee 
                    group by dno, sex;
                vs
                    select dno, 
                    sum(case when sex='M' then salary else 0 end) Male,       
                    sum(case when sex='F' then salary else 0 end) Female from employee group by dno
            </CrossTabs>
            <Histograms> transform every value to a bin value, group by bins value (count size along)
                Construct bin table containing bins for all
                count #bins of each category
                Eg- 
                    select 20000+bin*10000 'range', 
                            count(bin) 'frequency' 
                     from (
                            select floor((salary-20000)/10000) bin 
                            from employee
                           ) bintable 
                     group by bin;
            </Histograms>
            <SkylineQuery> Strategy - given a student, find students who bested him in every value of interest. If not found, Report him.
                Defn - the ones which were not bested in all values that matter.
                    REL throw a cloth from top- note the points that stand out.
                        ?this is probably not the best way to relate. cuz many point may not stand out still that qualify below the "slope" but above the "rectangle"
                        slope of 2 best dots and rectange formed by axes and dots.
                    REL another visualization from best on y-axis to best on x-axis as we connect the dots for skyline data objects - 
                        as x-axis value increases the y-axis value must decrease and vv.
                    Eg - students who didn't have someone better in all the values of interest.
                    ?Insight - finding the best student when you don't know the weightage to be given to the values.
                        this should not be used for 2nd best 
                        cuz s person right below a skyline dot didn't make to 1st cuz of him 
                        but is no way lesser than skyline dot 2 if in 1 value he exceeds him.
                ignore all points to the left (on x-axis) of highest spike.
                    rollno	GATE	CGPA 
                    r01	500	3.3
                    r02	520	3.4
                    r03	490	3.5
                    r04	550	3.5
                    r05	560	3.6
                    r06	570	3.3
                    r07	540	3.64
                    r08	510	3.65
                Eg- a person from set who had no1 exceeding both Gate and cgpa.
                    select * 
                    from gatecgpa g 
                    where not exists 
                          (select * from gatecgpa g2 
                           where  
                           g2.GATE >= g.GATE and g2.CGPA > g.CGPA 
                           or 
                           g2.GATE > g.GATE and g2.CGPA >= g.CGPA);
            </SkylineQuery>
            <RelationalDivision>
                SQL doesn't have an operator for universal quantifier (ForAll)
                Division word comes frm fact that R = Applicants X Skills
                    then Applicants = R / Skills.
                Idea- going by count (using FK to make sure that javascript doesn't replace SQL)
                Eg - studs who have the all of the following skills. 
                Count Strategy - just checks for the number of skills not viz all skills. Works iff exhaustive!
                    select name 
                    from appskills 
                    group by name 
                    having count(skill) = (select count(*) from skills);
                Eg - Students who enrolled in all the courses under CS.
                    QUEL Strategy - ForAll
                        Purport- for student x, for every course, there was a corresponding entry (student x's) in enrollment table.
                        SELECT DISTINCT s.name
                        FROM student s
                        WHERE FORALL
                             (SELECT * 
                             FROM requirement r
                             WHERE r.major='CS'
                             AND EXISTS
                                  (SELECT *
                                  FROM enrollment e
                                  WHERE e.student_id=s.student_id
                                  AND e.course_id=r.course_id));
                    SQL Strategy (not exists)- replace FORALL and EXISTS with not exists each
                        Purport- for student x, we could not find a course for which there was no [course] entry in enrollment (for same student)
                        SELECT DISTINCT s.name
                        FROM student s
                        WHERE NOT EXISTS
                             (SELECT * 
                             FROM requirement r
                             WHERE r.major='CS'
                             AND NOT EXISTS
                                  (SELECT *
                                  FROM enrollment e
                                  WHERE e.student_id=s.student_id
                                  AND e.course_id=r.course_id));
            </RelationalDivision>
        </AnalyticsInOLTP>
    </ProducingData>
    <ConsumingData>
        About
            Consuming Data - Data Science - Data Mining, Analytics, Decision Making
            Churn information from available data.
            Informational Environment - Master Data Transactions (output from OLTP) --Business Data Warehouse+Analytics--> Business Strategy for OLTP (input for OLTPs)
            No control on what's thrown at us (so make sure we first understand it)
        <BuzzWords>
            or jargons
            <DataMining>
                Misnomer- information mining
                    REL In Gold mining- what do you excavate- get as net outcome.
                Generate
                    Implicit
                    Previously Unknown
                    Potentially Useful Information
                    Eg- Implicit but known information- Age from Date of Birth field in RDBMS.
            </DataMining>
            <MLvsDM>
                Machine Learning relates with the study, design and development of the algorithms that give computers the capability to learn without being explicitly programmed
                    ?reinforcement learning that belongs to ML, but not to data mining (DM) - it is arguable.
                    Machine Learning (ML) techniques are fairly generic and can be applied in various settings.
                Data Mining can be defined as the process that starting from apparently unstructured data tries to extract knowledge and/or unknown interesting patterns
                    During this process machine Learning algorithms are used - formulate in terms of what ML algo expects (abstract out the problem into features)
                    Data Mining (DM) has emphasis on utilizing data from a domain e.g., social media, sensor data, video streams, etc., to understand some questions in that domain
                    applications of statistical methods to derive new insights
            </MLvsDM>
            <OLAPvsDM>
                OLAP is a design paradigm to seek information from physical data store
                    aggregates information from multiple systems and stored in n-dimensional form
                    summarizes data.
                    Eg - “What are the average sales of cars, by region and by year?"
                    to improve its operational efficiency
                    used by the regular front and back office employees - for reporting and small time analysis.
                DM leverage information within and without the organization to aid in answering business questions
                    discovers hidden patterns in data and operates at a detailed level instead of a summary level.
                    Eg - Who is likely to shift service providers and what are the reasons for that?
                    future perspective on things
                    used by business strategists - as per info supplied.
                Complement
                    while OLAP pinpoints problems with the sales of a product in a certain region, data mining could be used to gain insight about the behavior of the individual customers.
                    data mining predicts something like a 5% increase in sales, OLAP could be used to track the net income

            </OLAPvsDM>
            <KDD>
            KDD- Knowledge Discovery in Databases
                More than knowledge discovery from data mining
                Data mining is a stage of KDD (how to ETL)
                KDD = ETL + DataMining + Report
            vs OLAP- Online Analytical Processing
                Sounds synonymous to KDD- but replaces Data Mining with any analysis to answer a query.
                Offline- Data CD given for analysis- disconnect to data source- no refresh, doesn't reflect changes
                Online- Batch at intervals/ real time.
                <DatawareHouse>
                    Transforming OLTP to OLAP

                </DatawareHouse>
                <DataAnalysis>
                    Data Mining + KDD + Statistics
                    <Qualitative>
                    Qualitative- from field in form of narrative- unstructured.
                        Eg- impact of IT on society.
                        Less emanable to machine processing.
                    </Qualitative>
                    <Quantitative>
                            lots of numerics.
                            CRISP DM framework : Data Analytics :: SDLC : SE.
                            Variant- Big Data Analytics
                                Vs to manage process- Volume, Velocity, Variety, Veracity.
                                Social Media is many a times equated to Data Analytics.
                            Statistics- Law of large numbers/ Central Limit Theorem- justifying that sample means (parameters) = population variables.
                                BDA questions it- why? when I can find for population- why restrict to samples (inherent loss of information)
                            <Domains>
                                Retail Analytics
                                Market Basket Analytics- Like peeping into 7's basket- what they are buying.
                                    Market Segmentation- Targetted marketting (categorize customers)
                                Customer Relation Management (CRM)
                                    Customer Churn- How frequently customer choose any competitors brands
                                RFV Analytics- Recently Frequently Value
                                Web Analytics
                                    Sentiment Analytics- Navigational behavior (deals with [semi/un] structed data)
                                    Text analytics
                                HR
                                    Employee Churn- Who is likely to stay with company and who'll leave.
                                Stock Market, Biology
                            </Domains>
                            <Types> auth : iiitb
                                These terms are given by vendors to pinpoint their offerings
                                Also describe maturity of analytics
                                Descriptive Analytics
                                    Mechanism to summarize data
                                    Already Known -> better know the data (non visual side)
                                    Done thru BI (visual side)- tableau, Grammar; OLAP
                                    This is how traditionally DWH did.
                                    Eg- mean or average sales of my stock
                                Exploratory Analytics
                                    Exploring what is not known
                                    hypothesization thru data.
                                    This is Data Mining (DM)
                                    Eg- Clustering.
                                Confirmatory Analytics
                                    Hypothesis Testing (using statistical support)
                                Predictive Analytics - analyzes past events or instances in a right sequence for predicting a future event (guru99)
                                    Use ML for DA.
                                    Forecasting the unknown (not hypothesizing)- from known 
                                    Very famous- that equated with DA.
                                    Output is a prediction- "this" will happen
                                    We will look at 
                                        classification for prediction
                                        association Rules
                                        Regression (and variants)
                                Prescriptive Analytics
                                    coined by IBM.
                                    give action plan- action to take
                                    gives actionable insights
                                        better give more discount on PC than shoes.
                                        what to do in face of sth
                                        Should there be a weekday sale?
                                    we may use predictive for prescriptive.
                                    Eg- Credit Card- If increase interest rate to 15% then its impact. Mr X will default. So, better keep it at 10% for best results.
                                    vs predictive we get outcome but evaluation and action plan is your onus.
                                    Eg- HR- whether to hire- what salary to offer.
                                Maturity of Analytics
                                    Descriptive -> Predictive -> Prescriptive       
                            </Types>
                            <BusinessIntelligence>
                            BI- Business Intelligence
                                Reports prepared thru COBOL (old days)
                                    Less info shown.
                                    Changing the format is non-trivial.
                                    A change is difficult
                                Vs BI- Front End for Data Mining- Dashboards ("reports" is insult)
                                    Online Reports- Data on demand- see it then and there (on the fly)
                                    Ad Hoc Reports- I can choose row/cols to see (customized)- give your requirements at runtime (what u wanna see)- filter results at run time.
                            </BusinessIntelligence>
                    </Quantitative>
                </DataAnalysis>
            </KDD>
            <MachineLearning>
                Routine Things- computations, DB query arithmetics, etc done thru computer vs giving machine capability to create.
                f(x) = y for 1000s of x and then it'll estimate f and tell f(p) for p != x encountered.
                even knowing that f is not a square function is a piece of knowledge.
                vs Artifical Intelligence- Demonstrates knowledge representation and reasoning.
                    ML is a tool to realize AI. Of course there is knowledge with f(x) = y and reasoning also.
                    AI was demonstrated in medical field (research)
            </MachineLearning>
        </BuzzWords>
        <DataAnalytics> For the lack of better word (KDD involves Data Mining Models but not OLAP, reports, etc)
                attributes are called as features (fancy term)
                <Meta>
                    ML Coursera course is quite low level but not as much as other more mathematical foundation courses like that of MIT.
                            They tell algorithm and maths behind it but skip some steps that won't be necessary.
                            Jigsaw is high level course with only parameters to tweak and all matrix multiplications already taken care of. This is what will be used in real life so most useful course worklife-wise.
                            The Data Analytics course I did in IIITB was a detailed course on architecture of data science world with all details and algorithms captures but only their end results and few words on how they got there not working thru it.
                    ML is a way of AI where rules are learnt by using data.
                            AI - machines doing human tasks. Perception, recognition, Translation.
                            Expert Systems - Rules determine actions. with such humungous rule base humans could be simulated.
                            DL - subset of ML with learning over many layers.
                            Applications - Self Driving Cars, Image Classification, Handwriting transcription, speech recognition, Language translation.
                            OpenCV best computer vision software - 2.5K optimized algos for facial recognition, Object id, action class, camera movement tracking, etc.
                            native in CPP but python wrapper.
                            DL Module of OpenCV is called DNN - not a full-fledged framework. no backpropagation so no learning.
                            Only inference using pretrained model. Only forward pass - fast on CPU. Supported models - Caffe, Tensorflow, Torch, Darknet, ONNX.
                            external dependencies to the minimum. 16-bit FPO faster than 32. No specialized hardware needed.
                            OpenVINO toolkit by Intel to speed up the tasks. Models loaded and converted into internal representation similar to Caffe.
                                    different types of layers supported. wiki for different architecture available.
                            BGR - famous with camera manufacturers when developed.
                </Meta>
                <DataWarehouse>
                    about - get data ready for analysis.
                    <ETL>
                        ETL or SETIL or SETIM
                            Transformation from OLTP RDB to DimensionalModel condusive for analysis
                                you may want to see dimensionalModel tag- how it looks- where we are heading (output to conform to)?
                                We want to reduce the access path length- from n-level join to 1-level join.
                            Select, clean, construct, Integrate, format
                            Equivalent of TI of SETIM.
                        <Select>
                        Select- Non-technical (1 time effort)
                            Subject of interest is scattered across systems.
                            Select those sources (not sql select)
                                begin talking- I want your data from system- what format is it?
                            Eg- Retail- Inventory management- eCommerce system and Point of Sale (POS) system
                                call all these for inventory information (DBNames, Table Names maybe all different)
                            Interesting Data Sets
                                https://dbie.rbi.org.in/DBIE/dbie.rbi?site=home
                                data.gov.in
                                https://www.kaggle.com/datasets
                                https://www.springboard.com/blog/free-public-data-sets-data-science-project/
                                https://github.com/caesar0301/awesome-public-datasets/blob/master/README.rst
                        </Select>
                        <Extract>
                        Extract- Time critical- with deadlines- 
                            Mechanism to extract the relevant data
                            Agree with them for Sql scripts- frequency kicks in.
                            Someone may say- he would dump all data in CSV (comma separated values) format in this file location to fetch (extract using FTP)
                            Top Down- DWH -> Data Marts (quite heavy- many projects fail)
                            Bottom Up- Create Data Marts -> DWH thru merging. Still would prefer to work with Data marts only.
                            @Waterfall : Agile :: Top Down : Bottom Up.
                        </Extract>
                        <Transform>
                            Data cleansing
                            Missing data, incorrect data (char, varchar), different formats, units of measurements (Rs, $)
                            Decide standards and transform all into common formats.
                            <RecordAndAttributeSelection> Transformation Phase
                                attribute- is a property of entity.
                                <AttributeValues>
                                attributeValues- is externalized in different ways.
                                    Eg- ECity, Electronic City, Electronics City- different externalizations of same attribute of IIITB being in a city.
                                    Eg- Quantity of Gold- 1Kg or 26Lakh Rs.
                                    <Issues>
                                        MissingValues- Predict thru ML.
                                            Ignore the object, manually fill, use mean, use class mean.
                                        DifferentScales
                                            Scores in class test, mid sem, end sem, etc; salary and age.
                                            Normalization techniques
                                                LinearScaling- eg- strike rate = run rate / 6 * 100
                                                    merge 2 tables cols- under heading of normalized rates.
                                                ZeroMeanSalary- xi - mean
                                                    eg- zero mean salary of all emp.
                                                ZScore- xi - mean / sd
                                                    how many standard deviations distance away from mean is the value.
                                                TScore
                                        Duplicate Data- when merging- may get duplicate or almost duplicate objects.
                                            major issue when merging from heterogenous systems
                                            same person with multiple email addresses.
                                            synonyms, homonyms identification.
                                            expert decision like identify and delete.
                                        Outliers- denote noise in data.
                                            replace with reasonable value/ missing value
                                            discard the data object.
                                    </Issues>
                                </AttributeValues>
                                <TypesOfAttribs>
                                    Categorical- discrete values (strings)
                                        Nominal- operations supported =,!=
                                        Ordinal- Ordering supported also, <,>,=
                                            Eg- Ranking on height (low, med, high), ranking on scale of 1-10.
                                    InternalAttrib- Notion of difference
                                        !Eg- Tasteof Potato - taste of curd (on rating scale 1-5)
                                        Eg- Temperature difference.
                                    Ratio- All BODMAS supported.
                                </TypesOfAttribs>
                                Hypothesis Testing
                                    //TODO
                            </RecordAndAttributeSelection>
                        </Transform>
                        <Integrate>
                            I have cleaned data for DWH but can't dump cuz OLAP DB design (following Dimensional Modeling) is different so integrate (or combine) step
                        </Integrate>
                        <Maintain>
                        Maintain/Load
                            Data Marts- Views created for analysis
                                not entire data is of interest
                                ready for consumption (descriptive or explorative analytics)
                            <QueryOptimization>
                                <CategoriesOfCharacteristicsForQuery>
                                    factors that Query Processor (QP looks at)
                                    <Column>
                                        Cardinality- Unique values for a column. = Domain size.
                                        Distribution- frequency of each unique value in a column.
                                        Value range- min and max for every column.
                                        Density- Ratio of #unique values to total rows.
                                            PK has highest density.
                                    </Column>
                                    <Query>
                                        Selectivity- What portion of table is returned.
                                            If 1 row, then highly selective query.
                                            matters in indexing strategy.
                                            low selective queries won't use indexing even if there (later)
                                        Access Path
                                            to reach to data what path to choose in DB class graph.
                                            look at where clause- join clause.
                                            minimize the access path for query.
                                        Join Condition
                                            If join based on FK => lossless join is guaranteed.
                                            See dynamic logs, then it's found which queries can be improved.
                                                FK, etc.
                                        DBA enables statistics on data for QP to keep track and optimize.
                                    </Query>
                                </CategoriesOfCharacteristicsForQuery>
                                <Indexing>
                                    There is always a cost of maintaining index on CRUD- but OLAP has fewer updates (once a day)
                                    Generally DBA studies query logs and then decides on indexing, etc in  physical design.
                                    some vendors permit programmers to invoke- specific indexing on demand- not recommended in general but client may know what it's doing
                                    <BTree>
                                        Dense Indexing- lot of keys in index file => huge size
                                            solution- sparse Indexing- it fits in MM and easy search.
                                            Indexing of indexing blocks- to minimize block accesses.
                                        Prefered when
                                            High Density- helps easy zooming in.
                                            High Column Cardinality
                                        Clustered Indexing- Ordered data.
                                            Eg- on dept Id.
                                            Eg- Customer care always asks for customerId. why? cuz of indexing
                                                but people give mobile number- so gotta change.
                                            shortlisting on index column.
                                        Secondary Indexing- Not ordered.
                                            blocks of record pointers.
                                    </BTree>
                                    <ColumnStore>
                                        Rowstore- Generally used earlier. rows are appended one after another in files
                                            Good for many updates and accessors- where,select.
                                            bad for reading records/block (hardly 5 records/block, say)
                                        ColumnStore- Many records/blocks- so faster search.
                                            each column is in separate file but sequence is maintained for corresponding entries.
                                            bad for multiple updates, accessors.
                                            HANA is in-memory column store.
                                    </ColumnStore>
                                    <BitmapIndex>
                                        Prefered when
                                            low cardinality
                                        How
                                            List all the values in the domain of the column
                                            make a vector for all those values
                                                size of vector = #rows
                                            for the column value- mark corresponding entry of concerned vector to 1 and 0 for others.
                                                given a query, where clause value = a
                                                    search in bit vector for a- where value = 1 (using ANDing or linear scan or array pick)
                                            Effect
                                                so many boolean values can come in 1 block.
                                                OR and And in queries- do bitmap vectors for different columns to get bitwise AND, OR.
                                                using it for OLTP, we have to evaluate trade offs.
                                            Encoded Bit Maps
                                                if size of domain is n- instead of having n bit vectors.
                                                    Eg- Ia,Ib,Ic = 001,000,100,010
                                                we can have lg(n) bit vectors
                                                    Eg- a = 01, b= 11, etc.
                                    </BitmapIndex>
                                    <JoinIndex>
                                        Join = Cartesian Product + Select
                                            very costly operation = n^2 (blows up)
                                            QP stores the Join-Pair of values.
                                                don't discard the hard work done.
                                                not so much used for OLTP cuz of maintenance
                                    </JoinIndex>
                                    <JoinBitmapIndex>
                                        Make a join index.
                                        on low cardinality column in the table, create bitmap (eg- gender)
                                    </JoinBitmapIndex>
                                    <ProjectionStore>
                                        Don't revamp which DB on column store but on a subset.
                                            @On OODB buzz word, RDBMS adapted to ORDBMS by some.
                                            same thing going on here- rowstore facilitating column store on columns.
                                        See frequently used columns. 
                                        if 2 where clauses- then search on 
                                    </ProjectionStore>
                                </Indexing>
                                <PartitioningStrategies>
                                    Splitting large table to smaller ones- for multiple back ups.
                                        Parallel Query Processing in oracle take advantage of it.
                                    <Ways>
                                        Vertical- along columns
                                        <Horizonal>
                                            along rows- based on group by dimensional attributes.
                                            Partitions by DBMS is better than manually
                                                cuz repeating your queries on all those tables.
                                                Physical data independence- change physical structure should not change my queries (logical)
                                            Cube Materialization- store cubes not as views but tables.
                                                but costly- base cube of n dimensions result in 2^n lattice cuboids.
                                                    No of rows in every cuboid = no of unique values in dimension we aggregate upon.
                                                    Eg- group by store- if 6 stores then 6 rows.
                                                Denser cubelet- more aggregation applied to get to a value in a lattice cuboid.
                                                Partially Materialized views are iceberg's cubes.
                                                    BUC- Bottom up Cubes Algorithm to let go off rows that won't give a degree of density of interest.
                                                        Loss of data- No! original base cube is still there.
                                                        We ignore certain rows for reporting purposes.
                                                        OLTP has to be exact but OLAP can give approximation (not incorrect)
                                                        Sparse cell's descendents will be sparse.
                                                        If Sales[Jan] = 24,500, there is no way, Sales[Jan][Hats] > 25,000 so drop the january row.
                                        </Horizonal>
                                    </Ways>
                                </PartitioningStrategies>
                                <Others>
                                    <ReferentialIntegrity>
                                        Forget referential integrity to let go of FK checking in fact table.
                                    </ReferentialIntegrity>
                                </Others>
                            </QueryOptimization>
                        </Maintain>
                    </ETL>
                    <CharacteristicsOfDWH>
                        Subject oriented- spans across departments to finish.
                            so all DB departments have data of interest (HR, employee DB, etc)
                            very specific subject it deals with (vs OLTP- 1 request spans thru different departments for biz operation)
                            Purpose not biz operations but analysis (managerial type)- planning, decision
                            Huge space requirements (need more indexing than OLTP)
                            SELECT phase of DWH
                        Integrated- Eg- bring all customer information together- basis for analysis.
                            Uses aggregation structures (in data ware house)- cubes, etc.
                                Denormalized data- redundancy no problem cuz no transactions.
                            Query operations are relatively complex
                                Navigational (joins) vs associative (where colname = 25)
                                group by, having, aggregate functions are frequent.
                            Processing speed- relatively more relaxed.
                                runs in hours so 1 hr to 1 hr 5 mins is acceptable.
                            Extract, Transform, Integrate of DWH.
                        Non-Volative- CRUD over Database.
                            Load + Read only DB operations.
                            Bulk Operation- insert many in 1 go- Index structure not changed.
                            ACID no worry- cuz read only- Isolation fear cuz of write()
                        Time Variant- Addition keep happening in batches not like inserts (non-transactional)
                    </CharacteristicsOfDWH>
                </DataWarehouse>
                <BusinessIntelligence>
                    <Defn>
                        BI(Business Intelligence) is a set of processes, architectures, and technologies that convert raw data into meaningful information that drives profitable business actions.
                        It is a suite of software and services to transform data into actionable intelligence and knowledge.
                        There may be multiple usecases of data analytics.
                    </Defn>
                    <Eg>
                        Change in revenue due to new produce line and price
                        Increase in customers due to budget increase
                        Will changing customer profile support high price products
                    </Eg>
                    <ReportsAndAlerts> - focus on a specific operation or dataset for a period
                        static in nature based on predefined queries
                            vs OLAP - interactive for user to fire a query of choice.
                        Eg - Monthly sales kinda stuffs
                        <ConventionalApproaches>
                            <MIS>
                                Online reporting
                                static, inflexible, non-interactive.
                                    hardcoded queries.
                            </MIS>
                            <Spreadsheet>
                                Eg- Excel (popularized by Microsoft), others
                                Formulae, pivots, etc supported
                                    but slow in computation
                                2-D tables are shown
                                    maximum 3-D thru exploiting many sheets.
                                now #cols run in GB (26 X 26 b4)
                                <ExcelExercise>
                                    workbook has many sheets
                                    sheets has rows and cols- cells in there- which are numbered.
                                    no inherent notion of column name- like tabular data model.
                                    sheet = table- give an apt name. workbook being DB- no col table- no describe table- you manage those thigns
                                    no attrib, no data type- store anything- limited notion but nth enforced.
                                    Understanding cell references- addresses of cells. eg- A2- intersection of row and col.
                                            to refer to cells- formula is first place we use cells and excel programs.
                                                    formulae- if I say A1- that's a string, so = A1 (relative) OR =$A$1 (absolute)
                                                    no notion of empty cell- so null values or 0 is taken- display formatting differences there.
                                                    text box shows that it's a formula and not data. So be careful with that.
                                                    Ctrl + C for the formula and paste the formula to selected cells. But updates the formula based on where based (relative)
                                                            this is the problem- attention to relative and absolute addressing.
                                                    Don't have to type cell references- = and then move cell or click cell to be referred.
                                            relative addressing
                                                    textbox shows this.
                                                    wrong formula may percolate
                                            absolute addressing
                                                    Once copy pasted and done
                                                    editing on original- the pasting not auto updated.
                                                    change the formula thru copy pasting again.
                                                    autoincrementing the cells in formula can be avoided thru $A$2
                                    Cell Range
                                            for carrying out aggregate operations.
                                            =sum(D2:D8) or drag the mouse
                                            absolute range as sum($D$2:$D$8)
                                            in sum i would want to use relative while copying the formula to other cells.
                                            Functions available
                                                    click f(x)- wile in null cell.
                                                    categories of functions
                                                    suggestions also supported.
                                    Analytics stuff
                                            bad- repot generator or editor those who don't know how to use it.
                                            Data preparation steps
                                                    analyst nightmare= 1st row shouldbe column headings. (fancy titles are useless)
                                                    merged cells for data to look good- is bad for analytics.
                                                            merge and center button
                                                            get rid of them.
                                                            can't selected a row at a time- all will ge selected together but formatting is good.
                                                            for short data we can copy paste but for huge? it's pain.
                                            Good File
                                                    boring data is interesting for analytics.
                                                    can do roll up in fact table but makes no sense to us. cuz no join in excel!
                                                    analytics can't be done without it so simulate it.
                                                    get additional attributes of interest to us. Dimensions attribs fetch in there worksheet.
                                                    cryptic data- get master data- (aka dimensions!!)
                                                            preferably in CSV or excel- sql dump is unacceptable.
                                                            we are doing ETL in crude way.
                                                    in product name in fact table- we use VLookUp(JoinKey, [click text box to see suggestions]table data, columnNo)
                                                            Eg- A2, Product!A:C (sheet and column range), 2, false
                                                                    like I could say for this cell to that cell.
                                                                    2 being col name for product name- to be fetched.
                                                                    false means approximate match no!
                                                                    Join key on the other side must be in the first column- if not there then do it.
                                                                    likewise, I can do the file also- referential integrity- send files together.
                                                                    How to delink the formula with the sheet
                                                                            select column and say plaste special under one tab- "only the values to retain"- formulae are gone!
                                                                            use it when data is present in other workbook.
                                                                            how to get data from different workbook?
                                                            so entry of this analysis can be on any dimenstion attrib!
                                                    Finish this file
                                            Limitation- anything we do = data has to be in a single sheet- do formula- lookup, do anything- but 1 sheet.
                                    Slice and Dice - this comes under OLAP feature of excel once data is imported.
                                            Create filters- go A1- data tab and filter- filtering criteria.
                                            click on drop down of column and choose the attribute you want to slice on.
                                            do another filter on another column- dice operation.
                                            or choose multiple values for slice.
                                            But rows will not be contiguous anymore- so don't do modifications with filters on.
                                            blue rows to hint to us- don't add new formula or copy paste gets funny. copy these cells or paste into filtered mode- worse!
                                            sign for which filters are active.
                                            filter button again to remove- toggle on and off.
                                            Test DWH with excel exam.
                                    Pivot
                                            make a summary sheet 
                                            have to insert a pivot table.
                                            wizard appears.
                                            click drag the cells to analyze.
                                            canvas appears for you
                                            Linux- select cells- create pivot table under data tab.
                                            Add rows to add to report- 
                                    Roll Up and Drill Down
                                            DV in there and F- rows and A to choose
                                            V to analyse- 
                                            Drag and drop the cols into row labels and value sections
                                            use triangle to change the aggregate- count is default.
                                    Pivot
                                            switch column labels and row labels to pivot.
                                            I added another column after product name- drill down happened.
                                            These values are all dynamic- use refresh on cells.
                                            This is powerful for dimensional modeling with excel
                                            Double click any cell in summary and data shown in new sheet.
                                    <Uncat>
                                        Excel- blank cell? may be first char is newline!
                                    </Uncat>
                                </ExcelExercise>
                            </Spreadsheet>
                            <SQLOnOLTP> Done B4- see AnalyticsInOLTP tag. </SQLOnOLTP>
                        </ConventionalApproaches>
                    </ReportsAndAlerts>
                    <OLAP> - approach to answer multi-dimensional analytical (MDA) queries swiftly in computing (wiki)
                        applications of OLAP include (wiki)
                            business reporting for sales, 
                            marketing, 
                            management reporting, 
                            business process management (BPM),
                            budgeting and forecasting, 
                            financial reporting
                        <DimensionalModeling> Basically understanding the Data Warehouse (which is a dimensional model)
                            <Types>
                                <ROLAP>
                                ROLAP- Relational OLAP.
                                    In OLTP, focus is not on details but on money, inventory- transactional completeness.
                                    DB Design but thinking differently.
                                    <Schema>
                                    Schema = {D,V,F,A}
                                        <Dimensions>
                                        D- Dimensions- time, place, etc
                                            properties used as analysis criteria (entry points for analysis)
                                                so called? picked from coordinate geometry.
                                                    on n-Dimension plane, with changing values along axes, how does the data/fact change.
                                                data details without this metadata is worthless!
                                                    REL I have sales data- don't know store, don't know time, don't know anything!
                                                Desirable to have a biggest dimension size possible- more capability to answer questions on analytics.
                                            Better be textual descriptors- cuz end user understands "New York" better than 1042.
                                                so <= or >= may not be useful- use with caution!
                                            Dimension Table
                                                Find Attributes (properties, description) for every dimension instance
                                                    Eg- YoFun Store- locId, city, in hindi, in persian, country, store manager, store size, store type.
                                                    More the properties- more power (amenable) added to DA- doors to enter for analysis- sales of Bangalore, sales of small stores.
                                                    Eg- Date Dimension- Date, day, year, yearInLunarCalendar, HolidayIndicator(locally may change)
                                                        could have caculated in run time but that can be very costly so precomputed values are kept (even tools available for that)
                                                Assign values for all those for all dimension instances and make a table outta that.
                                                    Eg- make a table outta all these values
                                                    How do these dimension distinguish themselves from others (properties)
                                                    Details of same details in different units, languages, mathematical transformations (to save run-time computation)
                                                Dimension table is generally long and shallow (many columns but few rows)
                                                Can we have #employees as dimension? Will you want to have analysis using #employees as criteria?
                                                    Find sales in companies with employees in this range- we may want to make bins (like histograms) so as to perform aggregation.
                                                Dimension Tables give the entry points for analysis
                                                    There may be a GUI- user enters the attributes along with sales data is to be measured- quarter 3, Bangalore- show me!!                                                                
                                                <SlowlyChangingDimensions>
                                                    Some Dimensions may change in their values (or domain for fact table)- this is infrequent (not OLTP)
                                                        Eg- Accessories is to be renamed to Spares.
                                                        General rule of thumb- deny it! unless it is really important- like misspelt in DB.
                                                    Type 1- Overwrite data
                                                        Result- all queries for accessories will return 0- heart attack for managers!
                                                    Type 2- Add new rows
                                                        Gotta change ETL to write FK for new entry on insertion into DWH.
                                                        Result- Queries results will be distributed over the 2 types.
                                                            Good if want to compare if a brand name change has had impact on sales!
                                                    Type 3- Add new column- old value, new value
                                                </SlowlyChangingDimensions>
                                                <DegenerateDimensions>
                                                    No attributes for the dimension- no property to dintinguish it against other.
                                                        Eg- Order No.
                                                </DegenerateDimensions>
                                                <JunkDimensions>
                                                    Misnomer for "Garage Dimensions"
                                                    used for low cardinality tables
                                                    Take cross products of k tables- cuz the 3 dimensions come together.
                                                        Why? all combinations of these dimensions are going to appear in the real life fact data.
                                                        why not union? then we will need more than 1 row simultaneously.
                                                </JunkDimensions>
                                        </Dimensions>
                                        <FactTable>
                                        V- Fact variables- metrics
                                            around on which the DA is to be done.
                                                Eg- Sales for stores
                                                All 'metrics' can be clubbed together as a set/relation under this. (represent points- by color, size, etc)
                                            Fact tables are generally short and deep (few columns, many rows)
                                            Give 1-level of Join (distance of access path from fact table) from Fact table.
                                                Advantage over OLTP SQL query- (k-1) joins is always better than k joins.
                                                1-level of join not 1-join- Distance of any table in join from Fact table in the graph.
                                                All FK in the Fact Table- All these FK form composite key- so that can't repeat!
                                                This forms a star like structure of connection- STAR SCHEMA.
                                                Normalization of Fact table is confined to Dimension table. Not ahead, if take it further then snowflake Schema.
                                                    snowflake schema may have multi-level joins.
                                            Sparse data- when certain points on the coordinate dimensional planes are missing data- eg- no sale on sundays.
                                            Factless Fact Table
                                                No facts- meaning the fact variables are not there- V = PHI.
                                                But dimensions are still there.
                                                Value? aggregation on count. Further rolling up the lattice, can use other aggregate functions.
                                            Accumulating snapshot
                                                Generally the Fact table is not supposed to change, it is once for all.
                                                But situ may demand it- eg- manufacture date, inventory date, projected date, shipping date, delivery date
                                                    these things may update with time.
                                        </FactTable>
                                        <Measurement>
                                        F- Measurement facts- mapping from D1 X D2 X ..X Dn -> R1 X R2 ..X Rm
                                            Cube (or cuboid?)- Rubic's cube with values for all. or k such rubic cubes.
                                            Matrix view- SalesData[NewYork][Jan][Hats] = {164,1232};...
                                        </Measurement>
                                        <Aggregation>
                                        A- Aggregation Operators- sum, average, etc.
                                            <Lattice>
                                            Lattice- Graph constructed from transformations between different levels of granularity.
                                                Granularity- number of dimensions along with a fact is presented.
                                                    a fact in which 1 dimension value is unknown is not 3-grain data but 4-grain data with 1 value being null.
                                                    n-grain- decided as per need of analysis and not OLTP systems- yes, they may make us compromise.
                                                nodes- data cube on k-D.
                                                edges- transformation using an aggregation function.
                                                    removing 1 dimension from the k-D cube to transform into (k-1)-D cube.
                                                    using apt aggregation function- Equivalent of columns select on group by, having in SQL.
                                                    transforming back (k-1) to k- see, our n-D is original. we are doing all rolling up on desire.
                                            </Lattice>
                                            <Eg>
                                            Eg- For sales, let's say we used 4-D (where analysis starts from)
                                                4-D data for sales- Time, product, Location, Supplier
                                                3-D data for sales- 4C3 such cubes- Time, product, Location (group by these 3 attribs and aggregate over 4th); etc. 
                                                    (edge between all of them with 4-D cube)
                                                    not that these 4C3 cubes are unique- may differ based on aggregation function- Equivalent of projection of points along on (k-1) dimension.
                                                0-D- just 1 value- aggregation of it all- may say- total sales or sth.
                                            Eg- weather information points in a 2-D (points mapped from time and location);
                                                for 3-D, maybe height.
                                            </Eg>
                                            <AggregateTypes>
                                                Distributive- Good for incremental computation- eg- max(), count(), sum()
                                                Algebraic- a function of distributive- avg = sum()/count()
                                                Holistic- Median, mode, Rank, etc.
                                            </AggregateTypes>
                                            <Mapping>
                                                Fact Table- Relations
                                                Dimension Table- Relations
                                                Cubes at lattice points- Views
                                                    Eg- create view TLP_cuboid as (
                                                    select time, location, product, sum(sales)
                                                    from sales_fact
                                                    group by time, location, product
                                                    )
                                                    Fun Assignment- make a lattice structure thru GUI thru program.
                                                        given a string create all permutations and combinations.
                                            </Mapping>
                                            <Eg>
                                            Eg- Sales table- Time, location, product, supplier
                                                    Fact table- 1 with 5 cols (4 in composite key)
                                                    Dimension table- 4 in number
                                                    cubes
                                                    #data dictionary in Information_schema
                                            </Eg>
                                        </Aggregation>
                                    </Schema>
                                </ROLAP>
                                <MOLAP>
                                MOLAP- Uses vocabulary like cuboid, etc and is more powerful for DA.
                                </MOLAP>
                            </Types>
                        </DimensionalModeling>
                        <OLAPOperations>
                            <SlicingAndDicing>
                                Slicing
                                    Selection of 1-dimension of cube for which results are shown
                                    1 Dimension is cut out using that dimension's entity.
                                    Eg- In matrix notation, SalesData[NewYork][Time*][Product*]
                                    Eg- tell sales data in country C5- take cutting of cube.
                                Dicing
                                    Sales[NewYork][January][Product*]
                                    when cut in 2 pieces!
                            </SlicingAndDicing>
                            <RollUp> aka consolidation
                                Sum(salesData[][][Product*])- group by products
                                Computing summary on 1 or more dimensions using aggregation.
                                Generally, slicing and then aggregation.
                                Views of cuboids
                                    View behave like tables- virtual tables- can be used in SQL queries.
                                        very powerful tool in programming
                                        made for convenience
                                        refer view inside view- why not! check
                                            cross product on views!
                                        vs Materialized views (by oracle)- different syntax
                                            Make this view and give it space and iin sync with base table.
                                            View refresh happens on ETL phase.
                                            So best of both worlds- advantages and convenience of view and performance of table.
                                    Views are "named" SQL queries
                                        no object or space associated.
                                        so bearing on performance- translation related at the run time.
                                        Eg- create view Big_stores as select * from stores where s.size > 1000;
                                            select * from Big_stores where location = BLR       //stores here is base relation.
                                            TRANSLATES TO- select * from stores S where S.size > 1000 and location = bangalore.
                                                this is done by Query Processor- in Query Plan phase (b4 improving and exec phase)
                                                Behaves exactly as #define in C- macros
                                        has hidden where clause vs making another new table using insert into table select...
                                            No updates with the change in data in OLAP.
                                    Updates/ Insert/ Delete
                                        Only for translation. So constraints of base table should remain intact.
                                        insert into table select...
                            </RollUp>
                            <DrillDown>
                                It's adding new dimensions to aggregated data- to see more details of a summary.
                                Eg- given total Bangalore rain- tell me separated by months!
                            </DrillDown>
                            DrillThrough
                                going back to OLTP to see details
                                this is vendor specific terminology.
                            Pivot
                                A different display mechanism- different view/analysis/ perspective of same data
                                for different kind of problems- we want to see table data differently.
                                    Eg- M[A][B][C] made into M[B][A][C]
                                    Bangalore           Chennai
                                    Jan Feb Mar     Jan Feb Mar
                                    4   5   7       8   0   7
                                    vs
                                    Jan         Feb         Mar
                                    Ban Che     Ban Che     Ban Che
                                    4   8       5   0       7   7
                                AKA rotation of axes of dimensions.
                            <EgThruExcel>
                                Data Model- Workbook, Worksheet, column, cell
                                    Star Schema in excel demonstrated
                                    Any organization (new ones esp) is happy to do their analytics on excel.
                                    Data thrown at you- contains dimension in vague names- ask for dimension files separately.
                                    Excel doesn't have built in join- but add-ons available.
                                    for OLAP, get them all in 1 worksheet to "merge"!
                                    Useful Operations
                                        VLOOKUP
                                            on every dimension key
                                        Import files in CSV into excel
                                        Slice and dice thru filter
                                        Drill down in excel using pivot option in excel
                            </EgThruExcel>
                        </OLAPOperations>
                        Tools - 
                            !Tableau - Tableau is fundamentally a data visualization tool and a great complement to a true OLAP tool
                            IBM Cognos, Microstrategy, Pali, Apache Kylin, OBIEE (from Oracle), SAP AG, etc.
                    </OLAP>
                    <DataMining> 
                        <Defn>
                            the practice of examining large pre-existing databases in order to generate new information (auth:google oxford dictionary)
                            applications of statistical methods to derive new insights ; auth : iiitb
                        </Defn>
                        <Applications> auth : guru99
                            Communications - Targetted campaigns - Data mining techniques are used in communication sector to predict customer behavior to offer highly targetted and relevant campaigns.
                            Insurance - Pricing and offers - Data mining helps insurance companies to price their products profitable and promote new offers to their new or existing customers.
                            Education - clustering weak students - Data mining benefits educators to access student data, predict achievement levels and find students or groups of students which need extra attention. 
                                    For example, students who are weak in maths subject.
                            Manufacturing - wear tear mgmt - With the help of Data Mining Manufacturers can predict wear and tear of production assets. 
                                    They can anticipate maintenance which helps them reduce them to minimize downtime.
                            Banking - risk mgmt (defaulters predictions), compliance - Data mining helps finance sector to get a view of market risks and manage regulatory compliance. 
                                    It helps banks to identify probable defaulters to decide whether to issue credit cards, loans, etc.
                            Retail - association rules, lucrative offers - Data Mining techniques help retail malls and grocery stores identify and arrange most sellable items in the most attentive positions. 
                                    It helps store owners to comes up with the offer which encourages customers to increase their spending.
                            Service Providers - attrition prediction, Service providers like mobile phone and utility industries use Data Mining to predict the reasons when a customer leaves their company. 
                                    They analyze billing details, customer service interactions, complaints made to the company to assign each customer a probability score and offers incentives.
                            E-Commerce - cross sells and upsells - E-commerce websites use Data Mining to offer cross-sells and up-sells through their websites. 
                                    One of the most famous names is Amazon, who use Data mining techniques to get more customers into their eCommerce store.
                            Super Markets - Targetted marketing, pattern recognition - Data Mining allows supermarket's develop rules to predict if their shoppers were likely to be expecting. 
                                    By evaluating their buying pattern, they could find woman customers who are most likely pregnant. 
                                    They can start targeting products like baby powder, baby shop, diapers and so on.
                            Crime Investigation - locale estimation - Data Mining helps crime investigation agencies to deploy police workforce 
                                    where is a crime most likely to happen and when?
                                    who to search at a border crossing etc.
                            Bioinformatics - Data Mining helps to mine biological data from massive datasets gathered in biology and medicine.
                        </Applications>
                        <Benefits> what we got by adding DM to our company's portfolio (following to be reviewed properly) auth : guru99
                            give knowledge-based information
                            enable profitable adjustments in operation and production
                            decision-making process
                            cost-effective and efficient solution
                            automated prediction of trends and behaviors as well as automated discovery of hidden patterns
                            implementable in new systems as well as existing platforms
                            speedy process to analyse huge data
                        </Benefits>
                        <Challenges> auth : guru99
                            sell useful information of their customers to other companies for money. Eg - Amex sold credit card information.
                            analytics software is difficult to operate and requires advance training 
                            tools work in different manners due to different algorithms employed - selection of correct data mining tool
                            techniques are not accurate, and so it can cause serious consequences in some conditions
                        </Challenges>
                        <CRISP_DM>
                            A Framework for DMLC (Data Mgmt Life Cycle)
                                structured approach to planning a data mining project (auth:sv-europe)
                            Full Form - Cross Industry Standard Process for Data Mining.
                            @Programming : SE Processes (like waterfall) :: Data : CRISP DM
                                boom nowadays, so need for standardization of process.
                            All stages have their objectives, goals, exit criteria.
                            about : also includes ETL inside it (see it in that context)
                            <BusinessUnderstanding>
                                Data mining, machine learning also for non-business purposes.
                                Understanding project objectives and requirements
                                    Difficult to get to out of people (like SRS was)
                                    gotta sell it (the service)- what we can do- you benefits- exploratory, descriptive, predictive.
                                Data mining problem definition
                                    must have a context of business.
                                    Exit Criteria- Success of project- cross sales by 10%
                                <ProjectWorkInfo>
                                    explore what your organization expects to gain from data mining
                                            discuss and document
                                                    Why? ensure that everyone is on the same page before expending valuable resources.
                                            Final Step- project plan
                                    <DetermineBusinessObjectives>
                                            minimize later risk by clarifying probs, goals, resources
                                            Eg- 
                                            e-retailer facing competition with others- how to remain profitable while acquisition cost growing
                                                    Solution- cultivate existing customer relationships
                                                            maximize current value of each customer.
                                                    4 objectives
                                                            Better recommendation -> inc sales
                                                            More personalized service -> Inc customer loyalty.
                                                    Success Criteria
                                                            Cross sales rise by 10%
                                                            Customers spend more time on web pages
                                                            Timely budgeted finish of project
                                            TASKS
                                                    Gather background information
                                                            Available resources- personnel and material
                                                                    aka Determine organizational structure
                                                                    charts- dept, divsions, project groups- 
                                                                            mgr name-resp
                                                                    Identify
                                                                            Id key indiv
                                                                            finantial support and domain expertise
                                                                            procure a list of members
                                                                    Identify BU affected by the DM Project				
                                                            Problems
                                                                    aka describe problem area
                                                                    DM's current status quo
                                                                            already used or need for advertisements.
                                                                    Identify problem area
                                                                            marketing, customer care, or business development.
                                                                    Describe problem
                                                                    Pre-requisites Clarified
                                                                            motivations
                                                            Goals
                                                                    aka describe current solution
                                                                    state of art for the business problem
                                                                            its advantages and disadvatages
                                                                            its acceptance level
                                                    Document key business objectives
                                                            construct objective- attested by sponsor and affected BU
                                                                    nebulous to concrete that will guide analytics
                                                                    Eg- from "reducing customer churn" to sth DM specific.
                                                            Task lists
                                                                    Describe the problem thru DM
                                                                    Specify all business questions precisely
                                                                    any other reqt? like retaining current cust
                                                                    Specify expected benefit (in biz)
                                                                            Eg- reducing churn among high-value customers by 10%
                                                    Criteria of Business Success criteria of data mining
                                                            aka how you tell that you are there
                                                            Objective- (for all biz objectives, there is a success criteria)
                                                                    Eg- specific increase in the accuracy of audits 
                                                                            an agreed-upon reduction in churn.
                                                            Subjective
                                                                    criteria like “discover clusters of effective treatments” are more difficult to pin down
                                                                    align the arbiters- get notes on expectations.
                                    </DetermineBusinessObjectives>
                                    <AssessingTheSituation>
                                            (Resource Inventory)
                                            aka where we are right now
                                            Eg- eRetail
                                                    Personnel- server logs and DB- for cleaning a DB adminstrator; roles- current or permanent?
                                                    Data- plenty of web logs and purchase data (restricted to registered users)
                                                    Risks- timely and budgeted finish (if budget grows then reduce the scope)
                                            Research Hardware Resources
                                                    what we need
                                            what data is available for analysis
                                                    data sources- 
                                                            data types, formats
                                                            storage mechanism
                                                            access to DWH
                                                    Knowledge stores
                                                    Plan to purchase- demographic data
                                                    Security issues to acces				
                                            personnel available?
                                                    Access to biz and data experts
                                                            identify db administrator, staff
                                                            contacts
                                                    Team
                                            Risk factors?
                                                    contingency plan for each
                                                    Time, money!
                                                    Scheduling- takes longer than anticipated
                                                    Financial- sponsor faces budgetary probs
                                                    Data- poor quality/coverage data
                                                    Results- less dramatic than expected
                                            Liabilities
                                                    Determine Requirements
                                                            security/legal restrictions on data/results
                                                            scheduling requirements- e1 aligned
                                                            requirement on results deployment- publishing/ reading scores in db.
                                                    Clarify assumptions
                                                            Economic factor- consulting fee, competetive products
                                                            Data quality assumptions
                                                            Results- how mgmt wants to see- only results or model understanding also
                                                    Verify Constraints
                                                            passwords for data access
                                                            Legal- on data usage
                                                            Financial- covered in budget
                                            Terminology
                                                    Biz and DM teams speak same language
                                                            compiling a glossary of technical terms and buzzwords
                                                            Eg- "Churn" meaning, "Gains Chart plot"
                                            Cost Benefit Analysis
                                                    cost on project vs benefits from success
                                                    Costs sources- Data collection (external), Results deployment, Operating costs
                                                    Benefits- primary objectives being met means what, additional insights from exploration
                                    </AssessingTheSituation>
                                    <DMGoals>
                                            Determine DM Goals
                                                    Biz goal to DM Reality
                                                    Reduce churn =>
                                                            Identify high value customers as per recent purchase
                                                            Model to predict churn likelihood of each cust
                                                            rank cust on value and churn likelihood
                                                    Type of DM Problem
                                                            Cluster, prediction, classification
                                                            Document goals using specific time
                                                            Eg- provide churn score for 80% cust
                                            Objectives Eg- 
                                                    Market Basket Analysis
                                                            Previous purchase analysis -> related items => Item description page would give links for other related items
                                                    Profiling
                                                            Web logs -> what ppl try find => redesign site to highlight these -> different main page for cust.
                                                    Sequence analysis
                                                            Web logs -> where will the person go next from current => links!
                                            DM Success Criteria
                                                    in DM terms- formulate benchmarks as per DM goal (previously)
                                                    Define subjective measurements the best you can
                                    </DMGoals>
                                    <ProjectPlanProduction>
                                            Master document for DM work
                                            informs e1 of goals, resources, risks, schedule for all phases
                                                    discuss tasks, plan with e1 involved
                                                    Marks phases where iterations typically happen
                                                    highlight decision points and review requests
                                            Assessing tools and Techniques
                                    </ProjectPlanProduction>
                                </ProjectWorkInfo>
                            </BusinessUnderstanding>
                            <DataUnderstanding>
                                Initial data collection and familiarization
                                    Identify Data Sources (inventory) and nature of data.
                                        Equivalent of Select and Extract in DWH.
                                    Describe Data- ER diagrams, formats, attrib types.
                                    Explore Data- look at the data.
                                casually, Identify data quality issues
                                    Verify Data Qualities- is it sufficient, amenable, etc.
                                Initial, obvious results
                                    Description of data made available
                                    data not amenable then go back to previous stage
                                <KNOWTHEDATA>
                                Before we apply analytics on data, we gotta KNOW THE DATA- X (system, who) is sending data about Y (format, what) from Z (data source, where) at frequency A (when)
                                    <Who> Origin Of Data
                                        User Generated
                                            End user generates it
                                            may [not] be machine readable
                                            may [not] be reliable
                                            Bearing on poorly designed input source
                                                Eg- 2k8 admissions to IIITB- couldn't tell which state/ city has generated many students
                                                cuz all were text boxes in the form.
                                        System Generated
                                            independent of application.
                                                Eg- Web servers, frameworks.
                                            analyse- which pages to which page, people are likely to go- refer to/ get data from web server
                                        Application Generated
                                            source of data is OLTP/ app.
                                            eg- sales stats of last quarter, which item of this brand is famous these days.
                                    </Who>
                                    <Where> What is the data about (Concerns with)?
                                        Data Sources can be 1 of following
                                            can't understand data yourself => can't program it.
                                            speech is what and not where- has a bearning of course
                                        <Field>
                                        From The Field (users fill information)
                                            Ground level data- collected thru census- fill the forms- fill data thru tablets
                                            Structured cuz of form filling but unstructured cuz of freedom of choices (so not fully machine readable)
                                            There will be missing data, anomalies (inconsistencies)- sth to worry about in analysis.
                                            Digital Pen/ Smart Pen- what u write is captured as image
                                                OCR- Optical character Recognition- to recognize printed text
                                                ICR- Intelligent character Recognition- to recognize handwritten text
                                        </Field>
                                        <OLTP>
                                        Data today OLTP systems
                                            More towards structured side
                                        </OLTP>
                                        <Client>
                                        Generated by client
                                            vs OLTP- it is database server but here it can be IOT data generated to you but different in extent of structuredness.
                                            Eg- census, blogs.
                                        </Client>
                                        <Department>
                                        Department Systems
                                            Info from finance dept, accounting, IT, HR, etc
                                            precedence of info (hard info, soft info)- audit dept info more important than any BU.
                                            Imagine if you were told the data you are staring at is HR ministry data- it will make more sense!
                                        </Department>
                                    </Where>
                                    <What>
                                        How much data (in volume)
                                        how usable is data- format (as follows)
                                            eg- 01K84 is marks- K is karntaka 84/120.
                                        <Structured>
                                            machine readability- Well Separated data
                                                Bit Addressable- Information of interest can be retrived/ addressed.
                                                Eg- Tell me address location of a person- in RDB vs resume- resume search in non-trivial.
                                            Presence of metadata
                                                Eg- In RDBMS, column names, table names are additional metadata.
                                        </Structured>
                                        <Unstructured>
                                            Eg- Resume
                                                it is very much structured, has metadata.
                                                but not readable thru machine (only human)- machine readability paramount importance.
                                                Every person writes resume in different format- so machine can't automate its reading.
                                                If I standardize grammar to read
                                                    that grammar should not be in .doc format understandable to only microsoft usrs (universal)
                                                    if universal grammar, then it is structured.
                                            Eg- Images, videos, text
                                                Invoices, Bar graph, scanned images of sign and photo (transcripts) for admission
                                        </Unstructured>
                                        <Semistructured>
                                            Elements of structuredness and unstructuredness- sth can be retrieved but not others.
                                            Eg- Milind's Resume in HTML format
                                                readable thru screen readers- Eg- for, sr-only in bootstrap, html
                                                address tag for retrieving some information.
                                                can get all bullet points using metadata in form of tags
                                                    but tags are primarily for display (by browser) and not about content
                                        </Semistructured>
                                    </What>
                                    <When>
                                        Data Frequency- How frequently you get data. How recent is data.
                                        Batch Data- periodically data is sent as a file
                                            can be addition to existing data or independent set.
                                            Good for analaysis/ business
                                            less frequent- say twice a day.
                                        Real Time Data
                                            RTS- any system that reacts to events as they happen
                                            vs real world data- here we have stream data- bearing on tools, technology, etc we are gonna use.
                                    </When>
                                </KNOWTHEDATA>
                            </DataUnderstanding>
                            <DataPreparation>
                                About : 
                                    why do these phases look a lot like ETL?
                                        Because the source of data for CRISP-DM can be from ETL's output or it can be extracted from some scraping or REST Calls, etc.
                                    this phase consumes most of the time and is reiterative.
                                <Tasks> auth: dummies.com
                                    <SelectingData>
                                        portion of the data that you have is actually going to be used for data mining
                                            decide inclusion and exclusion
                                        based on 
                                            relevance to your goals, 
                                            data quality, 
                                            technical issues — such as limits to the number of fields or rows that your tools can handle, 
                                            the suitability of the data formats for your needs.
                                    </SelectingData>
                                    <CleaningData>
                                        make specific data corrections
                                            replacing with defaults or regression
                                        excluding bad data items
                                        subsetting the data
                                        remove redundancies of data objects
                                        make data-cleaning report documenting - 
                                            every decision and action used to clean your data
                                    </CleaningData>
                                    <ConstructingData>
                                        derive some new fields
                                            Derived attributes - new columns (how and why)
                                            Generated records - new rows (how and why)
                                        aggregate, etc
                                    </ConstructingData>
                                    <IntegratingData>
                                        merge disparate datasets together
                                    </IntegratingData>
                                    <FormattingData>
                                        formats may not be uniform 
                                            or suitable for training
                                        deliverable for this task is your reformatted data
                                    </FormattingData>
                                </Tasks>
                                <DataQuality>
                                    Data Quality- explores why we need data preparation in first place.
                                    REL S/w Quality- Load Tolerance, Scalable, defects free.
                                    Data Quality- GIGO- worthless analysis- not trustworthy.
                                    Indicators- All of these issues are addressed at ETL (or data preparation at warehouse.)
                                        <SELECT>
                                        Accuracy- Source of data- instrumentation.
                                            if application not programmed to collect data correctly- whom to blame- bro, we have problem here!
                                        </SELECT>
                                        <EXTRACT>
                                            Timeliness
                                                Issues with stale data.
                                                #business rules problems trace back to OLTP; OLAP guys yelling at 7 OLTP.
                                        </EXTRACT>
                                        <TRANSFORM>
                                            users may give no text (optional field), bad text (10 kg, 2 dozens) or variety text (Ecity, ElecCity)
                                            Domain Integrity
                                                like column values-
                                                Eg- Invalid telephone- No, it's accuracy issue.
                                                Different ways of writing telephone?
                                                Eg- M.Tech, MTech, MTechnology, ME, etc.
                                                General purpose fields- useless for any analysis (misc)- eth goes there!
                                            Completeness
                                                Lots of optional fields.
                                                    Semantic integrity constraints.
                                                    NULL allowed in OLTP- bytes back here!
                                                        in OLTP, we want to enter minimal possible data.
                                                    Eg- Credit Card- optional card held already (optional field user can fill)
                                                        can't do analysis on that dimension but on others surely.
                                            Structured
                                                "10 Kg"- String- cryptic data- units mean multiple units.
                                        </TRANSFORM>
                                        <Integration>
                                        ?INTEGRATION (Schema making)
                                            Consistency
                                                ?Data doesn't contradict.
                                                Data with different names- Product Id- same item has different Ids.
                                            Anomaly
                                                Synonyms- different names same meaning
                                                    Eg- unit or dozen?
                                                    Eg- amount or quantity for product.
                                                Homonyms
                                                    Same pronunciation and spelling but means different things
                                                    Eg- Bark- means tree bark or dog's bark.
                                        </Integration>
                                </DataQuality>
                            </DataPreparation>
                            <Modeling>
                                Run the data mining tools
                                    All DA Techniques, algorithms
                                    Assess models in terms of algorithm chosen
                                Goes back to Data preparation cuz data preparation is not discrete stage
                                Performs unit testing.
                                <Purpose>
                                    <about> auth: https://whatis.techtarget.com/definition/descriptive-analytics 
                                        Descriptive analytics: What happened? (not obvious - increase in Twitter followers after a particular tweet) 
                                        Diagnostic analytics: Why did it happen? ?(investigation, hypothesize)
                                        Predictive analytics: What could happen in the future? ?(trends - RNN)
                                        Prescriptive analytics: How should we respond to those potential future events?
                                            try to identify the best outcome to events, given the parameters, and suggest decision options to best take advantage of a future opportunity or mitigate a future risk.
                                    </about>
                                    <DescriptiveAnalytics>
                                        <Defn>
                                            Descriptive analytics is a preliminary stage of data processing that creates a summary of historical data to yield useful information and possibly prepare the data for further analysis.
                                            Diagnostic analytics is a deeper look at data to attempt to understand the causes of events and behaviors.
                                        </Defn>
                                        Ref - geeksforgeeks.org (dicing, drill down, lattice, accurate info from past data)
                                        Covered B4- in DimensionalModeling Tag.
                                    </DescriptiveAnalytics>
                                    <DataExploration>
                                        Personal
                                            Explore - travel through (an unfamiliar area) in order to learn about it.
                                            Exploratory Data Analysis (EDA) - In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.
                                                A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.
                                        Not sure of context (Confirm!)
                                        aka Exploratory [data] analytics- helps formulate hypotheses.
                                        <StatisticalDescriptors>
                                            <SummaryStatistics>
                                                Frequency
                                                    Frequency of categorical attribs generally.
                                                    that of numeric thru bins.
                                                MeasureOfLocation
                                                    Mean- most common measure
                                                        subject to outliers (1 bad value skews it completely)
                                                    Median- more immune to outliers.
                                                MeasureOfSpread
                                                    Range- from min to max.
                                                    Standard Deviation, Variance.
                                                        Sensitive to outliers.
                                            </SummaryStatistics>
                                        </StatisticalDescriptors>
                                        <DataVisualization>
                                            Extract (or spot) information from visual patterns
                                                related to descriptive analysis.
                                                UCI ML website has lots of data sets.
                                                Conversion- Data -> Table / Visual Format
                                                Intent- Compress lots of data to "what u see is what we have!"
                                            <KeyElements>
                                                <Selection>
                                                    Which attrib to let go and which 1 to select.
                                                    Suitable attribs for viz
                                                    Selecting subset of objects
                                                        cuz whole may not be possible
                                                        may use sampling
                                                        may use aggregation.
                                                </Selection>
                                                <Arrangement>
                                                    Manipulate the data thru arranging it in different ways for visual hints.
                                                    Eg- Matrix becomes symmetric if sequence of rows, cols metadata is permuted.
                                                    Eg- Using pivot on the 3-D or 4-D data for different types of questions
                                                </Arrangement>
                                                <Representation>
                                                    Data can be n-D but we have only 2-D with us
                                                        Strategy- Visual Cues (aka metaphors)
                                                            viz- shape, size, color, spatial separation
                                                            Eg- Histogram, Bar chart, etc.
                                                        Quantitative -> Categorical (making bins)
                                                        Categorical -> Quantitative (aggregate over count)
                                                    <PieChart>
                                                        when only 1 categorical data + anything (to aggregate over)
                                                    </PieChart>
                                                    <Others>
                                                        <StarPlots>
                                                            size of axes (in 4 directions) determine a quantitative attribute
                                                            overlap them with different colors for analysis over categorical attrib.
                                                            helps model many attribs along on axis.
                                                        </StarPlots>
                                                        <ChernoffFaces>
                                                            Size of different parts of a face to determine an attribute.
                                                        </ChernoffFaces>
                                                    </Others>
                                                    <BarChart>
                                                        used for quantitative vs Categorical Data
                                                            Eg- Sales over months.
                                                        vertical bars for every month
                                                            can be vertical, horizonal, etc
                                                    </BarChart>
                                                    <Histogram>
                                                        vs histogram- used for quantitative data.
                                                        may have to make bins over quantitative data on x-axis.
                                                        Equiwidth bins- same width for all bins.
                                                        Equiheight bins
                                                    </Histogram>
                                                    <ScatterPlot>
                                                        used for Quantitative vs Quantitative data
                                                            Eg- Sales vs Profit.
                                                        Dot represents presence of an entity.
                                                        Matrix of scatter plot- concatenate many diagrams together to have 1 view.
                                                    </ScatterPlot>
                                                    <StackedBarCharts>
                                                        used when categorical data vs categorical data.
                                                            Apply aggregate on 1 attribute
                                                            on the bar chart- create a partition for #items for different categories of 1 attrib.
                                                            Eg- Months vs Products sold (hat, cap)
                                                        used when 2 categorical and 1 quantitative
                                                            replace aggregation axis by the quantitative attributes
                                                            Eg- Month, Products, sales
                                                                draw graph- Month vs sales and patition bars for products.
                                                                could have chosen to partition on month (pivoting)
                                                                can even subdivide partition further.
                                                    </StackedBarCharts>
                                                    <TreeMap>
                                                        Manipulating color for quantitative variable.
                                                        using color scales (shades, range) for ranking along 1 quantitative attribute.
                                                            Eg- may want to show poverty with darker color on maps.
                                                        use area of rectangle to get sales- can also write the numbers
                                                            maintaining same width may give intuition but diagrams are built in.

                                                    </TreeMap>
                                                    <ParetoChart>
                                                        used when 1 categorical and 2 quantitative.
                                                        used to overlapped visuals (maybe thru different colors) or metaphors (line and bars)
                                                            metadata on either side of vertical/ horizonal axis.
                                                            read contextually if interested in only 1.
                                                    </ParetoChart>
                                                    <BoxPlot>
                                                        To show full distribution of data
                                                            no compressing like frequency histograms.
                                                            Eg- score at different percentiles.
                                                        25th Percentile- 25% below me in gate context
                                                            it is the quantitative value (say gate score) below which 25% people lie.
                                                            50th Percentile means median.
                                                        End Posts- ?Viscores
                                                        Height of box (25th to 75th)- Inter Quartile Range (IQR)
                                                            Find outliers, members beyond, 1.5 * IQR from 75th Percentile.
                                                        Width has no representation in tablue but in R
                                                            No of objects in distribution- configurable.
                                                    </BoxPlot>
                                                    <ContourPlots>
                                                        for Geographical Overlaps- color gradients
                                                            Eg- SST- Sea Surface temperature across globe by geographers.
                                                        like election time
                                                        Longitude, Latitude are built-in (city, nation, pin automatic)
                                                        but not necessarily politically correct- tableau for Kashmir- import geographic files
                                                    </ContourPlots>
                                                    <MatrixPlot>
                                                        Stripe shows data point- we can tell what its standard deviation is thru color.
                                                    </MatrixPlot>
                                                </Representation>
                                                <Tools>
                                                    <Tableau>
                                                        currently only for windows
                                                        Data Source can be Relation, CSV, etc.
                                                            divides into dimensions and measures (fact) automatically.
                                                            check if correct interpretation.
                                                            right click to convert betw each other.
                                                        Drag dimensions to rows section and fact attrib to col section
                                                            apply or change aggregate using a drop down for every attrib in the section.
                                                        Drag Product dimension on color icon
                                                        Show me menu- for other charts that are available for data inputs.
                                                        For transposing the graph- change sequence of column attributes.
                                                        Data -> New Data Source - to import data.
                                                        <Notes>
                                                        CS - Tableau
                                                                https://interworks.com/blog/ccapitula/2014/12/09/tableau-essentials-chart-types-box-and-whisker-plot/
                                                                Diagrams
                                                                        Text Table (Crosstab) - Pivot tables
                                                                        Heat Map - same as tables but numbers replaced by color brightness.
                                                                        Highlight Table - numbers and colors
                                                                        Symbol Map - map with bubbles of different size.
                                                                        Filled Map - color the states.
                                                                        Pie Chart - proportion by area in circle.
                                                                        Horizontal Bar Chart - compare heights
                                                                        Stacked Bar Chart - 3D with colors stacked for 3rd dimension
                                                                        Side-by-Side Bar Chart - 1 dimension nested inside another for 3D. OR colored bar for distinction.
                                                                        Treemap - proportion by area in rectangle - color code for size intensity.
                                                                        Circle View - 3D by including color and size of circle for fact table values (even shape is an option)
                                                                        Side-by-Side Circle View - Facts on y-axis ; x-axis and circle colors for predictor variables. Also table like where every cell is a scale from 0 to 100 for different params.
                                                                        Line Charts (Continuous & Discrete) - 2D with facts continuum of change of facts (y-axis) over dimensions. Color code for 3rd dimension.
                                                                        Dual-Line Chart (Non-Synchronized) - 3rd dimension parallel to y-dimension but represented thru different colors for line chart - xy denotes a graph different from xy'
                                                                        Area Charts (Continuous & Discrete) - similar but talks about area.
                                                                        Scatter Plot - 2D points with 3rd dimension thru shapes of crosses.
                                                                        Histogram - vertical bars for 2D including facts. Color code for highlights of numbers.
                                                                        Box-and-Whisker Plot - to study a distribution of a variable - can be shown on 2D matrix for some variables together.
                                                                        Gantt Chart - time fill for a set of dimensions
                                                                        Bullet Graph - distribution showing progress towards a goal behind the bar. Color changes with how much of goal is being achieved denoted by the bar. Vertical bar for 100% goal.
                                                                        Packed Bubbles - Bubbles size in a square to denote a comparison.
                                                                Tryst - not finalized - Categorizations based on dimensions (excluding target or fact variables or measures)
                                                                    0-variables - a number representing your business - KPI.
                                                                        box plot. Bullet graph.
                                                                    viz - Pie chart, Donut chart, TreeMap, Histogram (pareto extension to capture cumulative part also), packed bubbles.
                                                                    Histogram (1 continous and 1 categorical dimension (can create bins to convert))
                                                                    Basis - height (histogram), area (circle, donut), size measure (treemap, pareto).
                                                                    bar chart (2 continuous dimensions)
                                                                    scatter chart (discrete - 2 categorical/continuous dimensions and 1 boolean fact)
                                                                        circle view chart - when >1 boolean fact variables.
                                                                    gantt chart (1 time dimension, 1 categorical dimension with boolean values)
                                                                    area chart - 1 time dimension and n similar continuous dimensions.
                                                                        overlapping bar charts. Convert to bins -> stacked histograms, side by side histograms.
                                                                    dual line charts (3 continuous dimensions)
                                                                    heat map / highlight table - 2 categorical dimensions and 3rd dimension by rectangle color intensity.
                                                                        geographical - 2 geographical axes or dimensions and 3rd dimension anything thru color density.
                                                                        filled map - 2 geographical axes - 3rd boolean non-overlapping dimension - eg states of India.
                                                                        symbol map - 2 categorical dimensions and 3rd continuous dimension by size.
                                                                    side by side circle view - 2 categorical dimension, 1 continuous dimension, distribution shown.
                                                                    Note - Chart is nothing but ability to map or reduce information to 2+ dimension - with other dimensions expressed thru tricks like height, size, partitions, etc.
                                                                Tips
                                                                        Tableau Essentials: Formatting Tips – Introduction
                                                                        Tableau Essentials: Formatting Tips – Custom Shapes
                                                                        Tableau Essentials: Formatting Tips – Labels
                                                                        Tableau Essentials: Formatting Tips – Color
                                                                        Tableau Essentials: Formatting Tips – Tooltips
                                                                        Tableau Essentials: Formatting Tips – Maps
                                                                        Tableau Essentials: Calculated Fields – Introduction
                                                                        Tableau Essentials: Calculated Fields – Logical Functions
                                                                        Tableau Essentials: Calculated Fields – Number Functions
                                                                        Tableau Essentials: Calculated Fields – Date Functions
                                                                        Tableau Essentials: Calculated Fields – String Functions
                                                                        Tableau Essentials: Calculated Fields – Type Conversion
                                                                        Tableau Essentials: Calculated Fields – Aggregate Functions
                                                                        Tableau Essentials: Calculated Fields – User Functions

                                                        </Notes>
                                                    </Tableau>
                                                    <R>
                                                        R: the statistical programming language.
                                                        RStudio
                                                            it's a free, open-source, integrated development environment or IDE for R
                                                            Should install RStudio but don't need to.
                                                                RStudio helps keep R bit more organized
                                                                adds a lot more functionality mainly through menus;
                                                        How
                                                            <Installation>
                                                                    www.r-project.org to download- CRAN link -> choose mirror (nearest) -> choose OS -> download latest version
                                                                        install with default options
                                                                        can see R in applications folder
                                                                        download www.rstudio.com/ide/download- desktop- OS.
                                                                    Installing Packages in R
                                                                            Packages are add-ons that can extend R's functionality and perform specific tasks covering a wide range of modern statistics
                                                                            5000 packages available on R
                                                                            GUI- 
                                                                            CLI- help(install.packages)
                                                                                    install.packages("epiR")
                                                                                    install.packages()- not supported in latest versions
                                                                                    library(epiR)- to make the package commands available
                                                                                    all libraries mentioned on www.r-project.org
                                                                                            mirrors, packages, sorted by name/ date
                                                                                    help(package = epiR)
                                                                                    remove.packages("epiR")
                                                                            GUI- tools -> install packages -> from CRAN -> epiR -> 
                                                            </Installation>    
                                                            <Views>
                                                                <Console>
                                                                    Here you write your commands
                                                                        if incomplete operation- R gives + sign asking for more chars as inputs.
                                                                            arrow up key- showcases commands from history.
                                                                            #this is a comment (useful with lengthy code and notes for what you did)
                                                                    Importing data into R easy- from excel into R (csv, txt)
                                                                            save file as .csv, .txt- former is easy
                                                                                    make an excel sheet but save as .csv
                                                                                    save as .txt (delimited) but format tab delimited- open with excel, text
                                                                            open with -> R, textpad
                                                                            help(read.csv) or ?read.csv
                                                                            data1 <- read.csv(file.choose(), header= T)- menu pops up to choose from
                                                                                    data1 <- read.csv("C:\data", header=TRUE)- header if 1st row is variable names in csv.
                                                                                    data1 <- read.csv("C:\\Users\\Saurabh\\Desktop\\RStudio\\ExcelDataCSV.csv", header=TRUE)
                                                                                    read.table(file.choose(),header=T, sep=",")
                                                                                    read.delim(file.choose(),header=T)- for tab delimited
                                                                                            read.table(file.choose(),header=T, sep="\t")
                                                                    create a vector X
                                                                        x <- 1:5, y <- 6:10
                                                                        x <- 1:5
                                                                        y <- c(1,3,5,7,9) (c for concatenate)
                                                                        x for printing it
                                                                                2:7- prints 2 to 5
                                                                        gender <- c("male","female")
                                                                        seq(from=1, to=7, by=1)
                                                                                seq(from=1, to=7, by=1/3)- but decimal output
                                                                        rep(1, times=10)
                                                                                rep("marin",times=5)
                                                                                rep(1:3, times=5)
                                                                                rep(seq(from=2, to=5, by=0.25), times=5)
                                                                                rep(c("m","f"),times-5)
                                                                        x + 10- adds 10 to every elt of x
                                                                                x-10, x*2, x/3
                                                                        x+y, *,/,-
                                                                                if vectors of same length then operations for corresponding
                                                                        y[3] -- 5 (square brackets to see subset of data)
                                                                                y[-3]- spits all elts but 3rd
                                                                                y[1:3]- first 3 elts
                                                                                y[c(1,5)]- 1st and 5th elt
                                                                                y[-c(1,5)]- all elts but 1st and 5th
                                                                                y[y < 6]- all elts satisfying the condition
                                                                        matrix(c(1,2,3,4,5,6,7,8,9), nrow=3, byrow=TRUE)
                                                                                spits the matrix in matrix form
                                                                                byrow = FALSE- columnwise (transpose)
                                                                                mat <- matrix(c(1,2,3,4,5,6,7,8,9), nrow=3, byrow=TRUE)
                                                                                mat -- prints
                                                                                mat[1,2]- the corresponding elt
                                                                                        mat[c(1,3), 2]- 2nd elt of 1st and 3rd row
                                                                                        mat[2,]- all elts in row 2
                                                                                        mat[,1]- all rows but col 1
                                                                                mat*10- all elts multiplied
                                                                    make a plot
                                                                        plot(x,y)- Y versus X.
                                                                        plot pops up in a separate Graphics/Detailing view
                                                                            may need to resize it
                                                                    Operations
                                                                        11+14 -- 25
                                                                        x -- 11
                                                                        x+y -- 20
                                                                        z <- x+y ; z -- 20
                                                                        x*y, x/y, x^2, x^(1/2) + sqrt(y), 
                                                                        log(y), exp(y)- antilog, log2(y)- log base 2, abs(-14)- 14
                                                                    assigning values to objects
                                                                        x <- 11 or x = 11 (case sensitive, 1x not allowed)
                                                                                x.1 <- 14
                                                                                xx <- "marin" (string type)
                                                                                yy <- "1" (treated as char)
                                                                        print(x) or x
                                                                        x <- 9 (overwritten)    
                                                                        rm(y)- remove y object from workspace.
                                                                    Working with Data in R
                                                                            Import also thru import button- can see in data view pane of RStudio
                                                                            rm(Data1) to remove from workspace
                                                                            dim(LungCapData)- dimensions of data- rows and cols
                                                                            head(LungCapData)- to see first 6 rows
                                                                                    tail(LungCapData)- last 6 rows
                                                                            LungCapData[c(5,6,7,8,9),]- to view specified rows (it's a matrix)
                                                                                    LungCapData[c(5:9),], LungCapData[-(4:722),]
                                                                            names(LungCapData)- all variables involved
                                                                    Working with variables and data
                                                                            mean(Age)- object Age not found- it is stored within LungCapData
                                                                            mean(LungCapData$Age)- $ to extract Age and then find mean
                                                                            attach(LungCapData)- attach to be able to address variables directly- given to workspace
                                                                                    mean(Age) or Age
                                                                                    detach(LungCapData)- it forgets Age.
                                                                            class(Age) or class(Height)- Data type used
                                                                                    Inteer, numeric (decimal), factor(radiobutton type)
                                                                                    levels(Smoke)- enumerate all values of factor variable smoke
                                                                                    convert numeric to factor
                                                                                            x <- c(0,1,1,1,1,0,0,1,0)- summary(x) yields numeric summary
                                                                                            as.factor(x)- start treating x as factor.
                                                                                            class(x), summary(x)
                                                                            summary(LungCapData)- apt summaries for all variables. 
                                                                                    Freq for categories.
                                                                                    Factors- characters or words- used 0,1 for things.
                                                                    Subsetting data with R with square brackets and logic statements
                                                                            length(Age)- number of entries for Age
                                                                            Age[11:14]- subset
                                                                            LungCapData[11:14,]
                                                                            mean age for females- mean(Age[Gender=="female"])
                                                                                    levels(Gender)
                                                                                    FemData <- LungCapData[Gender == "female", ]- all cols included
                                                                                    summary(Gender), FemData[1:4, ], dim(femData)
                                                                            MaleOver15 <- LungCapData[Gender=="male" & Age > 15, ]- ', ' means all cols included
                                                                    Logic Statements (TRUE/ FALSE) and cbind, rbind command
                                                                            Age[1:5]
                                                                            temp <- Age > 15
                                                                            temp [1:5]
                                                                            temp2 <- as.numeric(Age > 15)
                                                                            temp2[1:5]
                                                                            LungCapData[1:5, ]
                                                                            FemSmoke <- Gender == "female" & Smoke == "yes"
                                                                            FemSmoke[1:5]
                                                                            MoreData <- cbind(LungCapData, FemSmoke)	//append column FemSmoke to dataset.
                                                                            rm(list = ls())
                                                                </Console>
                                                                <Internal>
                                                                    R's workspace memory, use LS command- ls()
                                                                    Setting up working directories in R
                                                                            Keeping your work organized- a spot (folder) for saving all of your work- all summaries, code, current workspace, plots, etc
                                                                            getwd()- current working directory
                                                                            setwd()- decide your working directory
                                                                                    setwd("/users/marin")
                                                                            tip useful- projectWD <- "Users/oldMarin"
                                                                                    setwd(projectWD)- useful to rely on code for scripts
                                                                                    OR GUI- session/tool -> set working directory -> choose.
                                                                            MeanAge <- mean(Age)
                                                                            x <- c(1:5)8098 characters, 1416 words, 189 lines
                                                                            y <- 14
                                                                            z = summary(LungCapData)
                                                                            CLI- save.image("FirstProject.Rdata")- workspace image file
                                                                            GUI- Session -> save workspace as
                                                                            rm(list = ls()) OR Session -> Clear workspace
                                                                                    ls()-- character(0)
                                                                            q()- to quit- to save b4 doing neTh.
                                                                            setwd(...), getwd(), load("FirstProject.Rdata")
                                                                                    load(file.choose())
                                                                                    GUI- session -> load workspace
                                                                </Internal>
                                                                <MenuBar>
                                                                menu options- Scroll through
                                                                        notice they are pretty limited
                                                                        if we want to save the plot we can click on this export tab here and then we can
                                                                            save as an image, pdf (give directory, filename)- can change height, width
                                                                        you can also see under the menu here you have this create new project; 
                                                                            It allows you to manage all of your files and output related to a project in one spot. 
                                                                        Customizing The looks of RStudio
                                                                            Tools -> options or Rstudio -> preferences
                                                                            .RData into workspace at startup- for continuity
                                                                            ask for saving b4 exit
                                                                </MenuBar>
                                                                ?1st Quadrant
                                                                        using the import dataset tab, select from a local file saved on PC
                                                                            get a preview down here of what the data is going to look like
                                                                            play around with the data- filter, sort, etc
                                                                <ScriptView>
                                                                Script View- easily create and manage scripts within RStudio
                                                                        if you go here to file and then new file, and new R script; 
                                                                        create and submit a few new commands here. 
                                                                        z <- 11:15 and then sum(x,y,z)- submit those commands- run button.
                                                                        save this file- so that we can reproduce our analysis on any given day; 
                                                                        work with a script directly in R and not RStudio but working this way within RStudio is a lot simpler and more user-friendly
                                                                    More on Scripts
                                                                            useful as they allow 1 to easily pick up where they left off on a project and progressively build and refine code and analysis
                                                                            reproduce analysis that was run earlier
                                                                            Script is a set of commands usually include commenting on what each piece of code is intended to do
                                                                            Copy Paste from the script written in text
                                                                            OR write script written into R
                                                                                    File -> new RScript or open file
                                                                                    all panes are resizable.
                                                                                    Run for line by line or select the executable text
                                                                                            run thru GUI
                                                                                            or command (ctrl)+ Enter(return)
                                                                                            alt + Enter- to re-execute
                                                                                    Edit -> find and replace
                                                                                    code -> comment and uncomment
                                                                                    tab key- suggestions are returned to you
                                                                </ScriptView>
                                                                you can also see here if you create a new file R MarkDown
                                                                        what markdown does is it allows you to embed R code and R output directly into documents, slides, PDF documents, HTML ,word document, and many more
                                                                <DetailedView>
                                                                    Also, Graphics view- shows the descriptive analytics in charts, package details etc.
                                                                </DetailedView>
                                                            </Views>
                                                        Section 2- Descriptive Analytics
                                                        Barcharts and piecharts are apt for summarizing the distribution of a categorical variable
                                                                Barchart is a visual display of the frequency for each category of a categorical variable or the relative frequency (%) for each category
                                                                import and attach LungCapData
                                                                help(barplot) or ?barplot
                                                                count <- table(Gender)
                                                                        relativePercentage <- table(Gender)/ 725 (total count)
                                                                barplot(count)
                                                                        barplot(relativePercentage)
                                                                        barplot(relativePercentage, main="TITLE",ylab="Gender", xlab="%", las=1, names.arg=c("Female","Male"), horiz=TRUE)
                                                                                title to the plot is name, x label, y-label
                                                                                las is to rotate the y-axis values written in horizontal way
                                                                                names.arg to change the labels specifically
                                                                                horiz for horizontal or vertical- transposing it
                                                                pie(count, main="TITLE here")
                                                                box()- putting output in a box.
                                                        Boxplot is apt for summarizing the distribution of a numeric variable
                                                                5 numbers summary- minimum, first quartile, median, second quartile, maximum
                                                                help(boxplot) or ?boxplot
                                                                boxplot(Lungcap)
                                                                quantile(LungCap, probs= c(0,0.25,0.5,0.75,1))
                                                                        main,ylab, las
                                                                        quantile(LungCap, probs= c(0,0.25,0.5,0.75,1), ylim=c(0,16))
                                                                                ylim- for values of y running from 0 thru 16
                                                                st we want to compare distribution of numeric variable for different groups formed by categorical variable
                                                                        boxplot(LungCap ~ Gender)
                                                                        or boxplot(LungCap[Gender=="female"], LungCap[Gender=="male"])
                                                                        main
                                                        Stratified Boxplots are useful for examining the relationship between a categorical variable and a numeric variable, within strata or groups defined by a third categorical variable
                                                                examine the relp between smoking and lung capacity within age group (or age strata)
                                                                AgeGroups <- cut(Age, breaks=c(0,13,15,17,25), labels=c("< 13","14/15","16/17","18+"))
                                                                        AgeGroups[1:5], levels(AgeGroups)
                                                                boxplot(LungCap, ylab="LungCapacity", main="Boxplot of LungCap", las=1)
                                                                        boxplot(LungCap ~ Smoke, ylab="LungCapacity", main="LungCap vsSmokers", las=1)
                                                                        it shows reverse!!- smokers are older than non-smokers, hence bigger bodies and lungs.
                                                                        Smoking Effect on lungs is "confounded" with the age effect
                                                                boxplot(LungCap[Age >= 18] ~ Smoke[Age >= 18], main="LungCap for 18+")
                                                                boxplot(LungCap ~ Smoke * AgeGroups, ylab="LungCapacity",main=LungCap vs Smoke and Age", las=2, col=c(4,2))
                                                                        las=2- vertical labels along x-axis
                                                                        col for color, also col=c("blue","red")
                                                                        box()
                                                                        axis=F
                                                                        axis(2, at=seq(0,20,2), seq=(0,20,2),las=1)
                                                                        axis(1, at=c(1.5,3.5,5.5,7.5), labels=c("< 13","14-15","16-17","18+"))
                                                                        legend(x=5.5,y=4.5, legend=c("Non-Smoke","Smoke"), col=c(4,2), pch=15, cex=0.8)
                                                                Featured Playlist- https://www.youtube.com/user/marinstatlectures/playlists
                                                        Histogram is apt for summarizing the distribution of a numeric variable
                                                                help(hist) or ?hist
                                                                hist(LungCap)
                                                                hist(LungCap, freq=F,prob=T, ylim=c(0,0.2), breaks=7)
                                                                        freq for probability  density
                                                                        breaks for #bins or breaks=c(0,2,4,6,8,..) or breaks=seq(from=0, to=16, by=2)
                                                                        main, xlab, las=1
                                                                lines(density(LungCap), col=2, lwd=3)
                                                                        lwd for thinkness
                                                        Stem and leaf plots are apt for summarizing the distribution of a numeric variable and are most apt for smaller datasets
                                                                femaleLungCap <- LungCap[Gender=="female"]
                                                                help(stem)
                                                                stem(femaleLungCap)
                                                                        smallest lung cap is 0.5 and highest is 13.1
                                                                        stem(femaleLungCap, scale = 2)- 2 decimals entries for every int (< 5 and > 5)
                                                        Stacked Bar Charts, Grouped Bar Charts and Mosaic Plots
                                                                for relp betw 2 categorical variables
                                                                examine relp betw Gender and smoking
                                                                help(barplot)
                                                                Table1 = table(Smoke, Gender)
                                                                barplot(Table1)
                                                                        barplot(Table1, beside=T)
                                                                                move to side
                                                                                conditional probability- given you are female
                                                                                legend.text=T or c("Non-Smoke","Smoke")
                                                                                main, las, xlab, col
                                                                Mosaic plot- 
                                                                        mosaicplot(Table1)
                                                        Scatter Plots
                                                                apt for examining the relp betw 2 numeric variables.
                                                                help(plot) or ?plot
                                                                Pearson's correlation is used to examine the strength of linear relp betw 2 numeric variables.
                                                                cor(Age, Height)- strong correlation
                                                                plot(Age, Height)
                                                                        main, xlab, ylab, xlim, col
                                                                        cex=0.5- point characters made half of original size
                                                                        pch=8- plotting character
                                                                        abline(lm(Height ~ Age))- linear line shows up
                                                                                col=4
                                                                        lines(smooth, spline(Age, Height))
                                                                                lty- line type
                                                                                lwd- width of line
                                                                                lines(smooth, spline(Age, Height), lty=2, lwd=5)
                                                        Produce numeric summaries for categorical and numeric variables
                                                                it is often of interest to quantify the centre of spread of the distribution of a variable.
                                                                categorical: Smoke and Numeric Variable : LungCap
                                                                help(mean) or ?mean
                                                                categorical by frequency
                                                                table(Smoke)
                                                                        table(Smoke)/length(Smoke)
                                                                        table(smoke, Gender)- respective frequency of elts obtained by cross product
                                                                mean(LungCap)
                                                                        mean(LungCap, trim = 0.10) -remove top and bottom 10%
                                                                        median(LungCap)
                                                                        var(LungCap) or sd(LungCap)^2
                                                                        sd(LungCap) or sqrt(var(LungCap))
                                                                        min(LungCap)
                                                                        max(LungCap)
                                                                        range(LungCap)
                                                                        quantile(LungCap, probs=0.90)
                                                                                quantile(LungCap, probs=c(0.20,0.5,0.9,1))
                                                                        sum(LungCap)
                                                                                sum(LungCap)/725
                                                                        cor(LungCap,Age)
                                                                                Pearson by default
                                                                                method= "spearman" to change
                                                                        cov(LungCap, Age) or var(LungCap, Age)
                                                                        summary(LungCap) also summary(Smoke)
                                                                        summary(LungCapData)- for entire data set variables 1 by 1.
                                                        Modifying Plots
                                                                we will stick to scatter plot but it's generic
                                                                help(par) or ?par
                                                                plot(Age, Height, main="Scatterplot")
                                                                        cex=0.5 -- reduces size of points
                                                                        cex.main=2 -- title size
                                                                        cex.lab = 1.5 -- larger labels
                                                                        cex.axis = 0.7 -- 70% of original size
                                                                Font
                                                                        font.main=3 -- italisized font, 4 for bold
                                                                        font.lab=2 -- bold
                                                                        font.axis=3-- italic
                                                                col=5- skyblue
                                                                        col.main=4
                                                                        col.lab=2 for labels
                                                                        col.axis=3-- axis values
                                                                pch
                                                                        plotting character
                                                                        pch=2-- triangle
                                                                        pch="w"- w is used
                                                                        abline(lm(Height~Age))
                                                                                for regressionline
                                                                                linear model that is best fit to guess Age.
                                                                                col=4
                                                                                lty=2
                                                                                lwd=6
                                                                identify genders onthe same curve
                                                                        plot(Age[Gender="male"], Height[Gender="male"], col=4, pch="m")
                                                                        xlab, ylab, main
                                                                        points(Age[Gender="female"], Height[Gender="female"], col=4, pch="m")
                                                                                on top of the plot
                                                                        for multiple plots to existing screens
                                                                                par(mfrow=c(1,2))
                                                                                plot(Age[Gender="male"], Height[Gender="male"], col=4, pch="m")
                                                                                        xlab, ylab, main, xlim, ylim
                                                                                plot(...for females)
                                                                                par(mfrow=c(1,1))
                                                                plot(Age, Height, main="TITLE")
                                                                        axes=F- with axis value labels removes
                                                                        axis(side=1)-- to label bottom of chart
                                                                                at(c(7,12.3, 15),labels=c("sev","mean","15"))
                                                                                you maynot want to do these
                                                                        axis(side=2)-- for labelling left side
                                                                                at(c(55,65,75), labels=c(55,65,75))
                                                                        axis(side=4, at=c(50,60,70), labels=c(50,60,70))-- labels on right side
                                                                box()
                                                        Adding Text to plots in R
                                                                Often you would like to enhance an existing plot by adding some descriptive text to the plot
                                                                help(text) or ?text
                                                                plot(Age, LungCap, main="Scatterplot", las=1)
                                                                        las=1 to have vals of y-axis rotated
                                                                cor(Age, LungCap)
                                                                        text(x=5, y=11, label="r=0.82")- coordinates for text
                                                                                adj=0- where text should get started (centered by default)
                                                                                        1- text finishes there
                                                                        text(x=3.5, y=13, adj=0, label="r=0.82", cex=1, col=4, font=4)
                                                                        abline(h=mean(LungCap),col=2)-- gives the line for mean
                                                                                lwd=2-- double its default width
                                                                        text(x=2.5, y=8.5, adj=0, label="Mean LungCap", cex=0.65, col=2)
                                                                texts in margins
                                                                        mtext(text="r=0.82",side=1)-- bottom text (2,3,4 for left, top, right)
                                                                                adj=1-- right side of axes (0 for left-- 0.75- 75% way from left margin)
                                                        Adding Legends to the plots
                                                                Often 2 or more groups of observations are displayed on a single plot
                                                                help(legend) or ?legend
                                                                plot(Age[Smoke=="no"], LungCap[Smoke="no"], main="LungCap vs Age for Smoke/ Non-Smoke", col=4)
                                                                        xlab, ylab
                                                                        to add additional points- 
                                                                        points(Age[Smoke=="yes"], LungCap[Smoke="yes"], col=2)
                                                                legend(x=3.5, y=14, legend=c("Non-Smoke", "Smoke"), fill=c(4,2))
                                                                        legend for names we want written for 
                                                                        fill for boxes besides those names and drawing
                                                                        plot(Age[Smoke="yes"], LungCap[Smoke=="yes"], col=2, pch=16)
                                                                        points(Age[Smoke="yes"], LungCap[Smoke=="yes"], col=2, pch=17)
                                                                        legend(x=3.5, y=14, legend=c("Non-smoke", "Smoke"), col=c(4,2), pch=c(16,17))
                                                                                bty="n"-- no box (for cleaner look)
                                                                        lines(smooth.spline(Age[Smoke=="no"],LungCap[Smoke=="no"]),col=4, lwd=3)
                                                                                so for yes
                                                                                lty=2-- linetype (dotted)
                                                                        legend(x=3.5, y=14,legend=c("Non-Smoke","Smoke"), col=c(4,2), lty=1, bty="n")
                                                                                legend(..,lwd=3, lty=c(2,3))

                                                    </R>
                                                </Tools>
                                            </KeyElements>
                                        </DataVisualization>
                                    </DataExploration>
                                    <ConfirmatoryAnalytics> But comes under the heading of Data Preparation- so please confirm.
                                        Defn (sisense.com) - Confirmatory Data Analysis is the part where you evaluate your evidence using traditional statistical tools such as significance, inference, and confidence. At this point, you're really challenging your assumptions.
                                            In this way, your confirmatory data analysis is where you put your findings and arguments to trial.
                                            testing, experimenting, hypothesizing, checking, and interrogating both your data and approach
                                            figuring out what to make of the data, establishing the questions you want to ask and how you’re going to frame them (hypothesis testing)
                                            and coming up with the best way to present and manipulate the data you have to draw out those important insights (visualization)
                                        <DataReduction>
                                            why? algos sensitive to size (#data objects =~ #rows)
                                            reqd points increase exponentially to maintain same sampling density.
                                            Strategies
                                                <ReduceAttributes>
                                                    Redundant attrib- remove attribs correlated to another
                                                    Irrelevant attribs- do not add analysis value- remove them- like product Id.
                                                    Eliminate dimensions- Roll up and pivoting
                                                    Group dimensions- merge Jan-Mar as qtr 1 and so on.
                                                    PCA- each data object has numerous attribs.
                                                        PCA transforms data from high dimensions to low- improves computational complexity and improved density.
                                                        take p variables of n data objects and summarize in terms of linear combination of original p variables.
                                                        number of objects remain same but principle componets- less than specified.
                                                        Correlations preserved- inspite of reducing cols
                                                            summarize all p variables (which may be correlated) by uncorrelated axes
                                                        PCA Steps
                                                            Collect Data
                                                            Convert to zero-mean
                                                            Calculate covariance matrix
                                                            Calculate Eigen values and vectors
                                                            Create transformed Dataset
                                                            Restoring original data
                                                        Useful when many numerical attribs with data objects.
                                                        Extensively used in image analysis.
                                                </ReduceAttributes>
                                                <ReduceAttribValues></ReduceAttribValues>
                                                <ReduceDataObjects> Reduce No of rows
                                                    <Sampling>
                                                        helps in reducing the number of objects
                                                        Law of large numbers- sample from population (aka sampled population) s.t representative of entire data set (aka target population).
                                                            representative if it has approx same property of interest as original data set.
                                                            permits us to draw conclusions about a population based on a sample.
                                                        Done for 
                                                            cost- sameple of 1K TV viewers is easy than 100M viewers.
                                                            practicality- performing crash test on every automobile!
                                                        Methods of collecting Data
                                                            Direct Observation- It's like sensors for traffic analysis.
                                                            Experiments- conduct and get readings at specific times and situ.
                                                            Surveys- ask users for opinions.
                                                        Challenges
                                                            Expenses- costly instruments, logistics, travel, etc.
                                                            Time Consuming- underlying process is inherently slow.
                                                                Eg- rainfall data, crop data
                                                            Hazardous- Wildlife data, geophysical data (like volcano)
                                                        <SamplingPlan>
                                                        Sampling Plan- method or procedure for specifying how a sample will be taken from a population.
                                                            Convenience Sampling
                                                                aka availability sampling
                                                                collection from conveniently available members of population
                                                                Eg- asking classmates in algebra class- #jeans studs own.
                                                            Simple Random Sampling- the most common one used.
                                                                selection s.t every possible sample of same size is equally likely to be chosen.
                                                                add RANDOM column- add rand() for all data objects- sort them- pick first N vals.
                                                                eg- average CGPA of studs.
                                                                Eg- Medical researcher asked every 3rd patient (of given disease) for experience.
                                                                Eg- Counselor picks names from association with random numbers in a given range.
                                                            Stratified Random Sampling
                                                                Separate population into mutually exclusive sets or strata
                                                                draw simple random samples from each statum (preferable as per %strata size)
                                                                    Strata- can be gender, age, occupation, combination, etc.
                                                                Use simple random sampling- from strata as per population proportion of each strata.
                                                                Brings heterogeneity in our sample being preserved.
                                                                Eg- soccer coach selects #players from different age groups.
                                                                Eg- High school selects/recruits 50 female teachers and 50 male teachers.
                                                            Cluster Sampling
                                                                simple random sample of groups/ clusters of objects.
                                                                this was used more in "old days"- difficult/ expensive to gather data over population widely dispersed.
                                                                may increase sampling error due to similarities among grouop members.
                                                                Divide Population into multiple clusters
                                                                Randomly pick 2 of the 4 clusters.
                                                                can be extended into multi-stage by sampling again from randomly chosen clusters.
                                                                Eg- metro population using internet- find all cities and pick 3-4 at random
                                                                    assuming they are all similar.
                                                                Eg- interviews all HR personnel in 5 different high tech companies.
                                                            Eg- for most popular cereal, ask every 10th child passing supermarket what his favorite cereal is.
                                                                Random seems good.
                                                            Eg- GPA- random seems good.
                                                            Eg- %ppl using public transport in ECity
                                                                asking ppl sitting next to you- convenience- not good
                                                                on road survey
                                                                filling google form in selected companies- stratified over job timings.
                                                            Eg- average cost of 2 day hospital stay in BLR- 100 hospitals at random.
                                                                maybe create strata over costly areas- randomly select hospitals in the area- cost per person normalized over types of rooms.
                                                        </SamplingPlan>
                                                        Sample Size- Larger the sample size, more accurate we can expect the sample estimates to be.
                                                            Graph- to get 1 object from each of 10 groups- what sample size should I have?
                                                                Probability vs Sample size- S-shaped- > 0.9 after 44.
                                                            More Not always possible - processing/ cost restrictions
                                                        <PotentialErrors>
                                                            Sampling Errors- differences betw sample and population cuz of bad selection.
                                                                it's random and no control over.
                                                                Solution- 
                                                                    take more of samples of sample sample size.
                                                                    increase the sample size.
                                                            Non-Sampling Errors
                                                                more serious- due to 
                                                                mistakes made in acquisition of data
                                                                    incorrect measurements- faulty equipment
                                                                    transcription mistakes
                                                                    misinterpretation of terms => inaccurate recording of data
                                                                    inaccurate responses to sensitive issue questions.
                                                                Selection Bias- Improper selection process- not due to randomness.
                                                                    sampling plan s.t some members can't possibly be selected for inclusion in sample.
                                                                Non-response Errors
                                                                    Validity of the survey- correctness, time relevant.
                                                                    Selection of participants- many didn't respond to survey!
                                                                    Solution- incentivize thru some money, what you see on screen!
                                                                Increasing sample size will not reduce this type of error.
                                                        </PotentialErrors>
                                                    </Sampling>
                                                </ReduceDataObjects>
                                        </DataReduction>
                                        <SuccessQuantification>
                                        How to quantify success of sampling process?
                                            given sample readings- find mean in population with margin error/ confidense interval and confidense level.
                                                statisticians ask a thought experiment- how much would the value of statistic fluctuate if experiment study is repeated over same sample size.
                                                This gives them how much uncertainity is associated with a given statistic.
                                            Inference Types
                                                Estimation
                                                    Approximate value- of a population parameter on basis of sample statistic.
                                                    Sample mean is employed to estimate population mean.
                                                    Experiment- How many heads come up in 100 coin tosses?
                                                        conducted 30,000 times => 30000 results
                                                        Plot on graph -> Bell-shaped curve
                                                        Conclusion- we usually get betw (40,60) heads. Getting 30/70 is less likely.
                                                    Central Limit Theorem- all random samples (each of size n) taken from population
                                                        Mean of pupulation = Mean of sample when repeated.
                                                        Standard deviation of population = that of sample/sqrt(n)
                                                        means will be approximately normally distributed regardless of shape of parent of population.
                                                            normality improves with larger n.
                                                    determine approximate value of a population parameter on basis of sample statistic.
                                                    Types of estimators
                                                        Point estimator- estimating value of unknown parameter using single point or value.
                                                            increased sample size- gets closer to parameter value 
                                                            but it doesn't reflect effects of larger sample sizes.
                                                            Eg- mean income for a batch of students- take n=25, mean income is expected to be 6 Lakh/annum.
                                                        Interval estimator- 
                                                            inference about population thru estimating interval for unknown paramter.
                                                            We say- with some % of certainty that population parameter of interest in betw some lower and upper bounds.
                                                            Eg- mean income for a batch of studs- take n=25, mean income is between 5 to 7 Lakh/annum- with 90%certainity- 0 and 60 Lakh- 100%certainity.
                                                            Confidence Interval Estimator
                                                                Confidence = 1-ALPHA
                                                                Upper/Lower limit = MEAN +/- (CONFIDENCE_LIMIT*StandardDeviation/Sqrt(N))
                                                                    Width = f(confidence_limit, SD, n)
                                                                    confidence limit is aka z-score over ALPHA/2.
                                                                    Greater SD, greater range, lesser confidence over previous range.
                                                                    Greater sample size, narrower the limit- but confidence level remains unchanged(?)
                                                                        we can control the width of interval (for given confidence), by determining sample size 
                                                                            REL necessary for narrow interval.
                                                                        say we want Marginal error (width) +/- 5 from mean, z-score given by confidence, SD being same, find thru substitution in formula.
                                                                Eg- Need to check diameter of trees to estimate harvest- what sample size to ensure 95% confidence in +/- 1 inch.
                                                                Eg- Chemist measuring Boiling Point of a liquid- what is confidence interval at 95% confidence level.
                                                                    we are 95% confident that BP is in the range (100.86, 102.78)
                                                                Graphically- See diagram
                                                                Population mean is unknown quantity
                                                                Increasing confidence level, increases 1-ALPHA, reduces ALPHA/2, increases the z-score, range of possible values.
                                                                With a little lesser confidense, we can provide more narrower, more precise information.
                                                            Standard Error
                                                                Most practical work, the SD of population is not known.
                                                                So, we use estimated standard deviation.
                                                                T-Distribution- degrees of freedom.
                                                                    sample size n has n-1 degrees of freedom.
                                                                    n increases, t gets closer to normal.
                                                                    Range = m +/- (t*SD/sqrt(n))
                                                                        t = (1-C)/2
                                        </SuccessQuantification>
                                    </ConfirmatoryAnalytics>
                                </Purpose>
                                <DataMiningTechniques>
                                        Who- Class Notes (Next- Incorporate complete slides, Sadawi Slides)
                                        Defn of Data mining- implicit previously unknown info
                                        Methods for achieving Purpose - 
                                        TODO - 
                                        Outer Detection - aka Outlier Analysis or Outlier mining
                                            observation of data items in the dataset which do not match an expected pattern or expected behavior
                                            used in domains, such as intrusion, detection, fraud or fault detection
                                        Sequential Patterns
                                        Prediction
                                        <Clustering> Descriptive
                                                defn - identify data that are like each other (understand the differences and similarities betw data objects)
                                                Clustering distinct from grouping (where clause)
                                                    Grouping- based on attribs already known- that's not clustering
                                                    Clustering- a set of clusters. Each one is cluster for a given data set- there can be many clusterings (ways to cluster them)
                                                        it can consider all/some attributes (we don't call them dimensions cuz evolved from AI)
                                                Why Cluster- viewed as analytics activities
                                                        Applications in your project- 
                                                            market segmentation (I could have used grouping- southern, youth markets) but how data objects closely resemble.
                                                            Species identification based on attribute values.
                                                        Clustering output- a set of clusters with a set of objects in each cluster
                                                                put this info to multiple applications
                                                                use different clusters for target operations (Prescriptive analytics)
                                                                    market segmentations, target marketing, etc
                                                                Characterise the clusters (descriptive)
                                                                        1. helps the target operations (helps us understand the subset (cluster) of population. This is more important than clustering itself.)
                                                                            grouped on some notion (implicit)- what are characteristics of clusters (before hand didn't know)
                                                                        2. Clustering could be an approach for sampling, remember.
                                                                            Useful for summarization compression- descriptive (abstraction of cluster)
                                                                            REL 1000 data objects reduced to 20/1 and say they are representative of cluster.
                                                <MeasuringSimilarity>
                                                        What- How to tell similarity between clusters/ data objects
                                                        <NumericMeasures>
                                                                numeric measures of how similar data objects are- higher on scale [0,1] means more similar.
                                                                        Proximity- degree of similarity/dissimilarity.
                                                                        similarity reverse of dissimilarity (s = 1 - d)
                                                                Properties
                                                                        s(p,q)=1 <= p=q
                                                                        s(p,q)=s(q,p)- symmetry
                                                                Similarity between binary vectors (nominal attribs- distance works on ratio, etc)
                                                                        Common situation is object p and q have binary attribs
                                                                        Similarities measured using 
                                                                                SMC (Simple Matching Coefficient) = (M11+M00)/(M01+M10+M11+M00)
                                                                                Jaccard Coefficients, J = M11/(M01+M10+M11)
                                                                                        what is it disregarding- M00- absence of sth common (it's ok, don't add that as similarity)
                                                                                        Used for asymmetric attributes- concerned with presence than absence.
                                                                                        Eg- 2 documents with no terms- doesn't mean both are exactly similar, maybe diverse.
                                                                                        @Lost brothers meeting again- Nagin ka tattoo nahi hai- so we are brothers.
                                                                                        where M00- the number of attribs where p was 0 and q was 1.
                                                                                Cosine Similarity- cos(d1,d2) = d1.d2/|d1|.|d2|
                                                                                        more no of times a word is repeating in 2 documents.
                                                                        CS If talking about clustering of documents- these are widely used in search engines (dealing with nominal data)
                                                                                Map raw data set to one we can do computation on.
                                                                                Prepare data for machine readable- XML/PDF to standards.
                                                                                Business understanding to Data Und (machine readable data + metadata) then Data Prep 
                                                                                        Generic- Cleaning- data quality issues like missing values, inconsistency, etc.
                                                                                        Model Specific- Make data suitable for algorithm- Model prepare, evaluate and repeat
                                                                                                Eg- Euclidean distance then all numeric values and specify clustering based on these attributes.
                                                                                Document Term Matrix- Can't straight away work with HTML documents- convert documents into document term matrix
                                                                                        Formulate as binary document or count of appearances.
                                                                                        Eg- 	Term1	Term2
                                                                                        Doc1	1	0
                                                                                        Doc2	0	0
                                                                                        Doc3	1	1
                                                                                        Term- data, query, normalization (terms in documents)
                                                                                        All open data in form of pdf files- not in position to be used- so come with some mechanism to convert tables in pdf to machine readable tables.
                                                                                                Ppl have create tools to make csv from pdf.
                                                                                        Compare Doc1 and Doc3- Use SMC or J.
                                                        </NumericMeasures>
                                                        <Criteria> of differentiating data objects
                                                                nominal- d = values 0, else 1
                                                                Ordinal- d = (p-q)/(n-1); s= 1-d
                                                                Interval/Ratio- d = |p-q|; s= -d | 1/(1+d) | 1 - ((d - min)/(max-min))
                                                                Multidimensional Numerics- (Distance metrics)
                                                                        Euclidean distance- sqrt(SIGMA(pk-qk)^2) = geometric distance in 2-D
                                                                            In GIS Parlance (geographics)- as the crow flies terminology
                                                                        Minkowski Distance- generalization- rthRoot(SIGMA|pk-qk|^r)
                                                                                r=1 called manhattan distance, L1 Norm, Taxicab. 
                                                                                        In Manhattan- All roads are rectangular there- so from (0,0) to (x,y), it takes x+y distance.
                                                                                        Eg- Hamming distance.
                                                                                r=2- Euclidean (L2 Norm)- as crow flies.
                                                                                r=INFINITY- supremum, Lmax norm, L(INFINITY) norm
                                                                                        most dominant axis travel (like L1) gets its value written
                                                                        Mahalanobis Distance
                                                                                refers to covariance matrix to calculate.
                                                                                takes correlation between attributes into account for distance matrix.
                                                                        Which one to choose
                                                                                It depends- one that gives best results- euclidean to start with.
                                                                                Different clustering based on different distance metric.
                                                                                R & D end up coming with new distance measures.
                                                                Distance metrics properties- euclidean is as follows (metric if these hold)
                                                                        d(p,q) > 0 for all p,q and d(p,q)=0 <= p=q
                                                                        d(p,q)=d(q,p)- symmetry
                                                                        d(p,r) < d(p,q)+d(q,r)- triangle inequality.
                                                        </Criteria>
                                                        <Normalization>
                                                                Involves scaling attribute values so that it falls within a specified range.
                                                                        by using mean, sd- when min and max are unknown or outliers exist.
                                                                Why- with different ranges, avoid biasing the results towards one attrib than other.
                                                                Eg- Emp, salary, age, years of experience
                                                                        E1 looks similar to E3, E3 to E2; E5 to E2
                                                                How
                                                                    Scaling by using mean and standard deviation (useful when min and max are unknown or when there are outliers)
                                                                    Simple scaling by dividing by 1000 (no z-score- could have)
                                                                            euclidean distance is very sensitive to scaling.
                                                        </Normalization>
                                                </MeasuringSimilarity>
                                                <ClusteringStrategies>
                                                        Clustering is a set of clusters
                                                        Partitional Clustering- A division data objects into non-overlapping subsets (cluster) s.t each data objects is in exactly one subset.
                                                        Hierarchical Clustering- A set of nested clusters organized as a hierarchical tree.
                                                                Constructs a tree- larger clusters and within that another cluster and so on.
                                                        Variations
                                                                Non-Exclusive- Points may belong to multiple clusteres or border points vs exclusive.
                                                                Fuzzy- a point belongs to every cluster with some weight betw 0 and 1 (belongs to A with 10% fuzziness, to B with 20%) vs non-fuzzy
                                                                Partial- cluster some of data vs complete
                                                                Heterogeneous- cluster of widely different sizes and shapes and densities vs homongeneous.
                                                                        no of elements in clusters should be this, etc.
                                                </ClusteringStrategies>
                                                <ClusteringIdeas>
                                                Types- Ideas for clustering
                                                        Goal- Well-separated- intracluster distance is low and intercluster is high.
                                                                Distance will all data objects is minimized. Closest to all- we belong together.
                                                        Center based- data object belongs to a cluster depends on proximity to center of cluster 
                                                                centroid- center of cluster (average out all distances)- centroid may have no data object sitting there
                                                                vs medoid- closest data object to centroid- sampling based on cluster- 20 objects to choose from- pick medoid.
                                                                Here we compare only with center.
                                                        Contiguous Cluster- Nearest Neighbor or transitive- compare with nearest neighbor- all belong to same cluster.
                                                                Imagine S shaped clusters, etc- such inherent clusters could not be identified using previous appraoches.
                                                                Fission kinda shapes may not work out well- we need try and error- to find best clustering.
                                                        Density Based- Dense region of points which is separated by low density regions from other regions of high density.
                                                                used when clusters are irregular or intertwined, noise and outliers present.
                                                        We don't always have such pretty pictures on k-planes.
                                                </ClusteringIdeas>
                                                <Algos>
                                                        <KMeans>
                                                        K-means and variants
                                                                Partitional clustering approach
                                                                Each cluster is associated with a centroid- center point (initial choices matter!!)
                                                                Each Point is assigned to the cluster with the closest centroid
                                                                Number of clusters K must be specified
                                                                        We determine thru some ways
                                                                Simple algo- O(nKId)- points, clusters, iterations, attribs.
                                                                        Select K points as the initial centroids
                                                                        repeat
                                                                                Form K clusters by assigning all points to closest centroid
                                                                                Recompute the centroid of each cluster.
                                                                        until The centroids don't change.
                                                                St looks good- so repeat clustering exercise many times.
                                                                Evaluation- SSE (Sum of Squared Error)
                                                                        For each point, the error is the distance to the nearest cluster.
                                                                        To get SSE, we square these errors and sum them.
                                                                        SSE = SIGMA(SIGMA(dist^2 (mi,x) for every data object x and mean (or center) mi in cluster Ci) for every cluster)
                                                                        inceasing K, reduces SSE (high cohesion)
                                                                        (Good clustering with smaller K) can have lower SSE than (poor clustering with higher K)
                                                                        We can use this to rank the clustering- which one was best.
                                                                        And rank the clusters also.
                                                        </KMeans>
                                                        <Hierarchical>
                                                        Hierarchical
                                                                https://www.kdnuggets.com/2019/09/hierarchical-clustering.html
                                                                2 Types
                                                                        Agglomerative- Begin with smallest object and keep combining (find proximity matrix and merge similar ones until biggest cluster)
                                                                                4 methods giving different results, what's sanctity
                                                                                        Min, Max, Group average, ward's method
                                                                                dendogram- height specifies dissimilarity between the points
                                                                                How do you characterize clusters. Campaign or incentive to give or experiment on set of objects- apply any clustering approach.
                                                                                        Use mechanisms to understand what those clusters mean
                                                                                        clustering tells you elements and min, max, average, etc- see them and tell if sth perceptible different exists.
                                                                        Divisive- 
                                                                vs Partition- former makes generalization-specialization taxonomy. Useful for creating some taxonomy.
                                                                        business application- grouping people into different hierarchies.
                                                                Problems and Limitations
                                                                        Once a decision is made to combine two clusters, it can't be undone.
                                                                        No objective function is directly minimized
                                                                        Different schemes have problems with one or more of the following
                                                                                Sensitivity to noise and outliers
                                                                                Difficulty handling different sized clusters and convex shapes
                                                                                Breaking large clusters.
                                                        </Hierarchical>
                                                        <DensityBased>
                                                                Used when areas of higher density separated by that of lower density
                                                                DBSCAN- Eg of density based algo- uses center based approach
                                                                        https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html
                                                                        Tag all the points- core, border, noise
                                                                        Eliminate noise points
                                                                        Perform clustering on the remaining points
                                                                                current_cluster_label = 1;
                                                                                for all core points do
                                                                                        if the core point has no cluster label then
                                                                                                current_cluster_label++;
                                                                                                Label the current core point with cluster label current_cluster_label
                                                                                        end if
                                                                                        for all points in the Eps-neighborhood, except ith the point itself do
                                                                                                if the point does not have a cluster label then
                                                                                                        Label the point with cluster label current_cluster_label
                                                                                                end if
                                                                                        end for
                                                                                end for
                                                                        Situ
                                                                                Resistant to Noise
                                                                                Can handle clusters of different shapes and sizes.
                                                                                Doesn't work well with varying densities.
                                                                defn of density- points within a specified radius (Eps)
                                                                        Determining EPS (Epsilon) and MinPts
                                                                                Idea is that for points in a cluster, their kth nearest neighbors are at roughly the same distance.
                                                                                Noise points have the kth nearest neighbor at farther distance.
                                                                                So, plot sorted distance of every point to its kth nearest neighbor.
                                                                                        like Elbow graph or scree plot for k-means.
                                                                                For what radius, the elbow is shown- for say 4th nearest data object.
                                                                                say 6th you are relaxing the radius so you will end up lower density clusters.
                                                                                too little- bad clustering and too much- bad clustering.
                                                                3 types of points interior of a cluster- iterate thru all points
                                                                        Core point- if it has more than a specified number of points (minpts, say 5) within Eps (say 10)
                                                                        Border point- has fewer than MinPts within Eps but is in the neighborhood of a core point.
                                                                        Noise Point- any point that is not a core point or a border point.
                                                                Some suggest to initialize the base line of hierarchical clustering with the k-means algo.
                                                        </DensityBased>
                                                </Algos>
                                                <ClusterValidity>
                                                        For supervised classification, we have a variety of measures to evaluate how good our model is- accuracy, precision, recall
                                                        For cluster analysis, we want to evaluate "goodness" of resulting clusters
                                                                clusters are in the eye of the beholder
                                                        Why?
                                                                To avoid finding patterns in noise (outliers)
                                                                To compare clustering algorithms
                                                                To compare 2 clusters (sets of clusters)
                                                        Internal mechanism- distance across/within
                                                        External mechanism- People prefer some external identification also to judge your clustering.
                                                                Determining the clustering tendency of a set of data, i.e distinguishing whether non-random structure actually exists in the data.
                                                                Comparing the results of a cluster analysis to externally known results- eg- to externally given class labels.
                                                                        Same adding labels for inherent clusters- credit card customer based on domain, reliable customer, high-value customer.
                                                                Evaluating how well the results of a cluster analysis fit the data w/o reference to external information.
                                                                        use only the data
                                                                        Do points belonging to same cluster share the same external label
                                                                Comparing the results of two different sets of cluster analyses to determine which is better.
                                                                We don't know how external labels are created, manual to start and automated henceforth.
                                                                        now external, it is no more shooting the dark- some approach, target.
                                                        Corrrelation
                                                                Correlation of incidence and proximity matrices for the K-means clusterings of the following 2 data.
                                                                Proximity matrix
                                                                Incidence Matrix- one row and 1 column for each data point
                                                                        associated pair of points BELONGS same cluster => entry is 1.
                                                                        0 otherwise
                                                                        Symmetric so nC2 entries need.
                                                                High Negative values of correlation- points that belong to me don't belong to another cluster.
                                                </ClusterValidity>
                                        </Clustering>
                                        <Classification> Predictive
                                                Classification- 
                                                    ?Predicting value of attribute (class- often characteric) using other attribute values and learning gained using data.
                                                    here also grouping the dataset but here, group them into named subsets ("class") based on subset of attributes called predictors.
                                                    comes under Supervised learning
                                                        correctness can be measured- the predicted attribute is a property- can be known/infered
                                                        I have prior information like trusted customer, high value, etc
                                                        vs Clustering- Preference to classification over external validation in clustering.
                                                Why
                                                    When- Applications- Predicting carcinogenics, credit card transactions as legit or fraud, protein structures classification, news stories categorized into finance, weather, entertainment, sports, etc.
                                                How
                                                    Divide your data into training set (collection of records) and test set.
                                                        each record contains a set of attribs along with class (We know it for training set but not for other set)
                                                        class/target values should be Mutually exclusive and not overlapping.
                                                            Eg- High-spender and low spender vs High spender and gadget lover.
                                                        RScript
                                                            s<- sample(455,410)
                                                            s
                                                            sal_train <- sal[s,]
                                                            sal_test <- sal[-s,]
                                                    Use a Learning Algorithm to make a [class] model to predict value as a fn of values of attribs
                                                        RScript
                                                            Make a model
                                                                dtm <- rpart(Compensation~.,sal_train, method="class") #library(rpart)
                                                            To view it
                                                                prp(dtm) OR 
                                                                rpart.plot(dtm) #library(rpart.plot)
                                                                    rpart.plot(dtm, type=4, extra = 101, tweak = 1.5, fallen.leaves = TRUE)
                                                        <LearningAlgorithms>
                                                            <RuleBasedMethods></RuleBasedMethods>
                                                            <MemoryBasedReasoning></MemoryBasedReasoning>
                                                            <InstanceBasedClassifiers>
                                                                    Rote Learners- Memorizes entire training data and does classification only if data match exactly.
                                                                        no training- just tell the answer. No model on training data.
                                                                        No use encountered by Sir.
                                                                    Nearest Neighbor- Uses k-closest points to perform classification.
                                                                        Basic idea- if it walks like duck, quacks like duck it's probably a duck.
                                                                        Compute distance with k closest neighbors.
                                                                            Set of stored records- Not that no training data reqd but no training phase reqd.
                                                                            Distance metric- Need some notion of distance to tell neighbor
                                                                                globular uses euclidian.
                                                                            value of k is needed- number of nearest neighbors to retrieve.
                                                                                too small => sensitive to noise, outliers.
                                                                                too large => neighborhood may include points from other classes.
                                                                                So trial and error and heuristics method.
                                                                        Process- Use class labels of nearest neighbors to determine class label (majority vote)- if a tie, then go with nearest among.
                                                                            Weight of vote- Weight Factor, w = 1/d^2.
                                                                            Voronoi Diagram (1 nearest neighbor)- takes a diagram- divides into regions. Every plane indicate that all points in the plane are closest to this point.
                                                                            These are lazy learners- price paid at run time for no model is created.
                                                            </InstanceBasedClassifiers>
                                                            <NaiveBayes>
                                                            Naive Bayes- Bayesian Classifier.
                                                                    P(C|E) = P(E|C).P(C)/P(E)
                                                                        or P(C|X) = SIGMA(P(xi|C)).P(C)
                                                                        Eg- P(Yes|Rainy, Mild, Normal) = P(Rainy|Yes).P(Mild|Yes).P(Normal|Yes).P(Yes)- given outlook temperature, humidity as evidence.
                                                                    where C- class, E- evidence
                                                                    P(E|C)- apriori probability, P(C|E)- Posteriori Probability
                                                                    Eg- P(E|C)- P(Sunny|NoPlay), P(C|E)- P(Play|Sunny)
                                                                    RHS is known and LHS is to be found.
                                                                    Gotta find posteriori probability for every class
                                                                            Ei, Ci, P(Ci|E)- from P(E|Ci)- 1 with maximum probability is chosen.
                                                                            P(C1|X) >< ? P(C2|X) OR P(X|C1).P(C1) >< ? P(X|C2).P(C2) OR P(x1x2x3x4|C1)=P(x1|C1).P(x2|C2).P(x3|C3).P(x4|C4)		
                                                            </NaiveBayes>
                                                            <BBN>
                                                            Bayesian Belief Networks
                                                                    Data Prep in Tableau and support for R also
                                                                    Server version of this is also being brought- besides desktop report
                                                                    Model driven- autogenerate programs
                                                                    Last we saw Bayesian trees, evaluation, etc.
                                                                            Predictors and Class Variables
                                                                    Why
                                                                        Some predictors affect values of others
                                                                            What's so naive about naive bayes classifier
                                                                                    Statistical Independence Assumptions- Here statistical independence on predictors. Is it sunny or not, windy or not
                                                                            Eg- Outcome (Class variable)- Winning a match
                                                                                    Evidence- Toss, Decision at toss, Weather performance, Bowling Performance, Batting Perf, Fieldin Perf.
                                                                                    Even Weather can affect Bowling perf (some cricket buffs here), Toss and batting.
                                                                            Here will talk about boolean random variables
                                                                                    but extendible to other random variables too.
                                                                                    Takes value of T/F- Will India win the match? will it rain, will india bat first, not who?
                                                                    How
                                                                    Probabilities, beliefs- that's why BBN
                                                                            P(A=true)- 
                                                                                    Probability- relative freq drawn from observations in experiments
                                                                                    Belief when assigned based on domain knowledge of experts.
                                                                                    all theory on prob relevant to belief
                                                                            Conditional and Joint Probabilities
                                                                                    We create joint probability tables for any number of variables
                                                                                    So 2^n entries for n variables
                                                                            Using Joint Probabilities
                                                                                    spectrum of probabilities can be found out
                                                                                    Eg- P(A=true) = SIGMA(all rows which...)
                                                                                            P(A=t,B=t|C=t)=P(A=t,B=t,C=t)|P(C=t)
                                                                                    Identify Discrete random variables, say C,E1,E2,..
                                                                                    Designate one of them as class variable say C
                                                                                    Build joint probability distribution table
                                                                                            Sad that not used cuz lots of of entries in the table to fill up
                                                                                            2^k for k Boolean Random Variables.
                                                                                            How to use fewer numbers.
                                                                                            Independent Variables- A doesn't influence B (can't predict)
                                                                                                    P(A,B)=P(A).P(B) and P(A|B)=P(A)
                                                                                                    Don't have to assume all variable independence like Naive Bayes.
                                                                                                        rather incorporate knowledge about independence selectively.
                                                                                                        Helps reduce the size of the joint distribution table.
                                                                                                    Naive Bayes and Joint table are 2 extremes- what we want is a balance
                                                                                                            Conditional Independence
                                                                                                                    P(A,B|C)=P(A|C).P(B|C)
                                                                                                                    P(A|B,C)=P(A|C)
                                                                                                            Document this in form of BBN.
                                                                                                                    Nodes denote Discrete Random variables
                                                                                                                    Edges- denote causality or influence
                                                                                                                    link of causality (A->B means A impacts B)
                                                                                                                            It's belief based- by domain experts.
                                                                                                                            causality has correlation has necessary but not sufficient condition- medical does field trials.
                                                                                                                    Make a DAG outta it.
                                                                                                                    Make a table for every incoming edge and node taken together- by domain experts
                                                                                                                            transitive dependence is automatically taken care of.
                                                                                                                    Eg- P(ABCD)=P(A).P(B|A).P(C|B).P(D|B)
                                                                                                                    Eg- fo => lo, do
                                                                                                                            bp=> do and do => hb
                                                                                                                            10 rows instead of 32 rows.
                                                                                                                            P(fo,bp,lo,do,hb)=P(fo).P(bp).P(lo.fo).P(do|fo,bp).P(hb|do)
                                                                                                                    We can compute the joint probability distribution over all Xi in the Network using
                                                                                                                            P(X1=x1,....)=PI(P(X=xi|Parents(Xi)) from i thru n)
                                                                                                                            with new evidence appearing, the network updates (belief values)
                                                                                                                                    computationally expensive though
                                                                                                                            This is more predictive analytics than classification.
                                                                                                                                    cuz no notion of class variable, predictors like decision trees.
                                                                                                                            AI uses BBN for knowledge representation
                                                                                    P(Result=Win|Toss=Win,Bat=First)
                                                            </BBN>
                                                            <DecisionTree>
                                                                Problem of overfitting
                                                                Keep on accumulating past knowledge => It might be look up based on test instances.
                                                            </DecisionTree>
                                                            <EnsembleMethods> Random Forests
                                                                What
                                                                    Construct a set of classifiers from training data and predict class by aggregating predictions by multiple classifiers.
                                                                Why
                                                                    All have different outcomes at times- get best of all worlds.
                                                                How
                                                                    Strategy
                                                                        Create multiple datasets
                                                                        Build multiple classifiers for each
                                                                        Combine classifiers in some form- some decision rule.
                                                                    Why it works- 
                                                                        assume 25 base classifiers with all having error e = 0.35
                                                                        assuming there are independent, 
                                                                        Probability of wrong prediction by ensemble = SIGMA (25Ci.e^i.(1-e)^(25-i) for i from 1/13 to 25) = 0.06
                                                                    Ensemble Strategies
                                                                        Manipulate training set- thru sampling.
                                                                            Bagging- for data- create a sample with replacement of same length, do it multiple times.
                                                                                many rounds and probability distribution, all data will be used up for training.
                                                                                build classifier for each bootstrap sample- decision boundary with only 1 check there.
                                                                                Make table of choice of class for all the values for every round- add thru all rounds
                                                                                if result is +, then 1 else if -, then -1.
                                                                            Boosting
                                                                                An iterative procedure to adaptively change distribution of training data by focusing more on previously misclassified records
                                                                                    Initially, all N records are assigned equal weights
                                                                                    Unlike bagging, weights may change at the end of boosting round
                                                                                        Records that are wrongly classified will have their weights increased
                                                                                            at last, for worst case if 4 is wrongly predicted, there will be only 1 row with all 4s- this will be memorized.
                                                                                        Records that are classified correctly will have their weights decreased
                                                                                    Way of collecting wisdom gained over rounds- maybe more weight to initial rounds.
                                                                        Manipulate input features
                                                                            Random Forest
                                                                                Specific ensemble method designed for decision tree classifiers
                                                                                Combines predictions made by multiple decision trees (e.g., majority voting)
                                                                                Each tree generated based on random input vectors
                                                                                    Random vector can contain all features (but only a subset of objects) as in bagging
                                                                                    Random vector can contain a random subset of features and may be combined with bagging too
                                                                        Manipulate algorithm parameteres- Eg- threshold, simlarity matrix, etc.
                                                                        Detail- First training set and input features, we have algos running, for algorithm parameters, we can write script.
                                                            </EnsembleMethods>
                                                            <LogisticRegression></LogisticRegression>
                                                            <SVM>
                                                            Support Vector Machines
                                                                    Why
                                                                            Some Classifiers (DT, Naive Bayes) suffer curse of dimensionality (which features are useful?)
                                                                            Identifying the right set of features (predictors) is difficult
                                                                            No good way to ensure classifier performs well on unseen data
                                                                    What
                                                                            Makes use of (non-linear) mapping function that transforms from input space to feature space
                                                                            transformed space renders the problem linearly separable
                                                                                    Consider a binary classification problem with N training examples (xi,yi) - xi being vector having all dimensions and yi being class label +/-1
                                                                                    The decision boundary could be written as wx+b=0 with each class being wx+b >< 0
                                                                                            many lines- the best line- this largest margin.
                                                                                            Margin- width that the boundary could be increased by before hitting a datapoint.
                                                                                            Why? misclassification can happen with lesser margin.
                                                                                            Finding those 2 vectors that make a margin called Support Vectors
                                                                                    Non-linear SVM- Datasets that are linearly separatable with some noise works great- what if it's hard
                                                                                            try mapping data to a higher dimensional space.
                                                                                            1-D looking for a point of separation, 2-D looking for a line, 3-D looking for a plane of separation.
                                                                                            The original feature space can always be mapped to some higher dimensional feature space where training set is separable.
                                                                                            PHI: x -> PHI(x). Conceive of transforming points on 2-D forming a circle around origin to distance on 3-D, then plane separator.
                                                                                            useful when it is difficult identify features that are good predictors.
                                                                            SVM discovers the optimum separating hyperplane (which serves as a decision boundary for classification)
                                                            </SVM>
                                                            <NeuralNetworks>
                                                                Ref - Jigsaw
                                                                NN
                                                                Sexiest Job of 21st Century
                                                                Harvard Biz Review - most exciting and high paying job of our time
                                                                How it works
                                                                        Creation of data products
                                                                        Provides actionable information
                                                                        Decision-makers not exposed to data or analytics
                                                                        Buying/Selling strategies for financial institutions
                                                                        A set of actions to improve product yield
                                                                        Steps to improve product marketing
                                                                Mesh of following
                                                                        ML (doesn't take domain knowledge into account)
                                                                                Needs
                                                                                        Maths and Stats knowledge
                                                                                        Hacking Skills
                                                                                Impacted by
                                                                                        Probability & Statistics
                                                                                        Computer Sciences
                                                                                        Computational Biology
                                                                                        Biology
                                                                                        Genetics
                                                                                        Clinical trials
                                                                                Trends
                                                                                        Growing Computing Power
                                                                                        Clusters help customers access enterprise scale hardware
                                                                                What - art of extracting patterns and insights from data
                                                                                        Predicting future
                                                                                        learns from historical data points
                                                                                        Uses mathematical algorithms to make its own judgements
                                                                                        automatestasks that are prone to human errors
                                                                                        Large scale credit card fraud can’t be detected by manual inspection
                                                                                                Credit Card Companies - Mastercard Visa, Amex, 
                                                                                                real time manual check of all real time credit card transactions is not feasible
                                                                                                Identify trends in credit card fraud using historical data
                                                                                                Identify potentially fraudulent transactions
                                                                                Defn - gives computers the ability to learn without being explicitly programmed
                                                                                        computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E
                                                                                Eg
                                                                                        Gmail spam filtering
                                                                                        Product recommendation by ecommerce websites
                                                                                        Handwritten character recognition
                                                                                        Automatic Breaking System (ABS)
                                                                                        Google’s Driverless Cars, Tesla Autopilot
                                                                        Traditional Research
                                                                                Maths and Stats knowledge
                                                                                Substantive Expertize - Improves understanding of complicated black-box mathematical algorithms

                                                        1.2
                                                                image recognition
                                                                        y? predicting which customer will default on their credit card payment has business value
                                                                        How
                                                                        •Once you have a question, relevant data must be gathered
                                                                        •A machine can learn from historical data
                                                                        •Machines require additional data to test how well they have learned
                                                                Categories of Learning
                                                                        Supervised 
                                                                                response column (Class Variable / outcome, dependent variable) with instructional examples - acts as teacher for algorithms
                                                                                applies lessons learned to unseen data
                                                                                Eg - Spam Filtering based on Features / independent variables / Explanatory Variables
                                                                                        Sender in ContactList
                                                                                        Number of special characters
                                                                                        specific words
                                                                                        It's a binary problem - yes / no.
                                                                                Rows used for training.
                                                                                Eg - Credit Default Problem - a new customer - will it default?
                                                                                Types
                                                                                        Classification - Categorical / Discrete Valued response
                                                                                        Regression - Continuous / Real valued Response
                                                                                                so called? a measure of the relation between the mean value of one variable (e.g. output) and corresponding values of other variables (e.g. time and cost).
                                                                        Unsupervised
                                                                                No Teacher or experience to learn from - no response column
                                                                                learns by looking at the features to find meaningful patterns - detect groups based on similarities
                                                                                Eg - group customers into different categories based on information - bill amounts and payment history
                                                                        Reinforcement Learning
                                                                                Behavioural Psychology
                                                                                computer is shown each data point sequentially - it is rewarded or penalized
                                                                                makes many mistakes at first then learns gradually
                                                                                @Baby touches hot kettle - burned, touches again and cries but learns what hot is
                                                                                Eg - Games of logic - involves sequence of decisions to be taken over a period.
                                                                        Deep Learning
                                                                                Computer game beats professional at Go. Deepmind of Google.
                                                        NN - Successful algorithms in image recognition, speech recognition,natural language processing use neural networks
                                                                inspired by how human beings learn
                                                                Eg 
                                                                        Handwritten Character Recognition
                                                                                researchers in Italy
                                                                                Deep Learning also successful
                                                                        Speech Recognition - Recurrent neural networks
                                                                        Self-driving/ autonomous cars
                                                        Drones - learning to find their own way in the world - pizza delivery, ocean navigation
                                                                Inspired by human brain.
                                                                        The brain is a learning machine that can:
                                                                                •Learn to recognize objects in images
                                                                                •Identify bad handwriting
                                                                                •Recognize voicesfrom audio
                                                                        Lobes - Each lobe in turn has multiple subpartseach associated with a set of tasks
                                                                                Lobes have specific jobs assigned
                                                                                Frontal
                                                                                Temporal
                                                                                Parietal
                                                                                Occipital
                                                                        Theory: Human intelligence is based on a single learning algorithm
                                                                                •Auditory cortex learns to process sound same way the visual cortex learns to process images
                                                                Neuron - transmit electricaland chemicalsignals like transmission line
                                                                        like a computational black box - many inputs to an output based on some threshold.
                                                                        Dendrite
                                                                        Neuron
                                                                        Axon
                                                                Decision Surfaces
                                                                        Decision Boundary - 𝑤.𝑥+𝑏=0 or w1x1+w2x2+b=0 for straight line
                                                                        make a line to divide a set of data points in 2 categories
                                                                        Decide the marker of a point based on which side of the line the point falls
                                                                                if >= 0, then mark a cross, else mark a circle.
                                                                        Decision Boundary will be plane for 3-D plots and hyperplane for more.
                                                        Types of Functions
                                                                Step Functions
                                                                        Remains mostly constant
                                                                        Makes abrupt changes in values
                                                                Heaviside Step Function
                                                                        0 when x is negative
                                                                        1 when x is positive or 0
                                                                        The decision boundary rule can be transformed into the Heaviside step function
                                                                                Jump in the function from -1 to 0 is very abrupt
                                                                                Up to 0, the function takes a value 0
                                                                                Then suddenly at 0 it becomes 1
                                                                                We do not want to make abrupt jumps like this
                                                                                We want a smooth transition from negative x values to positive x values
                                                                Sigmoid Functions / Logistic Functions
                                                                        S-shaped curve
                                                                        F(x) = 1 / (1 + e^-x)
                                                                        approaches 0 and 1 for extreme positive and negative values respectively
                                                                        At x equals 0, the function has a value of 0.5
                                                                        known as logistic curves
                                                                        special role in the construction of neural networks
                                                                Logical Classifiers and Perceptrons
                                                                        Binary Classifier
                                                                        has the same form as a linear decision boundary
                                                                        one of the first classifiers taught to students
                                                                        If a logistic classifier doesn’t work, try other advanced algorithms
                                                        Neuron
                                                                Inputs are received through the dendritic tree and computed
                                                                An output is sent along the axon
                                                                if activated based on inputs, signal is sent else it's not (0 or 1)
                                                                based on inputs x1 and x2 - a score is computed
                                                                        weights w1,w2 are assigned to respective inputs.
                                                                        Activation Function, h(x) = score 
                                                                if h(x) >= threshold, output is 1.
                                                                        Linear - h(x) = w.x where b is taken as input of x0 = 1 with weight of b - called bias node.
                                                                        Sigmoid
                                                                                Popular choice 
                                                                                h(x) = 1 / (1+e^-w.x)
                                                                        Popular choice of thresholdfor Logistic Activation Function is 0.5
                                                                        The threshold can be increased or decreased as desired
                                                                        Eg - Sigmoid Function - When x becomes 0.1, 
                                                                                Heaviside - the neuron is activated, at -0.1 it's not cuz of value of function being less or greater than 0.5.
                                                                                Logistic - Produces a continuousreal valued score for the value of x. 0.48 vs 0.52
                                                                                In such cases neither the choice of error measure nor the misclassificationrate work
                                                                                output score has a direct probabilistic interpretation
                                                                                Logistic function is one of many functions in the family of Sigmoid curves
                                                                Brain has 100 Billion Neurons
                                                                        •Interconnections between neurons are extremely complicated
                                                                        •Complicated network within the neurons helps brain learn and process information effectively
                                                                Solution
                                                                        breaking them down into simpler manageable pieces
                                                                        We solve each simple piece
                                                                        bring these solutions together
                                                                single neuron is essentially a linear classifier
                                                                        Single neuron cannot solve this classification problem with no line to separate.
                                                                        but 2 can - target a set of points for grouping - connect them with OR.
                                                                        The result of 2 neurons can be fed to 3rd neuron as inputs.
                                                                        These 2 neurons make hidden layer - both of which get inputs x1 and x2.
                                                                        This is feed forward cuz forward direction.
                                                        Multi-layer networks
                                                                w01[2] - 0th input is fed to 1st neuron in 2nd layer.
                                                                Minimum has 3 layers.
                                                                Dimension = Number of nodes = 3 for 2nd layer. 1 for 3rd being output.
                                                                Notes
                                                                        L layers => L-2 hidden layers
                                                                        d(l) nodes in layer l, 0 <= l <= L.
                                                                        Weight vector, wij[l], i <= d(l-1) & j <= d(l)
                                                        Classifier	
                                                                infinite number of lines is possible
                                                                Error measure: The classifier has committed a bigger error with the red circle
                                                                •Misclassification measure: Both points are equally erroneously placed
                                                                Cost Function for an observation (or data point) 
                                                                        𝐶=−𝑦𝑙𝑜𝑔ℎ(𝑥)−(1−𝑦)𝑙𝑜𝑔(1−ℎ(𝑥))
                                                                        y being response variable, x feature vectors, w weights, h(x) being as stated above
                                                                Optimum classifier: -𝑚𝑖𝑛𝑖𝑚𝑖𝑧𝑒𝐶𝒘𝑤.𝑟.𝑡𝒘
                                                                Multivariate Optimization Function
                                                                        SIGMA (Ci(w)) where i is ith data point.
                                                                        Find those values for the weights for which Function C is minimum
                                                        Gradient Descent Algo
                                                                Partial derivatives are useful for multivariate functions
                                                                measures the change in the value of the function for a small change in the value of that variable, keeping all other variables constant
                                                                𝑓(𝑥1,𝑥2)=20+𝑎𝑥1+𝑏𝑥2^2
                                                                        •𝜕𝑓/𝜕𝑥1=𝑎
                                                                        •𝜕𝑓/𝜕𝑥2=2𝑏𝑥2
                                                                        Gradient of the multivariate function ‘𝑓’
                                                                        •𝛻𝑓=(𝑎 2𝑏𝑥2)^-1
                                                                Initialize 𝒘to some random values
                                                                        •Example: 𝒘=[1,4,7,0,…]
                                                                        •Repeat
                                                                Equivalently w≔𝑤−η𝛻𝐶(η: Learning Rate)
                                                                        iow, 𝑤𝑗≔𝑤𝑗-η(𝜕𝐶(𝑤)/𝜕𝑤𝑗) for all j
                                                                        •Small etavalue=slow process
                                                                        •Large eta value=Algorithm running indefinitely
                                                                •Once initialized, the weights are continuously updated
                                                                •Updating=(Current value of the weights) -(quantity)
                                                                •Repeat these steps until the weights stop experiencing significant change
                                                                •Gradient descent will always find the optimum value
                                                                •Gradient descent can be slow
                                                                •Algorithms could converge slowly, in spite of reasonable learning rate value
                                                                •Alternative: Stochastic Gradient Descent
                                                        𝑥𝑖(𝑙)=𝐹(𝒘𝟏,𝒘𝟐,….𝒘𝒍)
                                                                The gradient of the cost function in a neural network is a long vector
                                                                Computing each partial derivative is not feasible
                                                                So, Backpropagation Deltas
                                                                A new, recursive approach, to computing cost function gradient
                                                                •It’s discovery has reduced computation time
                                                                𝒙𝟏(𝟐)=𝒉(𝒂𝒉𝒃𝒉.)
                                                                𝜕𝐶𝜕𝑤112=𝜕𝐶/𝜕𝑠1(2)𝑋𝜕𝑠1(2)/𝜕𝑤11(2)
                                                                        Three parameters require 3 partial derivatives to calculate the gradient
                                                                        cost function does not directly depend upon the weight but signal.
                                                                        Partial derivative of C wrt signal received (del1[2]) and change of signal wrt weight = x1[1] - at first layer how 
                                                                𝜕𝐶𝜕𝑠1(1)=𝜕𝐶𝜕𝑠1(2)𝑋𝜕𝑠1(2)𝜕𝑥11𝑋𝜕𝑥1(1)𝜕𝑠11
                                                                dC/ds1[1] = d1[1] = d1[2].w11[2].h'(s1[1])
                                                                The deltas in each layer are determined by the deltas in previous layers
                                                                Backpropagation can trace previous deltas all the way back to the first hidden layer
                                                                Algo to find weights
                                                                        •Compute delta in the output layer: 𝜕1(𝐿)
                                                                        •Propagate them backwards starting from the output layer: 𝜕𝑖(𝑙−1)= 𝑗=1𝑑(𝑙)𝜕𝑗(𝑙)𝑋𝜕𝑠𝑗(𝑙)𝜕𝑥𝑖𝑙−1𝑋𝜕𝑥𝑖(𝑙)𝜕𝑠𝑖𝑙−1
                                                                        •Apply gradient descent algorithm to compute optimal parameter values of neural network
                                                                Train
                                                                        Gauge performance once a classifier has been trained
                                                                        Performance determines the quality of the classifier
                                                                        A classifier that performs badly, must be discarded
                                                                        If it performs reasonably well, it can be retained as a final candidate
                                                                Accuracy: The first measure of a binary classifier’s performance is accuracy, i.e., the number of observations a classifier has predicted correctly
                                                                Sensitivity: Measures actual Positives identified as Positives, e.g., the % of cancer cases that were correctly identified in being cancerous
                                                                Specificity: Measures actual Negatives identified as Negatives, e.g., the % healthy cases that were correctly identified as being healthy
                                                                Eg - Cancer Test.
                                                        Confusion Matrix
                                                                                                        Predicted Positives					Predicted Negatives
                                                                Actual Positives		True Positives						False Negatives
                                                                Actual Negatives		False Positives						True Negatives
                                                                Accuracy =  (TP + TN) / Total.
                                                                Sensitivity = TP / Actual Positives
                                                                Specificity = TN / Actual Negatives.
                                                                ROC Curves - to compare performances of 2 classifiers
                                                                        Plot Sensitivity or True Positive Rate vs. (1-Specificity) or False Positive Rate for all values of threshold
                                                                        AUC
                                                                                •Area under curve
                                                                                •Perfect classifier: AUC of 1
                                                                                •Higher is better
                                                                Sampling
                                                                        •Computers utilize sample data to learn algorithm that can predict a specific response
                                                                        •Litmus test: Applying a trained classifier to unseen or non-sample data
                                                                        •E.g.: Utilize a classifier trained on 1000 sample emails, to classify previously unseen data
                                                                        •Viable machine learning models are those that can classify unseen or new data
                                                                The perfect model (100% success rate with training unseen datasets) is a myth
                                                                •Models are subject to a bias-variance trade-off
                                                                To perform well on a training data set
                                                                To generalize unseen data sets
                                                                Bias Variance Trade Off
                                                                        •Bias=How well an algorithm performs on a training set
                                                                        •Variance=How well the algorithm will perform on unseen data
                                                                        The overfitting problem: When a complicated model fits the training data perfectly but does a terrible job predicting new observations
                                                                Generally
                                                                        •A model that overfits sample data have low bias but very high variance
                                                                        •A model that underfits sample data has a high bias (and likely low variance)
                                                                        •A point on the target represents a prediction
                                                                        •Point placement denotes how far we are off with respect to the true value
                                                                                what Bias Variance - 00,01,10,11 mean?
                                                                                model with a low bias and low variance is desirable
                                                                                Always keep the bias-variance trade-off principle in mind when designing a regression or classification model
                                                                                Complex model performs poor on unseen data.
                                                                                thumb rule: Simpler models tend to generalize better
                                                                Split data into 3 parts train, test & validate (80-10-10or 70-20-10)
                                                                        •Estimate model parameters on train
                                                                        Using these calculate error on test
                                                                        Compare errors on test to select final model
                                                                        •Once the final model has been selected, predict the validate split and obtain the error
                                                                        •The error measure computed on the validate split is based on unseen data
                                                                        •The model was trained on the train split and compared with other models on the test split
                                                                        •The data in the validate split is completely new to the model
                                                                        •Therefore, a performance measure on this split functions as a proxy for the model’s real world performance
                                                                        •Other methods of determining error measures that reflect performance on unseen data: K Fold Cross Validation
                                                                        You have a dataset that the model never sees, but whose responses are known
                                                                        generalizes well to unseen data
                                                                        Testing split acts as a proxy for unseen data (80/20 or so)
                                                                                perform exploratory and model traiing on 80%.
                                                                                A model that does well on training and testing splits is expected to have low bias and low variance
                                                                        Testing split results do not reflect an algorithm’s generalization capabilities
                                                                How it works - K-fold cross validation
                                                                        Partition data into K groups
                                                                        Step 1: Model is trained on combined data for 2ndto Kthpartition, and tested on 1stpartition
                                                                        Step 2: Model is trained on combined data for 1stand 3rdtill the Kthpartition, leaving out partition 2
                                                                        Second partition is the testing split
                                                                        The process continues till all partitions have served as testing splits
                                                        Greedy Descent algo variant - adam
                                                        metrics eval - accuracy
                                                            </NeuralNetworks>
                                                        </LearningAlgorithms>
                                                    Run the model on test data to assign a class as accurately as possible.
                                                        p <- predict(dtm, sal_test, type="class")
                                                    Measure the accuracy using test set.
                                                        table(sal_test[,3],p)
                                                    Deduction- Apply model (deployment) for other values.
                                        </Classification>
                                        <Regression> Predictive
                                            Defn - method of identifying and analyzing the relationship between variables (guru99)
                                        </Regression>
                                        <AssociationAnalysis> iiitb. Descriptive
                                                Defn - find the association between two or more Items [for a transaction] (guru99)
                                                <What>
                                                Association Analysis- Concepts and Algos
                                                    Given a set of transactions, find rules that predict the occurrence of an item based on the occurences of other items in the transaction.
                                                    Eg- Market Basket Transactions
                                                    Negative Association Rules- on symmetric binary variables (Symmetric- interested in Y and N both)
                                                        Infrequent patterns- maybe interesting          
                                                            Useful for following
                                                                Competing items- Eg- {Buys_DVD, Buys_VCR}, {Buys_Desktop, Buys_Laptop}, {Buys_in_Flipkart, Buys_in_Amazon}- are competetor sets.
                                                                Exceptional situations
                                                                Rare events- Spotting rare events- Fire- yes, alarm- on.
                                                            How- challenge
                                                                infrequent are enormous in number, how to identify interesting ones and effeciently.
                                                        Negative patterns- Definition similar to an association rule except that the rule is derived from a negative itemset. DVD => !VCR
                                                            Now, for all items, we need to add its negative counterpart- thus, making lattice larger.
                                                            Negative itemset- if contains >=1 negative items.
                                                        Negatively correlated patterns- s(X) < s(x1) x s(x2) x … x s(xk)- smaller the s(X) compared to RHS, more negative correlation.
                                                            Eg- When will the following be true? s(DVD,VCR) < s(DVD) x s(VCR)
                                                        Negative association rules
                                                            For a rule X  Y (partial condition)
                                                                    s(X U Y) < s(X)*s(Y)
                                                            For a rule X  Y (full condition) 
                                                                    s(XUY) < ∏s(xi) ∏s(yi) 
                                                            Another defn- s(X U Y).s(X' U Y') < s(X U Y').s(X' U Y)
                                                                Eg- 
                                                                        Distinction     NoDist
                                                                Rural   0.01            0.55
                                                                !R      0.03            0.41
                                                                    0.01*0.44 < 0.55*0.03, so negatively pattern.
                                                            Compare full condition vs partial condition for the following rule
                                                            {eyeglass, lens cleaner}  
                                                                    {contact lens, saline solution}
                                                        Why
                                                            Generally arules for presence and not absence.
                                                            so, rarely found patterns are uninteresting. Support pruning eliminates them.
                                                </What>
                                                <Why>
                                                    Peep into basket/kart of every person and make prescriptions
                                                        REL Keep beer and diapers together.
                                                        Big competition of using shelf space.
                                                    When
                                                    Descriptive- Generation of patterns is describing data to you in some way. Some form of compaction.
                                                        In descriptive is run in non-restrictive mode- no lhs, rhs- let all come.
                                                    Predictive- what you predict and how you predict.
                                                        LHS (Predictors) and RHS (Target)- people when purchase LHS tend to purchase RHS as well.
                                                        I want to evaluate- Confidence- Any rules that predict with given inputs (70% confidence or sth like that)
                                                        Keep threshold, sort them and lift as third parameter and use for prediction- then confidence.
                                                        Rows in 10,000s then even 0.01 support is good and reasonable. confidence be 50%
                                                    Prescriptive- X => Y
                                                        More of business decision but Prescriptive- system should tell you what to do
                                                        shelf eg has human intervention.
                                                        Can I some come up with what value of discount to use? It will require a mathematical model maybe based on it.
                                                        We need to maintain another layer on top of it to make it prescriptive.
                                                        Domain knowledge can be embedded in layer on top. AI, etc are all about encoding domain knowledge.
                                                    Exploratory- is gonna tell me previously unknown relationships.
                                                        Hypothesis testing can used here.
                                                    Association Rules for Clustering
                                                        Clustering tells what objects belong to each of my clusters.
                                                        Cluster characterization- what these clusters mean. No grouping!
                                                            statistical summary or features (median, mode). Include even those features which are not included.
                                                            Use association rules on clusters- descriptive analytics very useful.
                                                    Association Rules for Classification
                                                        Decision Trees- Paths- Rules should corroborate to that- compare to validate.
                                                </Why>
                                                <How>
                                                        <Scenario>
                                                        Scenario (5 karts carrying 5 Products)
                                                            <Data>
                                                                1- Bread, Milk
                                                                2- Bread, Diaper, Beer, Eggs
                                                                3- Milk, Diaper, Beer, Coke
                                                                4- Bread, Milk, Diaper, Beer
                                                                5- Bread, Milk, Diaper, Coke
                                                            </Data>
                                                            <Understanding>
                                                                Here it's not table structure but TID, cartId, etc
                                                                Product vs Item vs Brand- Product is generic term and item is an instance of it. 
                                                                    Eg- Bread1, Bread2 are 2 items and Bread is product. Bready is brand.
                                                                Quantity, price can be interesting to put but classic doesn't use them.
                                                                Master list of items is rather large- items sold (set of sold items)
                                                            </Understanding>
                                                            <Analysis>
                                                            Terms relating Frequent Itemset
                                                                Item set- a set/collection of 1 or more items.
                                                                        Eg- {Milk, Bread, Diaper}
                                                                k-Itemset- an itemset that contains k-items
                                                                Support Count (sigma)- Frequency of occurrence of an itemset
                                                                        Eg- sigma({Milk, Bread, Diaper})=2
                                                                        how many baskets did this appear.
                                                                Support- Fraction of transactions that contain an itemset
                                                                        Eg- s({Milk, Bread, Diaper})=2/5
                                                                Frequent Itemset (cornerstone of it)- An itemset whose support >= minsup threshold
                                                                        2^n - 1 itemsets are possible- scan thru DB and maintain count of occurrence
                                                                                not all of interest to me- exponential- 1000s of items.
                                                                Implication/ Rule- 
                                                            </Analysis>
                                                            <SampleResult>
                                                            Eg of Association Rule- 
                                                                Diaper -> Beer (many theories but not applicable to indian context)
                                                                Milk, Bread -> Eggs, Coke
                                                                Beer, Bread -> Milk
                                                            </SampleResult>
                                                            <Use>
                                                                Diapers and Beer were stocked together to increase their sales.
                                                                Implication means co-occurence not causality -> if this happens this also happens (many that don't in real world)     
                                                            </Use>
                                                        </Scenario>
                                                        <PatternEvaluation>
                                                                (For X -> Y)
                                                                Association rule algorithms tend to produce too many rules 
                                                                        many of them are uninteresting or redundant
                                                                        Redundant if {A,B,C} => {D} and {A,B} => {D} have same support & confidence
                                                                Interestingness measures can be used to prune/rank the derived patterns
                                                                        Interestingness can be obtained from a contingency table
                                                                            Defines Support, confidence, lift, Gini, J-measure, etc.
                                                                <Support>
                                                                Support (s)- Fraction of transactions that contain both X and Y.
                                                                        tells frequent itemsets.
                                                                </Support>
                                                                <Confidence>
                                                                Confidence (c)- Measures how often items in Y appear in transactions that contain X (Conditional Probability)
                                                                    Implication- gives lhs, rhs to association rule;
                                                                        1-way- (x => y) !=> (y => x) has support but does it have sufficient confidence. 
                                                                        ?Transitivity also doesn't hold
                                                                        x => yz !=> x=>y and x=>z but togetherness. Little mathematical rules.
                                                                    Drawbacks of confidence
                                                                        Confidence= P(Coffee|Tea) = 0.75
                                                                        but P(Coffee) = 0.9
                                                                         Although confidence is high, rule is misleading
                                                                         P(Coffee|Tea) = 0.9375
                                                                        Tea drinker will drink coffee is quite lower to coffee drinker in population.
                                                                        So, increasing Tea sales, may not help coffee sales. These rules talk about co-occurrence not correlation.
                                                                </Confidence>
                                                                <StatisticalDependence>
                                                                    Population of 1000 students
                                                                        600 students know how to swim (S)
                                                                        700 students know how to bike (B)
                                                                        420 students know how to swim and bike (S,B)
                                                                        P(S,B) = 420/1000 = 0.42
                                                                        P(S) x P(B) = 0.6 x 0.7 = 0.42
                                                                        P(S,B) = P(S)x P(B) => Statistical independence
                                                                        P(S,B) > P(S)x P(B) => Positively correlated
                                                                        P(S,B) < P(S)x P(B) => Negatively correlated
                                                                </StatisticalDependence>
                                                                <StatisticalBasedMeasures>
                                                                        What- Statistical dependence also taken into account
                                                                        <Viz>
                                                                        Lift = P(Y|X)/ P(Y) = Confidence/ P(Y) = Support(X,Y)/(P(X).P(Y))
                                                                                Greater the lift, greater the interestingness of the rule.
                                                                                Support and confidence only used for filtering (pruning)
                                                                                For coffee eg, Lift = 0.75/0.9= 0.8333 (< 1, therefore is negatively associated)
                                                                        Interest = P(X,Y)/(P(X).P(Y)) = Support/(Px.Py)
                                                                        PS = P(X,Y) - P(X).P(Y)
                                                                        PHI-Coefficient = (P(X,Y)-P(X).P(Y))/sqrt(P(X)(1-P(X)).P(Y).(1-P(Y)))
                                                                        </Viz>
                                                                </StatisticalBasedMeasures>
                                                                <Eg>
                                                                Item Set of a rule- Milk, Diaper -> Beer is {Milk, Diaper, Beer}
                                                                        s = sigma(Milk, Diaper, Beer)/T = 2/5 = 0.4
                                                                        c = sigma(Milk, Diaper, Beer)/sigma(Milk, Diaper)=2/3=0.67
                                                                </Eg>
                                                        </PatternEvaluation>
                                                        <How>
                                                            <Strategy>
                                                            Goal- we need to decouple the support and confidence requirements
                                                                why- Rules originating from the same itemset are binary partitions of the same itemset
                                                                    And have identical support but can have different confidence
                                                            </Strategy>
                                                            <FrequentItemsetGeneration>
                                                            1. Frequent Itemset generation- s.t support >= minSup threshold
                                                                <BruteForce>
                                                                Brute Force is costly- Why- #Itemsets/Unique transactions = 2^n-1 for n products
                                                                        Frequent itemset generation is still computationally expensive
                                                                        Brute Force Approach
                                                                                List all possible association rules
                                                                                Compute the support and confidence for each rule
                                                                                Prune rules that fail the minsup and minconf thresholds
                                                                                Computationally prohibitive!
                                                                                Match each transaction against every candidate- O(NMw) where M=2^d.
                                                                                        Given d unique items
                                                                                        Total #itemset = 2^d
                                                                                        Total number of association rules, R = SIGMA(dCk . SIGMA(((d-k)Cj) j from 1 to d-k) k from 1 to d-1)
                                                                                                = 3^d-2^(d+1)+1
                                                                                                if d=6, R=602
                                                                </BruteForce>
                                                                <FrequentItemsetGenerationStrategies>
                                                                        <ReduceCandidateItemSets>
                                                                        Reduce the number of candidates (M)
                                                                                Complete search: M=2^d for d Products
                                                                                Use pruning techniques to reduce M
                                                                                Algos- Apriori principle:
                                                                                        What
                                                                                            If an itemset is frequent, then all of its subsets must also be frequent
                                                                                            Conversely, if an itemset is infrequent, then all of its supersets are also infrequent
                                                                                            Anti-monotone property of support
                                                                                                    Support of an itemset never exceeds the support of its subsets
                                                                                                    For all X,Y: X SUBSEToF Y => s(X) >= s(Y)
                                                                                                    support distribution of items sorted is quite skewed.
                                                                                        Why [not]
                                                                                                REL only frequent items (like wheat,rice) will get into rules generation and others shaving kit, etc- may not even be considered.
                                                                                            Value of MinSup
                                                                                                If minsup is set too high, we could miss itemsets involving interesting rare items (e.g., expensive products)
                                                                                                If minsup is set too low, it is computationally expensive and the number of itemsets is very large
                                                                                                Solution- Specify Minsup for items individually or at least rare items
                                                                                                        MS(i): minimum support for item i 
                                                                                                        e.g.:     MS(Milk)=5%,   	    MS(Coke) = 3%, MS(Broccoli)=0.1%, MS(Salmon)=0.5%
                                                                                                                MS({Milk, Broccoli}) = min (MS(Milk), MS(Broccoli))= 0.1%
                                                                                                        Challenge: Support is no longer anti-monotone
                                                                                                          Suppose: 	Support(Milk, Coke) = 1.5% and		Support(Milk, Coke, Broccoli) = 0.5%
                                                                                                          {Milk,Coke} is infrequent but {Milk,Coke,Broccoli} is frequent. We would have rejected going further.
                                                                                Compacting Frequent Itemsets- 
                                                                                        Many are redundant 
                                                                                            It's redundant when has identical support for super set.
                                                                                            eg- Teenage Group, Action Film has same support as Teen,Male,Action
                                                                                            Need a compact representation.
                                                                                        MaximalFrequentItemset- if none of its supersets are frequent. AD was frequent but no AD+ was.
                                                                                            REL see lattice- with 1 item- mark all TIDs for the node- for lower layers- just take intersection. Cardinality is support.
                                                                                            These MFI are compacting mechanism to represent rest of the tree- refer to lattice.
                                                                                        ClosedItemset- if none of its immediate supersets have same support (Functional Dependence doesn't exist)
                                                                                            Identify itemsets which are both maximal and closed.
                                                                                            But problem is lattice making is exponential operation grows with #items- so need to do sth smart.
                                                                                            Consider a table where when As are purchased B and C are not and vv. Eg- Electronic items, foods compatible, sports related (badminton- rackets with shuttle and net), etc.
                                                                                            All of these will be removed from closedfrequent itemset.
                                                                                <FactorsAffectingComplexity>
                                                                                        Choice of minimum support threshold
                                                                                         lowering support threshold results in more frequent itemsets- lots of rules but creating confusion and more time.
                                                                                         this may increase number of candidates and max length of frequent itemsets
                                                                                         high minSup => lesser itemsets => lesser rules (maybe none)
                                                                                        Dimensionality (number of items) of the data set
                                                                                         more space is needed to store support count of each item
                                                                                         if number of frequent items also increases, both computation and I/O costs may also increase
                                                                                         Eg- Let me do market basket analysis of electronics only. But may end up losing rule of Diaper => Beer.
                                                                                        Size of database
                                                                                         since Apriori makes multiple passes, run time of algorithm may increase with number of transactions
                                                                                        Average transaction width
                                                                                         transaction width increases with denser data sets
                                                                                         This may increase max length of frequent itemsets and traversals of hash tree (number of subsets in a transaction increases with its width)
                                                                                         Eg- can't randomly reduce items from basket from 20 to 10.
                                                                                </FactorsAffectingComplexity>
                                                                        </ReduceCandidateItemSets>
                                                                        <ReduceTransactions>
                                                                        Reduce the number of transactions (N)
                                                                                Reduce size of N as the size of itemset increases
                                                                                Used by DHP and vertical-based mining algorithms
                                                                        </ReduceTransactions>
                                                                        <ReduceComparisons>
                                                                        Reduce the number of comparisons (NM)
                                                                                Use efficient data structures to store the candidates or transactions
                                                                                No need to match every candidate against every transaction
                                                                                Can I avoid certain candidate itemsets here (other than apriori)
                                                                                Hashing 
                                                                                    What- DS used to efficiently represent all candidate itemsets.
                                                                                    Why
                                                                                        Reduce number of comparisons.
                                                                                    How
                                                                                        Make Hash Tree of all candidate itemsets
                                                                                            Pick first element of itemset and choose its bucket- Hash Function- mod n.
                                                                                            Repeat the process for each bucket considering the next element of itemsets until its exhausted.
                                                                                        Take a transaction- make it traverse downwards- considering whether there is a possibility of the next bucket.
                                                                                            This makes our candidate itemset smaller. Now, we can compare and tell.
                                                                                            carry out for all transactions to finally get support count for each node.

                                                                        </ReduceComparisons>
                                                                </FrequentItemsetGenerationStrategies>
                                                                <FPTrees>
                                                                    Another representation- FP (Frequent Pattern) Tree- apriori uses generate and test strategy- for support.
                                                                        FP Tree uses different approach- Generate all and then use DAC to mine frequent itemsets.
                                                                        I want to reduce no of scans thru DB => Compact DS.
                                                                        For all TID- list frequent items in descending order
                                                                        Organize all the strings generated in the form of ?Tries- s.t path represents transaction and all nodes have their support
                                                                            When items repeat in the tree- a linked list is used to keep them connected.
                                                                            Keep adding transactions to the tree one by one- and update the count of support for nodes.
                                                                            This tree represents all the set of transactions of my DB. 20 items => O(20) nodes.
                                                                        Finding Frequent Patterns in FP Tree
                                                                            List all the items (N)- Remove items from last 1 by 1 (if > minSup) to generate Freq Patterns iterate.
                                                                            Regenerate the FP trees with only prefix (excluding chosen item), update the count of each node.
                                                                            Recurse this process until no item remains and make a list of all itemsets with > minSup
                                                                </FPTrees>
                                                            </FrequentItemsetGeneration>    
                                                            <RuleGeneration>
                                                            2. Rule Generation- confidence >= minconf threshold
                                                                Brute Force can be really costly- for an itemset #Rules = 2^|itemset| - 2
                                                                    For each frequent itemset f, generate all nonempty subsets of f.
                                                                    For every nonempty subset s of f, output the rule “s --> (f - s)“
                                                                    Compute the confidence for each rule
                                                                Prune based on minconf- Confidence based Pruning- those generated from same itemset.
                                                                        If a rule X --> (Y – X) does not satisfy the confidence threshold, then any rule X’ --> (Y – X’) will not, where X’ SUBSEToF X
                                                                        Proof: Compare Sup(Y)/Sup(X) and Sup(Y)/Sup(X’)
                                                                        Discussion- 
                                                                                confidence does not have an anti-monotone property
                                                                                        c(ABC => D) can be larger or smaller than c(AB => D)
                                                                                But confidence of rules generated from the same itemset has an anti-monotone property
                                                                                        Confidence is anti-monotone w.r.t. number of items on the RHS of the rule
                                                                                        e.g., L = {A,B,C,D}:c(ABC => D) >= c(AB => CD) >= c(A => BCD)
                                                                        We can draw a lattice of rules for each itemset and prune
                                                                Eg- Frequent Itemset = {Bread, Milk, Diaper}
                                                                        Candidate Rules
                                                                        {Bread, Milk} --> {Diaper} (Confidence =  2/3 )
                                                                        {Bread, Diaper} --> {Milk} (Confidence = 2/3 )
                                                                        {Milk, Diaper} --> {Bread} (Confidence = 2/3 )
                                                                        {Diaper} --> {Bread, Milk}  (Confidence =  2/4)
                                                                        {Milk} --> {Bread, Diaper}  (Confidence = 2/4)
                                                                        {Bread} --> {Milk, Diaper} (Confidence = 2/4)
                                                            </RuleGeneration>
                                                        </How>
                                                Uncat
                                                        Redundant Rules
                                                                Consider the following rules:
                                                                        R1: Teen_Age_Group ? Action_Film
                                                                        R2: Teen_Age_Group, Male ? Action_Film
                                                                Rule R2 (X’?Y’) can be considered to be a redundant rule with respect to R1 (X?Y) if the following conditions are satisfied:
                                                                Y’ = Y (i.e., consequents are same)
                                                                X’ is subsumed by X (e.g., X’ superset-of X)
                                                                R2 is “inferior” (e.g., Lift(R2) <= Lift(R1))
                                                        Subjective Interestingness Measure
                                                                Objective measure: 
                                                                Rank patterns based on statistics computed from data
                                                                e.g., 21 measures of association (support, confidence, Laplace, Gini, mutual information, Jaccard, etc).

                                                                Subjective measure:
                                                                Rank patterns according to user’s interpretation
                                                                 A pattern is subjectively interesting if it contradicts the   expectation of a user (Silberschatz & Tuzhilin)
                                                                 A pattern is subjectively interesting if it is actionable   (Silberschatz & Tuzhilin)
                                                        Interestingness via Unexpectedness
                                                                Need to model expectation of users (domain knowledge)
                                                                Need to combine expectation of users with evidence from data (i.e., extracted patterns)
                                                        Summary
                                                                Association analysis is an unsupervised learning technique that helps extract interesting patterns in data
                                                                Generally suited for categorical data
                                                                Use support and confidence to fine tune the number of rules generated
                                                                Use Lift (or any other additional measure) to represent interestingness of rules
                                                </How>
                                        </AssociationAnalysis>
                                        <TimeSeries> ?aka Sequential Patterns
                                            Sequential Patterns - discover or identify similar patterns or trends in transaction data for certain period
                                            Time series is a collection of data objects that is collected over a period of time (generally at fixed intervals)
                                            If the data object contains one attribute, it is referred to as univariate time series else it is multivariate time series
                                            If time is modeled as a continuous variable, continuous time series else it is a discrete time series
                                            Eg- Quarterly earning per share (see its growing), Global temperature (see its growing overall but fluctuates for sure with specific period)
                                            Eg- Share value- it has seen strong irregularities some time.
                                            Components
                                                Y(t) = T(t) + S(t) + C(t) + I(t)
                                                Trend- There is a regularity of change.
                                                    Some types of data tends to increase, decrease or remain unchanged over a period of time
                                                    Long term movement of data in a time series is referred to as TREND
                                                    Eg- City population (upward trend), mortality (downward trend)
                                                Seasonality
                                                    Fluctuations observed within a year during a season
                                                    Seasons may repeat each year but not necessarily at the same points in time
                                                    Eg- Weather, commerce, sports, etc.
                                                Cyclicity
                                                    Some time series may show tendency to repeat itself over a longer period of time
                                                    That is, there could be combination of trend and seasonal data that repeats over a longer period of time
                                                    Eg- Economic phases- Prosperity, Decline, Recession, Recovery, repeat..
                                                Irregular
                                                    Randomness in data 
                                                    Generally does not repeat itself (at least not predictably)
                                            Why
                                                Building models of time series data for the purpose of:
                                                    Understanding what happened in the past
                                                    Predicting (forecasting) what will happen in the future
                                            Predicting
                                                Stochastic Process (Random prob distribution/pattern can be analyzed statistically but but not predicted precise)
                                                Smoothening
                                                    Analysis of time series usually involves smoothening of the time series curve
                                                    Smoothening essentially involves reducing the number of points thus reducing variability
                                                Analysis Approaches
                                                    Mean of the whole series
                                                    Linear regression of the whole series
                                                    Mean of subset of the series (moving average – MA)- order 12 means check 12 points on both sides to compute.
                                                    Linear regression of a subset of the series (auto-regression – AR)
                                                    Combination of AR, MA (ARMA, ARIMA, etc.)
                                        </TimeSeries>
                                        <ReferenceSadawiSlides>
                                            Data mining is about data analysis to
                                                    explaining the past
                                                            many biz store large data over years of operation- extract useful valuable insights
                                                            leverage extracted knowledge into more sales (profits) and clients
                                                    Predicting the future by means of modeling
                                                    combines stats, ML/AI, DBMS/DW
                                                            stats- science of collecting, classifying/ summarizing, analysing, interpreting data.
                                                            AI- study of algos relating simulation of intel behaviors
                                                            ML- study of algos to learn/improve thru experience.
                                                            DB- tech of collect, store, manage- so that users can CRUD.
                                                            DW- tech to support decision making processes from data.
                                                    its applications are highly valued
                                            DataAnalysisAndExploration
                                            Types
                                                    Supervised
                                                            classification- predicted value of categorical variable (target/class)
                                                                    How- Model is built based on predictors/features/attribute variables (numeric/categorical type) 
                                                                            Remember- transforming attributes 
                                                                                    Binning/Discretization- numeric to cat
                                                                                    Encoding/Continuization- cat to num
                                                                            Eg- fig1- which is best predictor?
                                                                    Frequency Tables
                                                                            ZeroR
                                                                                    simplest classification method- relies on target and ignores all predictors
                                                                                            predicts the majority category/class.
                                                                                    why- acts as baseline performance benchmark.
                                                                                    How- frequency table for the target and select most frequent value.
                                                                                            PlayGolf- Yes=9 and No=5, so Predict Yes
                                                                                            Accuracy=0.64
                                                                            OneR
                                                                                    One Rule- simple yet accurate
                                                                                            predict based on one feature
                                                                                    Why
                                                                                            shown to produce rules only slightly less accurate than state-of-art.
                                                                                                    Confusion matrix shows significant predictability power
                                                                                                    doesn't generate score/probability => Gain, Lift, KS, ROC charts are not applicable.
                                                                                            rules are simple for human interpretation
                                                                                    How
                                                                                            one rule for each predictor
                                                                                                    construct frequency table for each predictor against target.
                                                                                                    Algo-
                                                                                                            for each predictor,
                                                                                                                    for each value of predictor, make a rule
                                                                                                                            count frequency of all class values
                                                                                                                            find most frequent class
                                                                                                                            Assign that class to this value of predictor
                                                                                                                    find total error of the rules
                                                                                                            choose predictor with least total error.
                                                                                            select the rule with smallest total error
                                                                            Naive Bayesian
                                                                                    based on bayes' theorem- with independent assumptions between predictors
                                                                                    why
                                                                                            simple, easy to build- no complicated iterative parameter estimation => useful for large datasets.
                                                                                            does surprisingly well. often outperforms more sophisticated classification methods.
                                                                                    how
                                                                                            concept
                                                                                                    class conditional independence- given several independent xi (don't affect others' values)
                                                                                                    P(c|x)=P(x|c).P(c)/P(x) = PI(P(xi|c)).P(c)
                                                                                                            P(c)- class prior probability; P(x|c)- likelihood; P(x)- predictor prior probability; P(c|x)- posterior probability.
                                                                                            Steps
                                                                                                    If numeric variables, transform into categorical counterparts
                                                                                                            or use distribution of numerical variable to have a good guest of the frequency- normal distributions.
                                                                                                            P(humidity=84|play=yes)=N(74) using corresponding mean and SD.
                                                                                                    construct frequency tables- predictor vs target- calculate posterior probability (?likelihood), P(sunny|yes)
                                                                                                            zero frequency problem- 0 value in frequency table => messing with formula => add 1 to all values.
                                                                                                    use original tables to calculate- class prior probability and predictor prior probability (P(yes),P(sunny))
                                                                                                    use bayes formula to find- P(yes|sunny)
                                                                                                    Find likelihood of yes (target value)= P(predictor1=p1|yes)*P(predictor2=p2|yes)...*P(yes)
                                                                                                    Pick the greater probability
                                                                                                    #Kononenko's information gain as sum of information contributed by each attribute can offer an explanation on how values of predictors influence the class probability.
                                                                                                            lg(P(c|x))-lg(P(c))
                                                                                                    Nomograms- contribution of predictors can also be visualized by plotting nomograms
                                                                                                            plots log odds ratio for each value of each predictor
                                                                                                            length of the lines correspond to spans of odds ratios, suggesting importance of the related predictor
                                                                                                            also shows impacts of individual values of predictor
                                                                            DecisionTree
                                                                                    build models in the form of tree structure- can handle both categorical and numerical.
                                                                                    breaks down dataset into smaller subsets while developing decision tree
                                                                                            decision nodes- 2 or more branches for attribute value[s]
                                                                                            leaf node- represent decision
                                                                                            root node- topmost decision node.
                                                                                    How
                                                                                            concept
                                                                                                    employs a top-down, greedy search thru the space of possible branches with no backtracking.
                                                                                                            built top down from root node and involves partitioning the data into a set of (homongeneous subsets).
                                                                                                    calculates homogeneity of sample using entropy and information gain, 0 is homogenous, 1 if equally divided.
                                                                                                            entropy of target- E(PlayGolf)=Entropy(5,9)=Entropy(0.36,0.64)=SIGMA(-p.lg(p))
                                                                                                            entropy of (target, predictor)= E(Playgolf, weather)=P(Sunny)*E(3,2)+P(Overcast)*E(4,0)+P(Rainy)*E(2,3)
                                                                                                            Gain(T,X)=Entropy(T)-Entropy(T,X)	//decrease in entropy after the dataset is being split on the attribute (like weather)
                                                                                                            Choose the attribute that returns highest information gain.
                                                                                                    go on down the tree until entropy is 0- leaf node.
                                                                                                    we can transform decision tree to decision rules for textual description.
                                                                    Covariance Matrix
                                                                            Linear Disriminant Analytic
                                                                                    simple, mathematically robust and often produces models whose accuracy is as good as more complex methods.
                                                                                    LDA is based upon the concept of searching for a linear combination of varaibles that best separates 2 classes (targets)
                                                                                            Z=b1x1+b2x2+....
                                                                                            Score function, S(b)=(bTu1-bT.u2)/bT.C.b
                                                                                            S(b)=(bar(Z1)-bar(Z2))/Variance of Z within groups
                                                                                            Model coefficients, b = C^-1.(u1-u2)
                                                                                            Pooled covariance matrix, C = (n1C1+n2C2)/(n1+n2)
                                                                                    Assess the effectiveness of the discrimination is to calculate the mahalanobis distance between 2 groups.
                                                                                            a distance greater than 3 => 2 averages differ by greater than 3 SD.
                                                                                            it means that the overlap (probability of misclassification) is quite small.
                                                                                                    DELTA^2 = bT.(u1-u2) where DELTA is mahalanobis distance between 2 groups.
                                                                                    Finallly, a new point is classified by projecting it onto the maximally separating direction and classifying it as C1 if
                                                                                            bT(x-(u1-u2)/2) > log(p(c1)/p(c2))
                                                                            Logistic Regression
                                                                    Similarity Functions
                                                                            KNearestNeighbors
                                                                    Others
                                                                            SupportVectorMachine
                                                                            ArtificialNeuralNetwork
                                                            regression- predicted outcome is numeric
                                                                    FrequencyTable
                                                                            DecisionTree
                                                                    CovarianceMatrix
                                                                            MultipleLinearRegressions
                                                                    SimilarityFunctions
                                                                            kNearestNeighbors
                                                                    Others
                                                                            SupportVectorMachine
                                                                            ArtificialNeuralNetworks
                                                            ModelEvaluation
                                                    Unsupervised
                                                            clustering- assignment of observations into clusters (similar objects grouped)
                                                                    Hierarchical
                                                                            Agglomerative
                                                                            Division
                                                                    Partitive
                                                                            KMeans
                                                                            SelfOrganizingMap
                                                    Association Rules- associations amongst observations
                                        </ReferenceSadawiSlides>
                                </DataMiningTechniques>
                            </Modeling>
                            <EvaluateResults>
                                Personal - Metrics - Accuracy, Precision, Recall, F1 Score, True Positives, etc.
                                Determine if results meet business objectives
                                    UAT- User Acceptance Testing.
                                    Evaluate it against business goals.
                                    conforms to "SRS"
                                Identify business issues that should have been addressed earlier
                                    ~SE- Branch Coverage, Statement Coverage, etc.
                            </EvaluateResults>
                            <Deployment>
                                Personal - model deployment for outcomes, Web Service like REST, RShiny Service, Present the outcome without deployment.
                                aka Production
                                Put the resulting models into practice
                                    automated results flush
                                Actionable Insights- Insights Prepared, send reports to marketing guy
                                    Set up for repeated/continuous mining of the data
                                    maybe email, data visualization
                                    all applications emerging outta analytics.
                                Eg- let's choose ROLAP for all crystal functions.    
                            </Deployment>
                        </CRISP_DM>
                    </DataMining>
                </BusinessIntelligence>
                <FrontEnd>
                    BI- Business Intelligence.
                    <Visualization>
                        Evaluate for human consumption- color pallette- psychology- information overflow.
                        Nepolean's expedition- 1 map showing following details
                            Geographical route thru map
                            size of army by width of the path
                            temperature points marked.
                            Points of Dates given
                            Different color for going and coming
                        Transport map- eg- Bombay Metro
                            N E W S remains same
                            Relative distance ignored- ppl just want seq not numbers
                            Simple Design, complex data.
                        Visualization
                            Create Visuals
                                Purpose of computing is insight not number
                                What data to show (which idiom to use- Eg- Pie chart for approximate relative weights)
                            Interact with Visuals
                                Overview
                                Zoom
                                Details on Demand.
                    </Visualization>
                </FrontEnd>
        </DataAnalytics>
    </ConsumingData>
    <Misc>
        Source - Jigsaw
        Planning
                Big Data
                        3 slots - T1, T2, T3-5
                R - 28 sittings (avg per topic)
                SAS - 27 sittings (avg per topic)
                58 sittings reqd 
                2 per day - 30 days.
                        1 sitting in weekdays, 3 sittings on weekends.
        ======
        Text Mining for recommendations. Scraping data from webpages - Market analysis - which customer doesn't seem satisfied so that we can target them.
        Supervised learning - Obama had fun time in india meeting Modi - fun is mentioned so positive and politics field.
                training data set.
        T2
                https://rpubs.com/brianzive/textmining
                http://text-analytics101.rxnlp.com/2014/11/what-are-n-grams.html
                TDM - Term document matrix
                Remove unwanted lines - like IEEE paper's lines - deatils of author, year
                convert all words to upper - cuz R is case sensitive at data reading.
                Finding associations like Obama with other country's PM. Strong co-occurence - has visited a country or meeting conclusions.
                word cloud - size of word = number of time it was spoken.
                1st tweet is stored in 1st position of the corpus and so on.
                association climate occurs most with change ; care ; china - makes sense - correlation!
        T4
                Using it in marketing/advertising - Hydrabad ppl clearly like red label so why waste money in there.
                Sales in restaurant - look at ppl's taste and sentiments
                Zomato recommendations - past likes and dislikes and ratings and accordingly ; also comments.
                Sources of public sentiments - fb, twit, 
        Choose random samples of data - called bootstrapping.
                Bagging highest correlation of class with a feature.
        n>=5 ensemble - when using more than 5 algorithms, 67% of your documents agree on prediction but recall is high meaning that accuracy goes high when they do.
                so decide a compromise or trade off between coverage and recall.
                choice of algos by cross validation - choose algos with high accuracy across folds.
                http://codophile.com/2015/04/15/how-to-integrate-r-with-java-using-rjava/
        Kappa adjusts accuracy for flukes in accuracy.
        R, SaS, Fraud Analytics, Dimension Reduction, Big Data
                Data Visualization.
        Notes on Fraud Analytics
                Multivariate normal => Independent Var be normal (nay) but opp is true.
                        Multivariate outlier - like 20 year old earning 60K Euros. both are in range but not the combination.
                        If QQ plot is 45 degrees then normally distributed
                Density overlap in lda means model ain't very good.
                        even posterior prob shouldn't overlap.
                What you know, others also - but if you know that others don't makes you expert.
                        @you want pearls you gotta go to its bottom; At shore you get shells.
                High chargeback is indicative of fraudulent merchant
                        Higher than average transation amount is risk of fraud.
                        Review Round number amounts - 10K fraud would. But prices are 2.99 etc
                        Even Dollar amount - 2.99 vs 3.00
                        Fraud keeps trying until transaction is denied so gotta look into those.
                Risk scores are scores generated for merchants.
                SingleValue substituition - by mean replace
                If outliers ain't removed they skew the results.
                        no variation - no predictive power
                How much of fraud rate can we capture for a given decile - gains chart.
                        it is better than random guessing cuz it tell 67% of frauds correctly.
                        This graph we show to mgmt - how good our model is.
                        if performs well on validation, model is robust else overfitted.
                https://math.stackexchange.com/questions/2325713/why-multiply-a-matrix-with-its-transpose
                        A.At finds the distance - being the dot product of vector with itself.
                        https://www.youtube.com/watch?v=hkCT-6KJAK0
                        https://www.youtube.com/watch?v=dS7qyxVXcVQ
        Data Science Maps to Software Development as Follows
            Requirements or Functional / Problem Specification
                    Business Understanding, Problem formulation, Process and Data Model
            Design Preprocessing - Data Cubes or Data Warehouse
                    Gathering - Identification, Gathering, Understanding
                    Preprocessing - Data Cleaning
            Implementation - While !best
                    Design - Data Modeling Algorithm
                    Implementation - Traning the data
                    Validation - Evaluation Metrics
            Deployment - Expose the Model in consumable way - UI, REST, as software, part of full fledged software, etc.
            Maintenance - Update the Model over time.
        <PythonHighLevelAPI>
            CS - Python
                    Pandas
                            Pandas needs Numpy to operate. Pandas provide an easy way to create, manipulate and wrangle the data. Pandas is also an elegant solution for time series data.
                            Series - pd.Series([1., 2., 3.], index=['a', 'b', 'c'])
                                    pd.Series([1,2,np.nan])
                                    dates_d = pd.date_range('20300101', periods=6, freq='D')
                            array to dataframe
                                    h = [[1,2],[3,4]] 
                                    df_h = pd.DataFrame(h)
                            dataframe to array
                                    np.array(df_h)
                            random dataframe
                                    random = np.random.randn(6,4)
                                    df = pd.DataFrame(random,
                                                              index=dates_m,
                                                              columns=list('ABCD'))
                                    df.head(3)
                                    df.tail(3)
                                    df.describe()
                            Slice - 
                                    df['A']
                                    df[['A', 'B']]
                                    The first pair of bracket means you want to select columns, the second pairs of bracket tells what columns you want to return.
                                    df[0:3]
                                    df.loc[:,['A','B']]
                                    df.iloc[:, :2]	// use index instead of names.
                            df.drop(columns=['A', 'C'])
                            Concatenation
                                    import numpy as np
                                    df1 = pd.DataFrame({'name': ['John', 'Smith','Paul'],
                                                                             'Age': ['25', '30', '50']},
                                                                            index=[0, 1, 2])
                                    df2 = pd.DataFrame({'name': ['Adam', 'Smith' ],
                                                                             'Age': ['26', '11']},
                                                                            index=[3, 4])  
                                    df_concat = pd.concat([df1,df2]) 
                            Drop Duplicates - df_concat.drop_duplicates('name')
                            Sort Values - df_concat.sort_values('Age')
                            Rename - df_concat.rename(columns={"name": "Surname", "Age": "Age_ppl"})
                            File reading
                                    df_train = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
                                                                               skipinitialspace=True,
                                                                               names = ['age','workclass'],
                                                                               index_col=False)
                                    df_train.shape
                            Grouping
                                    df_train.groupby(['label']).mean()
                                    df_train.groupby(['label'])['age'].min()
                                    df_train.groupby(['label', 'marital'])['capital_gain'].max()
                            Plot
                                    %matplotlib inline
                                    df_plot = df_train.groupby(['label', 'marital'])['capital_gain'].mean().unstack()
                                    df_plot
                    Tensorflow core framework: Tensor
                            tensor - vector or matrix of n-dimensions that represents all types of data
                            source - input data or computation.
                            computations
                                    all the operations are conducted inside a graph
                                    set of computation that takes place successively
                                    Each operation is called an op node and are connected to each other.
                                    edge of the nodes is the tensor, i.e., a way to populate the operation with data.
                                    models are fed with a list of objects called feature vectors
                                    These values will flow into an op node through the tensor
                                    result of this operation/computation will create a new tensor which in turn will be used in a new operation.
                            tensor is an object with three properties:
                                    A unique label (name)
                                    A dimension (shape)
                                    A data type (dtype)
                            4 main tensors
                                    tf.Variable
                                    tf.constant
                                    tf.placeholder
                                    tf.SparseTensor
                            0-D = Tensor("Const_1:0", shape=(), dtype=float32)
                                    r1 = tf.constant(1, tf.int16) or tf.constant(1, tf.int16, name = "my_scalar")
                                    tf.constant(1.12345, tf.float32); tf.constant("Guru99", tf.string)
                            1-D - Tensor("Const_3:0", shape=(3,), dtype=int16)
                                    r1_vector = tf.constant([1,3,5], tf.int16)
                                    print(r1_vector)
                                    r2_boolean = tf.constant([True, True, False], tf.bool)
                                    print(r2_boolean)
                            2-D - Tensor("Const_5:0", shape=(2, 2), dtype=int16)
                                    r2_matrix = tf.constant([ [1, 2],
                                                                              [3, 4] ],tf.int16)
                                    print(r2_matrix)
                            3-D - Tensor("Const_6:0", shape=(1, 3, 2), dtype=int16)
                                    r3_matrix = tf.constant([ [[1, 2],
                                                                               [3, 4], 
                                                                               [5, 6]] ], tf.int16)
                                    print(r3_matrix)
                            shape = dimensionality.
                                    m_shape = tf.constant([ [10, 11],
                                                                    [12, 13],
                                                                    [14, 15] ]                      
                                                             ) 
                                    m_shape.shape
                                    TensorShape([Dimension(3), Dimension(2)])
                            specifics
                                    print(tf.zeros(10)) - Tensor("zeros:0", shape=(10,), dtype=float32)
                                    print(tf.ones([10, 10])) - Tensor("ones:0", shape=(10, 10), dtype=float32)
                                    print(tf.ones(m_shape.shape[0])) - use 1st dimension of m_shape to make ones series.
                                            print(m_shape.dtype)
                            tensor data typecast
                                    type_float = tf.constant(3.123456789, tf.float32)
                                    type_int = tf.cast(type_float, dtype=tf.int32)
                                    during creation guess what is the most likely types of data.
                            Operations
                                    x = tf.constant([2.0], dtype = tf.float32)
                                    print(tf.sqrt(x)) //print the definition of the tensor and not the actual evaluation of the operation
                                    tf.add(a, b)
                                    tf.substract(a, b)
                                    tf.multiply(a, b)
                                    tf.div(a, b)
                                    tf.pow(a, b)
                                    tf.exp(a)
                                    tf.sqrt(a)
                            variable tensor -
                                    var = tf.get_variable("var", [1, 2])
                                    print(var.shape) - (1, 2)
                                    //initials values of this tensor are zero - to find weights
                                    Initialize
                                            var_init_1 = tf.get_variable("var_init_1", [1, 2], dtype=tf.int32,  initializer=tf.zeros_initializer)
                                            tensor_const = tf.constant([[10, 20],
                                                            [30, 40]])
                                                    # Initialize the first value of the tensor equals to tensor_const
                                                    var_init_2 = tf.get_variable("var_init_2", dtype=tf.int32,  initializer=tensor_const)
                                                    print(var_init_2.shape)		
                                            tf.get_variable("var_init_2", dtype=tf.int32, initializer=[ [1, 2], [3, 4] ])
                            Placeholder
                                    purpose of feeding the tensor
                                    used to initialize the data to flow inside the tensors.
                                    method feed_dict - fed within a session.
                                    data_placeholder_a = tf.placeholder(tf.float32, name = "data_placeholder_a")
                                    print(data_placeholder_a)
                                    Tensor("data_placeholder_a:0", dtype=float32)
                            TensorFlow works around 3 main components
                                    Tensors: Represents the data (or value) that will flow in the graph. It is the edge in the graph
                                    Graph: Computational environment containing the operations and tensors
                                    Sessions: Allow the execution of the operations
                            Sample
                                    Create two tensors
                                    Create an operation
                                    Open a session
                                    Print the result
                                    Eg 
                                    x = tf.constant([2])
                                    y = tf.constant([4])
                                    multiply = tf.multiply(x, y)
                                    sess = tf.Session()		//Open a session. All the operations will flow within the sessions
                                    result_1 = sess.run(multiply)			//execute the operation created in step 2
                                    print(result_1)
                                    sess.close()
                                    output - [8]
                            Block Session
                                    with tf.Session() as sess:    
                                            result_2 = multiply.eval()
                                            print(result_2)
                                    see values
                                            sess = tf.Session()
                                            print(sess.run(r1))
                                            print(sess.run(r2_matrix))
                                            print(sess.run(r3_matrix))
                            Maha initialize
                                    sess.run(tf.global_variables_initializer())
                                    print(sess.run(var))
                            operation generalized
                                    import numpy as np
                                    power_a = tf.pow(data_placeholder_a, 2)	//for any data_placeholder_a - just square it.
                                    with tf.Session() as sess:  
                                    data = np.random.rand(1, 10)  
                                    print(sess.run(power_a, feed_dict={data_placeholder_a: data}))  # Will succeed.
                            Graph
                                    depends on a genius approach to render the operation
                                    All the computations are represented with a dataflow scheme.
                                    graph has been developed to see to data dependencies between individual operation (successive operations)
                                            convenient way to visualize how the computations are coordinated
                                    graph shows a node and an edge
                                            node is the representation of a operation - unit of computation
                                            edge is the tensor, it can produce a new tensor or consume the input data
                                            helps to visualize the connection between individual operations.
                                    Eg - f(x,z) = xz+x^2+z+5
                                            x = tf.get_variable("x", dtype=tf.int32,  initializer=tf.constant([5]))
                                            z = tf.get_variable("z", dtype=tf.int32,  initializer=tf.constant([6]))
                                            c = tf.constant([5], name =	"constant")
                                            square = tf.constant([2], name =	"square")
                                            f = tf.multiply(x, z) + tf.pow(x, square) + z + c

                                            //run
                                            init = tf.global_variables_initializer() # prepare to initialize all variables
                                            with tf.Session() as sess:    
                                                    init.run() # Initialize x and y    
                                                    function_result = f.eval()
                                            print(function_result)
                    Tensorboard is the interface used to visualize the graph and other tools to understand, debug, and optimize the model.
                            Sections
                                    Scalars: Show different useful information during the model training
                                    Graphs: Show the model
                                    Histogram: Display weights with a histogram
                                    Distribution: Display the distribution of the weight
                                    Projector: Show Principal component analysis and T-SNE algorithm. The technique uses for dimensionality reduction
                            basic idea behind tensorboard is that neural network can be something known as a black box and we need a tool to inspect what's inside this box.
                                    flashlight to start dive into the neural network.
                            benefits - When you bring all the following pieces of information together, you have a great tool to debug and find how to improve the model.
                                    helps to understand the dependencies between operations, 
                                    how the weights are computed, 
                                    displays the loss function and much other useful information.
                                    activate hello-tf
                                    tensorboard --logdir=.\train\linreg
                                    http://localhost:6006
                    Linear Regression
                            Low-level API: Build the architecture, optimization of the model from scratch. It is complicated for a beginner
                                    to have full control of the computations
                                    define by hand the:
                                            Loss function
                                            Optimize: Gradient descent
                                            Matrices multiplication
                                            Graph and tensor
                            High-level API: Define the algorithm. It is easer-friendly. TensorFlow provides a toolbox calls estimator to construct, train, evaluate and make a prediction
                            Gradient Descent
                                    one dependent variable - y = b + ax + epsilon
                                    Traditional analysis will try to predict the sale by let's say computing the average for each variable and try to estimate the sale for different scenarios. It will lead to poor predictions
                                    error is not equal to zero but stabilizes around 5. It means, the model makes a typical error of 5. If you want to reduce the error, you need to add more information to the model such as more variables or use different estimators.
                            Pandas
                                    import pandas as pd
                                    from sklearn import datasets
                                    import tensorflow as tf
                                    import itertools		
                                    COLUMNS = ["crim", "zn", "indus", "nox", "rm", "age",
                                                       "dis", "tax", "ptratio", "medv"]
                                    print(training_set.shape, test_set.shape, prediction_set.shape)
                                    FEATURES = ["crim", "zn", "indus", "nox", "rm",				
                                                                     "age", "dis", "tax", "ptratio"]
                                    LABEL = "medv"
                                    feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]
                                    estimator = tf.estimator.LinearRegressor(    
                                                    feature_columns=feature_cols,   
                                                    model_dir="train")
                                    def get_input_fn(data_set, num_epochs=None, n_batch = 128, shuffle=True):    		//instruct Tensorflow how to feed the model, you can use pandas_input_fn
                                            return tf.estimator.inputs.pandas_input_fn(       
                                                    x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),		//feature data
                                                    y = pd.Series(data_set[LABEL].values),							//label data
                                                    batch_size=n_batch,												//128 default
                                                    num_epochs=num_epochs,											//1 default
                                                    shuffle=shuffle)												//None default
                                    estimator.train(input_fn=get_input_fn(training_set,						//data to learn from
                                                                                                                       num_epochs=None,				//iterations to learn
                                                                                                                       n_batch = 128,				//break data into consumable chunks (sequential)  
                                                                                                                       shuffle=False),				//ensure no pattern
                                                                                                                       steps=1000)					//iterations to learn
                                    /*
                                    activate hello-tf
                                    # For MacOS
                                    tensorboard --logdir=./train
                                    # For Windows
                                    tensorboard --logdir=train
                                    */
                                    ev = estimator.evaluate(    
                                                      input_fn=get_input_fn(test_set,                          
                                                      num_epochs=1,                          
                                                      n_batch = 128,                          
                                                      shuffle=False))
                                    loss_score = ev["loss"]
                                    print("Loss: {0:f}".format(loss_score))
                                    training_set['medv'].describe()
                                    y = estimator.predict(    
                                                     input_fn=get_input_fn(prediction_set,                          
                                                     num_epochs=1,                          
                                                     n_batch = 128,                          
                                                     shuffle=False))
                                    predictions = list(p["predictions"] for p in itertools.islice(y, 6))
                                    print("Predictions: {}".format(str(predictions)))
                            Numpy
                                    training_set_n = pd.read_csv("E:/boston_train.csv").values
                                    test_set_n = pd.read_csv("E:/boston_test.csv").values
                                    prediction_set_n = pd.read_csv("E:/boston_predict.csv").values
                                    def prepare_data(df):     
                                            X_train = df[:, :-3]    
                                            y_train = df[:,-3]    
                                            return X_train, y_train
                                    X_train, y_train = prepare_data(training_set_n)
                                    X_test, y_test = prepare_data(test_set_n)
                                    x_predict = prediction_set_n[:, :-2]	//exclude last column it's nan
                                    print(X_train.shape, y_train.shape, x_predict.shape)
                                    feature_columns = [      tf.feature_column.numeric_column('x', shape=X_train.shape[1:])]
                                    estimator = tf.estimator.LinearRegressor(    	//instruct the feature columns and where to save the graph
                                             feature_columns=feature_columns,    
                                             model_dir="train1")
                                    # Train the estimator
                                    train_input = tf.estimator.inputs.numpy_input_fn(   
                                               x={"x": X_train},    
                                               y=y_train,    
                                               batch_size=128,    
                                               shuffle=False,    
                                               num_epochs=None)
                                    estimator.train(input_fn = train_input,steps=5000) 
                                    eval_input = tf.estimator.inputs.numpy_input_fn(    
                                       x={"x": X_test},    
                                       y=y_test, 
                                       shuffle=False,    
                                       batch_size=128,    
                                       num_epochs=1)
                               estimator.evaluate(eval_input,steps=None)
                               test_input = tf.estimator.inputs.numpy_input_fn(    
                                            x={"x": x_predict},    
                                            batch_size=128,    
                                            num_epochs=1,   
                                            shuffle=False)
                                            y = estimator.predict(test_input) 			
                                    predictions = list(p["predictions"] for p in itertools.islice(y, 6))
                                    print("Predictions: {}".format(str(predictions)))
                            Tensorflow
                                    Define the path and the format of the data
                                            import tensorflow as tf
                                            df_train = "E:/boston_train.csv"
                                            df_eval = "E:/boston_test.csv"
                                            COLUMNS = ["crim", "zn", "indus", "nox", "rm", "age",				
                                                                    "dis", "tax", "ptratio", "medv"]
                                            RECORDS_ALL = [[0.0], [0.0], [0.0], [0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]]	
                                    Define the input_fn function
                                            def input_fn(data_file, batch_size, num_epoch = None):				
                                                    # Step 1				
                                                    def parse_csv(value):        
                                                            columns = tf.decode_csv(value, record_defaults= RECORDS_ALL)     //the method decode_csv uses the output of the TextLineDataset to read the csv file. record_defaults instructs TensorFlow about the columns type   
                                                            features = dict(zip(COLUMNS, columns))							//Populate the dictionary with all the columns extracted during this data processing
                                                            #labels = features.pop('median_house_value')     				//Exclude the target variable from the feature variable and create a label variable   
                                                            labels =  features.pop('medv')        
                                                            return features, labels							

                                                    # Extract lines from input files using the Dataset API.
                                                    dataset = (tf.data.TextLineDataset(data_file) 					// Read text file       
                                                            .skip(1) 													# Skip header row       							
                                                            .map(parse_csv))			   								//parse the records into the tensors. You need to define a function to instruct the map object.
                                                    dataset = dataset.repeat(num_epoch)    							//to allow the dataset to continue indefinitely to feed the model else error after 1st.
                                                    dataset = dataset.batch(batch_size) 							//how many data you want to pass in the pipeline for each iteration; big batch size => slow			
                                                    # Step 3    
                                                    iterator = dataset.make_one_shot_iterator()    
                                                    features, labels = iterator.get_next()    
                                                    return features, labels			
                                    Consume the data
                                            next_batch = input_fn(df_train, batch_size = 1, num_epoch = None)
                                            with tf.Session() as sess:    
                                                     first_batch  = sess.run(next_batch)    
                                                     print(first_batch)
                                    Define the feature column
                                            X1= tf.feature_column.numeric_column('crim')
                                            X2= tf.feature_column.numeric_column('zn')
                                            X3= tf.feature_column.numeric_column('indus')
                                            X4= tf.feature_column.numeric_column('nox')
                                            X5= tf.feature_column.numeric_column('rm')
                                            X6= tf.feature_column.numeric_column('age')
                                            X7= tf.feature_column.numeric_column('dis')
                                            X8= tf.feature_column.numeric_column('tax')
                                            X9= tf.feature_column.numeric_column('ptratio')
                                            base_columns = [X1, X2, X3,X4, X5, X6,X7, X8, X9]
                                    Build the model
                                            model = tf.estimator.LinearRegressor(feature_columns=base_columns, model_dir='train3')
                                    Train the estimator
                                            model.train(steps =1000,    
                                              input_fn= lambda : input_fn(df_train,batch_size=128, num_epoch = None))
                                    Evaluation
                                            results = model.evaluate(steps =None,input_fn=lambda: input_fn(df_eval, batch_size =128, num_epoch = 1))
                                            for key in results:   
                                                    print("   {}, was: {}".format(key, results[key]))
                                    Predictions
                                            prediction_input = {				
                                                              'crim': [0.03359,5.09017,0.12650,0.05515,8.15174,0.24522],				
                                                              'zn': [75.0,0.0,25.0,33.0,0.0,0.0],				
                                                              'indus': [2.95,18.10,5.13,2.18,18.10,9.90],				
                                                              'nox': [0.428,0.713,0.453,0.472,0.700,0.544],				
                                                              'rm': [7.024,6.297,6.762,7.236,5.390,5.782],				
                                                              'age': [15.8,91.8,43.4,41.1,98.9,71.7],				
                                                              'dis': [5.4011,2.3682,7.9809,4.0220,1.7281,4.0317],				
                                                              'tax': [252,666,284,222,666,304],				
                                                              'ptratio': [18.3,20.2,19.7,18.4,20.2,18.4]
                                             }
                                             def test_input_fn():    
                                                     dataset = tf.data.Dataset.from_tensors(prediction_input)    
                                                     return dataset
                                             # Predict all our prediction_input 
                                             pred_results = model.predict(input_fn=test_input_fn)
        </PythonHighLevelAPI>
        <MLMaths>
            ML Maths
                    Customer segmentation - these are the people - can we group them together - manual work of checking similarity - computer can do it for you.
                            we know city distribution of people - where should we open our pizza stores. Random place and keep making it better (centroid) but now who will want to go to pizza stores (repeat)
                            Elbow method - try different numbers and be smart at picking the numbers. We know seeing all of them the best cluster but think like computers - algorithmize it (how good is a given clustering) diameter of a cluster (farthest distance in a cluster) - elbow to decide on number of clusters.
                            Applications - Genetics, Evolutionary biology, Recommender Systems, Social Networks (users into groups based on demographics, behavior)
                            Hierarchical clustering - bottom up. Dendogram - connect closest points as if in the same cluster. Get next distance and keep adding it to 2-D dendogram - mark a vertical cut for the number of cluster you are interested in.
                    They also bought 55 inch TV - well we are the trend creator and they are the trend follower.
                    PCA - to reduce the number of dimensions. You have a bunch of friends and you wanna take their picture - what's the best angle.
                            PCA is taking picture of data and trying to keep as much information as possible. Here, 3-D to 2-D.
                            Imagine 2-D points on 1-D - which one will you prefer. More spaced out.
                            Purpose - reducing features.
                            Applications - housing data - size, #rooms, #bathrooms clubbed into one size feature and school around, crime rate into location feature - that's an intuition but ML does it ideally.
                                    our prediction was right the bigger the size, more #rooms it has. Let's fit a line - it's not linear regression but projection and different purpose.
                                    get projection and find x-variance and y-variance.
                            3blue1brown on eigen vectors.
                    Layers of DS study - from SE point of view
                            Business Study of DS (what is it, when will you use it, why and how - libs) - All R and Python Libraries to apply ML while focus only on config
                            Design Study of DS - Architectures and why do they work at different DFD Levels 
                                    HDL - approach, ideas and proofs
                                    LDL (let's call it impl) - formulae devised from loops to LA
                            Implementation Study of DS (actualize the algorithms devised - in loops, in LA)
                            More - 
                                    But from DS point of view - Design and Implementations are already done and just for thinking of what framework does while you work at business or user or consumer level.
                    Matrix factorizations - movie recommender system
                            M1=M2, A=C, M5=Average(M2,M3) - average of twisters and jaws. sharknado. M1 = M2+M3 - 
                            Table has a lot of dependencies that we want to use.
                            Maths to identify complicated ones. Express this as resultant matrix obtained from this (how much some unknown features in the movie) X that (how much user likes features)
                    Lessons from ML - It doesn't have to be best. It has to be pursuit of ever betterment. If something is bad, so reject it - no ML says measure the error and just make it some better for the next iteration. Eg - wall is just bad so what welcome are we gonna do - well the best that we possibly can for gradient descent in subsequent iterations for perfection.
                    Linear Regression - A point or a set says come closer - ways - Translate (change y intercept) and rotate (change slope)
                            ML does no drastic move but move step by step towards final goal.
                    Logistic Regression - equation ax+by+c=0 and translate and rotate meaning changing a,b,c.
                    Perceptron finds error using the perpendicular distance from the line and makes changes thru translate and rotate.
                    Naive Bayes - too naive - no cartesian product but assume uniform distribution of texts (or independence) so good when less data.
                            applicable only to categorical variables. 
                            ?No library support. 
                            imagine all possible data points (cross product of all domains of features) already having different intensity for each class variable.
                            Given a query about one of those data points, we can quickly work it out - compare the class variables intensity and say which is most probable.
                            can it be thought of as clustering - no, cuz we don't even care about existing relations in the data set - it's too naive that way.
                            Decision trees very similar - clustering with class variables and finding criteria from them (reverse 
                                    but it is ultimately a linear model (not quite - refer stackoverflow) - think of finding clusters but not in traditional clustering sense but finding bunch of data points and creates "square" bounds on them to distinguish them from others.
                    SVM - Improvement over perceptron - where the whole point is to find a good separation.
                            Logisitic regression - separation is only for us to see the graph from higher dimension and line/plane of separation is just the value = 0.5 at higher dimension.
                                    But it's not the criteria but a way of seeing.
                            find initial line and errors are defined differently in terms of distance from red and blue line of points.
                            that error gradient descent tries to reduce ensuring good separation. 
                            Also margin error distance between lines - well can finish it off by increasing the a^2+b^2 but then classification error suffers.
                            So trade-off guarantees best separator with ideal margin chosen.
                    DNN
                            try direction where distance from cake reduces and finally get the cake.
                            we took derivative or gradient ot climb down the everest the fastest.
                            2 probs solved - minimize cake distance, minimize height; so in general minimize error (how far we are from solution)
                                    why not number of errors amount of error - cuz continuous function and gradual changes and instant update (fail as early as possible)
                                    imagine a pyramid like stage (school days 1st 2nd 3rd kind) - not sure which direction to go.
                            every point has a penalty - if misclassified then huge - if on the same side - then small penalty.
                            Ego - what all do you do and what does that tell me about you as a person (or spirit - or current state of art of the soul's attachments)
                            we want to convert function of error function (degree of blueness - from -INF to +INF) to a probability between 0 and 1 - so activation function.
                                    error function - a line with probability of each point at its side (degree of blueness and redness) - product of probabilities is a good way to judge how good arrangement it is.
                                    but how can I express the same thing in product - logarithms.
                                    how bad the point is classified thru the log of probability of it being there.
                            Think of every neuron as taking coordinate (x,y say) and say whether it lies in its blue or red area and spit the class out.
                                    how do we get non-linear functions - addition of 2 linear functions or regions - approximates a non-linear function.
                                    2 separators and their weighted summations - which separator should weigh more - the final looks more like that one - you apply activation function on the output to have the outcome.
                                    so linear combinations of 2 areas with some constant for better separators.
                    CNN - simplest image classifier.
                            4 pixels - forward slash and backward slash images - use A-B-C+D or (1,-1,-1,1) weights to identify the slash and it is robust for spill overs or unfinished.
                                    here we figured out the weights - how could computer figure that out.
                                    AI - check out all the possibilities - world with 2 letters and 16 possibilities - so not scalable.
                                    let me give you a fake idea. start with some weights say (1,1,1,1) and it gave huge error - make small changes until it can tell the 2 apart. pick more changes from the best one to navigate along the tree.
                                    we have continous values to check. Like NN eg of minimizing distance - we keep making changes to matrix until the error is minimum.
                            slightly more complex world - images with X,O,/,\ - some emotions possible.
                                    CNN comes here - 2 layers 
                                            2 sub components 
                                                    Convolution (to apply filters (-1,1,1,-1) and (1,-1,-1,1) to make 2 (n-1) matrix) 
                                                    pooling layer (read the obtained n-1 matrix) - detecting forward or backward diagonal using thresholds.
                                                    break 3X3 into 2X2 and identify / or \ there and later on combine the obtained 2X2 to say if it is X,0,/ or \.
                                            Fully connected layer (superimpose them) - input of previous layer and uses logic to find what's the net.
                                                    make 2-D with / and \ as basis and mark 1 or -1 for every pixel. multiply with filters for every class to get the outcome in terms of value like first step.
                    RNN
                            NN represents a bunch of matrix multiplications.
                            Perfect roommate who doesn't cook as per weather but as per what was cooked yesterday
                            a special matrix which given input of apple pie spits out burger - so M*[1 0 0]' = [0 1 0]'
                            M = [0 0 1; 1 0 0; 0 1 0]
                            RNN = X->ABC=>BCA->X
                            Now roommate decides cooking style as weather plus what was cooked yesterday. Sunny then same as yesterday; raining then next thing in the list.
                            So think of NN as X;weather-->ABC=>BCA-->X
                            Food matrix concatenation for today food and tomorrow's food.
                            Food outcome + weather matrix outcome 
                                    input of apple pie tryst with 2 matrices today and tomorrow plan
                                            [1 0 0; 0 1 0; 0 0 1 CONCAT 0 0 1; 1 0 0; 0 1 0] * [1 0 0]' = [(1 0 0)' | (0 1 0)']
                                    today's weather state tryst with 2 matrices same day or next day
                                            [1 0 ; 1 0; 1 0 CONCAT 0 1; 0 1; 0 1] * [0 1]' = [0 0 0; 1 1 1] meaning same day.
                                    add the output outcomes of 2 matrixes
                                            [(1 0 0)' | (0 1 0)'] + [(0 0 0)' | (1 1 1)'] = [(1 0 0)' | (1 2 1)']
                                            observe 2 it says that pick the next day and not the same day.
                                    Linear-ize it
                                            [(1 0 0)' | (1 2 1)'] converted to [(0 0 0)' | (0 1 0)'] 
                                                    how - using some linear function and sigmoid but later on.
                                    remove concatenation
                                            [(0 0 0)' | (0 1 0)'] = [(0 0 0)'] + [(0 1 0)'] = [0 1 0]' meaning burger next day.
                                                    how - [I3|I3] * [(0 0 0)' | (0 1 0)'] and treat concat like it is regular matrix.
                                    Next iteration
                                            this output acts as input for the next iteration.
                            If NN viewed not as matrix and linear maps but as edges weights and node inputs, then it's 
                            Applications - Stock Prediction, Sequence Generation, Text Generation, Voice Recognition.
                            Here the food and weather concat matrices were the parameters to be trained. So, start with random values and train over the gradient descent.
                                    new error functions not the same.
        </MLMaths>
    </Misc>
</Analytics>