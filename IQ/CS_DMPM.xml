<?xml version="1.0" encoding="UTF-8"?>
<CS>
    About - this is my DM PM take on different tools and everything. For details, refer to specific xml files - this is just few words for high level understanding.
    <RnD></RnD>
    <Systems>
        <Platforms>
            <Suse></Suse>
			<LINUX>
				Linux directory structures
					/
					IaaS
						/dev - virtual files for the devices.
						/media - removable drives as folder.
						/mount - system admins use this to manually mount a filesystem.
					OS (=windows)
						/boot - boot files
							contains the files of the kernel and boot image, in addition to LILO and Grub
						/proc - process and kernel files
							information about. to get runtime system information.
						/var - variable data files - appData
							programs store runtime information like system logging, user tracking, caches, and other files that system programs create and manage.
							Eg - login history - not cleaned automatically. All logs also in here - mail server logs, system boot logs, kernel logs, etc.
						/etc - configuration file (of system) used by admins or services (password file and networking files)
							system config changes like hostname change is done here.
					Shell - OS Facade (=windows system32 files)
						/bin - binaries
							executable files of many basic shell commands like ls, cp, etc.
						/sbin - only su can run these binaries.
						/lib - shared libs.
							codes that can be used by the executable binaries
					PaaS - softwares for dev or prod (= program files)
						/usr - user program data and binaries.
							all the executable files, libraries
							/usr/bin - basic user commands. sbin for additional user commands. lib - system libs.
							Eg - /usr/bin/python.
							Observation - by default all apt-get installations go here and other parts like etc and all.
						/opt - optional software
							installing/storing the files of third-party applications that are not available from the distribution’s repository
							eg - /opt/tomcat, /opt/eclipse-installer
					SaaS and personal data also (=D Drive)
						/root - for super user.
						/home - for all users - has bin, sbin, lib eth.
					Software Data (appData is actually /var)
						/srv - service data like website data.
						/tmp - temp files
							deleted when your system restarts. Apps use for temp storage.
					Eg - apt-get maven install writes to /usr/bin/mvn and also etc and /usr/share.
						Just like program files contains application executables but data and configuration like registry details is somewhere else.
			</LINUX>
			<Windows>
				Windows Toolbar shows all background processes - daemons.
					A typical PC user has lots of standalone software and clients softwares but almost no servers.
					Windows - 
						Analytics - Task Manager.
						Transactional - Daily business like excel sheets, etc.
						Master - Apps, data.
							Apps can be GUI based like IDEs or CMD based like maven.
						Foundation (authentication, etc) - User accounts, task views, util softwares like scheduler, etc.
						Configurations - settings and control panel.
				Windows -> C Drive
					Windows folder
					ProgramFiles
					ProgramData - Eg - Microsoft -> Windows -> StartMenu -> shortcuts in start Menu of windows.
					Users
						public
						Me
							Onedrive, Favorites, Contacts, 3D Objects, Links, Games, Searches.
							Desktop, Downloads, Pictures, Documents, Videos, Music, etc.
							Dumps of apps - .m2, .ssh, .gitconfig, etc.
							Workspace for IDEs - Pycharm project, etc.
							AppData
								Local, LocalLow, Roaming
								User specific storage by the process of the program.
			</Windows>
        </Platforms>
        <Frameworks></Frameworks>
    </Systems>
    <DevOps>
        <Scrum>
        Daily Scrum
            done status, Blockers, Plans.
        Sprint Review - DMPM
            Demo in review - Presenting a new world - completeness related questions. Curiosity Applications related. Usability related suggestions.
            updates for next sprint
            DoDs
            Done and Spillovers
            Retro - Good Bad Ugly
            what all is done and spillovers.
            productivity loss
        </Scrum>
        <Design>
            <DesignTools>
                <IDE></IDE>
            </DesignTools>
            <Servers>
                <Kafka>
                    DM
                        Kafka - Documentation says Distributed streaming platform. similar to message queue or enterprise messaging system.
                            Cluster - Group of computers sharing workload for common purpose
                                Broker - Kafka Server
                            zookeeper
                            Topics - a name for kafka stream
                                Partitions - part of topic
                                    Leader - which machine plays leader for a partition - producer consumers talk to that guy
                                    Offset - unique Id for message in partition (sequence id given when messages enter)
                                        current
                                        committed - offset position already processed by a consumer
                                            sync, async
                            Message Record
                        Interactions
                            Producer - app to publish record to topics
                                Producer Scaling - Thread pool.
                            Consumer - app to pull record from topics
                                Consumer Group - a group of consumers acting as a single logical unit
                                    Y? parallel processing a topic; manages partition assignment; can rebalance as per consumer entry exit.
                            Meta - Schema Evolution
                            Stream - client library to process data in kafka (like substitute of spark, storm, etc)
								Stream processors interact.
								continuous stream of messages can be handled. Stream processing apps - read and store back or pass on
                            Connectors - i/o kafka and systems (Kafka producer separated from App - Kafka Connect to handle producer and consumer part)
								a framework to build connectors for in / out movement of data between kafka and other systems. Like DB.
                    PM
                        Distributed streaming (platform) - publish subscribe to a record stream, store them, process the stream
                            Fault Tolerance -
                                enabling a system to continue operating properly in the event of failure of some of its components
                                thru replications factor - number of copies
                            CRUD
                        Producer
                            DFD L0 - Send Message
                                Properties Config - configure properties, Serialization for key and value
                                    bootstrap.servers
                                    key.serializer
                                    value.serializer
                                    partition.class
                                    acks - 0 (no ack - loss of messages, high thruput, no retry), 1 (respond back), all (all partition replica should know)
                                    retries, max.in.flight.requests.per.connection
                                    use.synchronous.send
                                    buffer.memory, compression.type, batch.size, linger.ms, client.id, max.request.size
                                Producer Record Creation - with KV to topic, partition, timestamp.
                                    Serializer KV
                                        Create class
                                        Create producer
                                        Create Serializer - implements Serializer(class) and implements serialize - change this if class is changed
                                        Create Deserializer
                                        Create Consumer
                                    Partitioner - Assign Partition
                                        Default
                                            specified then use it
                                            not specified then find the hash - utils.toPositive(utils.murmur2(keybytes)) % #Partitions
                                            no partition and key then round robin
                                        Customer
                                            ...
                                    Partition Buffer - batch
                                Sending
                                    Fire and Forget
                                    Asynchronous Send
                                    Synchronous Send
                                //update the current offset
                                Accept Record Metadata if successful else error reporting and retry.
                            Scaling Kafka Producer - thru threads
                                ...
                        Consumer
                            DFD L0 - Ask for messages, get messages
                            Consumer Group - listen to 1 topic across partitions with no duplication of messages (better than 1 consumer reading from 4 partitions is 4 consumers reading from 4)
                                entry/exit - reassign partition to another consumer;
                                Group coordinator - maintains the list of active consumers
                                    Manage a list of group members
                                    Initiates a re-balance activity (blocked read for all members) - when a list of consumers is modified
                                    Leader executes a re-balance activity
                                    Sends a new partition assignment to consumer
                                    communicate about new assignments to consumer
                            Consumer wrt Consumer Group
                                Properties - heartbeat.interval.ms = 3ms, session.timeout.ms=3ms
                                    enable.auto.commit and auto.commit.interval.ms
                                //subscribe to topic
                                Connect to group coordinator
                                Join the group (group.id property)
                                    Sends heartbeat
                                Receives partition assignment - which partition to which group - strategies - range, round robin
                                fetches you messages
                                    current offset, committed offset
                                    Poll for records
                                    commit the read records with broker
                                        what if rebalance triggered just b4 commit - maintain offset of processed record, commit when rebalance is triggered
                                        consumerRebalanceListener - onPartitionsRevoked, onPartitionsAssigned
                                        Maintain a list of offsets that are processed and ready to be committed (after polling)
                                        commit the offsets when the partitions are going away (onPartitionRevoked call - commitSync the offsets)
                                Automatic group mgmt and partition assignment
                                offset and consumer position control (consumer.seek)
                                And many more things
                        User
                            Deployment - taken care if cloud
                                Kafka download
                                kafka server start - zookeeper and kafka servers
                                    More instances - change broker listeners and logs dir
                                    config properties
                                        broker id, port,
                                        logs.dir - //where all your offset, topic information is stored
                                        delete.topic.enable
                                        zookeeper.connect, auto.create.topic.enable
                                        default.replication.factor, num.partitions
                                        log.retention.ms, log.retention.bytes
                            topic create
                            start producer console to write data
                            start consumer console to read data
                            send and receive messages
                        Vision
                            Apps listening from kafka topic and writing to another kafka topic
                            Kafka for backbone of all the system communications
                            Kafka for Data collection from n sources b4 analytics
                            Kafka to read agnostic to data format and connect across engines
                        Schema Evolution
                            ...
                        JUnit
                            ...
                        Kafka Connect
                            ...
                        Kafka Streaming
                            ...
                    Regex
						Kafka Cli - Most important scripts
							./zookeeper-server-start.bat ../../config/zookeeper.properties
							./kafka-server-start.bat ../../config/server.properties
							./kafka-topics.bat --create --topic quickstart-events --bootstrap-server localhost:9092
								./kafka-topics.bat --create --zookeeper localhost:2181 --partitions 1 --replication-factor 1 --topic topic1
							./kafka-console-producer.bat --topic topic1 --bootstrap-server localhost:9092
							./kafka-console-consumer.bat --topic topic1 --from-beginning --bootstrap-server localhost:9092
                        Kafka
							Producer
								Properties
									ProducerConfig.CLIENT_ID_CONFIG, "appName"
									bootstrap.servers - eg - localhost:9092,localhost:9093
										props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, AppConfigs.bootstrapServers);
									key.serializer - eg org.apache.kafka.common.serialization.StringSerializer
										ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG
										import org.apache.kafka.common.serialization.IntegerSerializer
									value.serializer - eg - SupplierSerializer class
										ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG - string, int whatever with path.
										JSONSerializer Class can be made - implements Serializer[T]
											public byte[] serialize(topic, T data) {
												if data != null, ObjectMapper().writeValueAsBytes(data);
											}
											ObjectMapper().readValue(new File(data_file), PosInvoice.class) -> responsible for converting json to object - outside of plugin.
											this class is written with @JsonProperty("JsonField") private String objectField;

									Others in respective places
										props.put("group.id", groupName); - for consumer
										props.put("partitioner.class", "SensorPartitioner"); - for partition
										props.put("speed.sensor.name", "TSS");
								KafkaProducer(props)
								send(ProducerRecord) - exception proned (?asynchronous)
									get() - for synchronous
								send(ProducerRecord, callback(recordMetadata, exception){})
									send(ProducerRecord, new MyProducerCallback())
								producer.close();
							dependencies ->
								ProducerCallback
									class MyProducerCallback implements Callback {
										public void onCompletion(RecordMetadata, Exception) {}
									}
								ProducerRecord[Object, Object](topic, key, value)
								Partition
									props.put("partitioner.class", "SensorPartitioner");
									Producer(props).
									class SensorPartitioner implements Partitioner {
										configure
										Partition("topic", Object key, byte[] keyBytes, value, valueBytes, Cluster cluster) {}
										close(){}
									}
								SupplierSerializer implements Serializer[Supplier] {
									configure
									serialize(String topic, Supplier data): byte[] {
										id.toString().getBytes("UTF8")
										ByteBuffer.allocate(space)
										ByteBuffer.putInt(id);
									}
									close
								}
								Deserialize
									Deserialize(topic, byte[] data): Supplier {
										ByteBuffer.wrap(data).getInt();
										new String(ByteBuffer.get(byte[]), "UTF8")
									}
							Consumer
								Properties
									key.deserializer - org.apache.kafka.common.serialization.StringDeserializer
									props.put("group.id", groupName);
									enable.auto.commit, false
									props.load(new FileInputStream("fileName"));
								KafkaConsumer[Object, Object] consumer = new KafkaConsumer[](props);
									consumer.subscribe(Arrays.asList(topicName));
										consumer.subscribe(Arrays.asList(topicName),rebalanceListner);
									records = consumer.poll(100) -> ConsumerRecords
									consumer.commitAsync();
									consumer.commitSync();
										consumer.commitSync(currentOffsets);    //Map[TopicPartition, OffsetAndMetadata] - useful with RebalanceListener
									consumer.close()
								ConsumerRecords[String, Supplier] records = consumer.poll(100);
									records.count()
									record.topic(), record.partition(),record.offset()
									for(ConsumerRecord record: records)
									record.value().getID()
								TopicPartition
									p0 = new TopicPartition(topicName, 0);
									p0.partition()
									consumer.assign(Arrays.asList(p0,p1,p2));
									consumer.position(p0)
									consumer.seek(p0, offset)
									consumer.position(p0)
								rebalanceListner extends ConsumerRebalanceListener
									new RebalanceListner(consumer)
									rebalanceListner.addOffset(record.topic(), record.partition(),record.offset());
										remember topicPartition and OffsetAndMetadata Map.
									onPartitionsRevoked(Collection[TopicPartitions] partitions) {
										use currentOffsets to consumer.commitSync(currentOffsets)
									}
									useful if you had read but not committed and rebalance happened in the meantime.
							AvroProducers and AvroConsumers -
								Use Value serializer - io.confluent.kafka.serializers.KafkaAvroSerializer
								make avsc json file
								add avro dependencies or plugins in sbt or pom files and both java model and java serializer will be taken care of.
									"org.apache.avro" % "avro" % "1.8.1",
									"io.confluent" % "kafka-avro-serializer" % "3.1.1",
									repo and resolvers - confluent.
							POMs
								kafka-clients, slf4j, maven compiler plugin.
							Multi-threaded kafka producer
								In a loop, Thread[] dispatchers = new Thread(new Dispatcher(producer, details)).start()
								for(Thread t: dispatchers) dispatchers[i].join()
								producer.close()
								ANOTHER
								ExecutorService executor = Executors.newFixedThreadPool(noOfProducers);
								RunnableProducer runnableProducer = new RunnableProducer(producer, details);
								executor.submit(runnableProducer);
								//addShutdownHook
									Runtime.getRuntime().addShutdownHook(thread);
									thread - new Thread(()-> {
										runnableProducer.shutdown(); //for all threads - ending code.
										executor.shutdown();
										executor.awaitTermination(50, TimeUnit.MILLISECONDS);
									})
							Transactions Producer - all or none.
								Property - ProducerConfig.TRANSACTIONAL_ID_CONFIG, AppConfigs.transaction_id
								producer.initTransactions();
								producer.beginTransaction();
								//do producer works
								producer.commitTransaction();
								producer.abortTransaction(); and close.
								start another thru beginTransaction.
							JSON to POJO - Json Schema to POJO plugin
								jsonschema2pojo-maven-plugin
								dependencies - jackson-databind; commons-lang.
							Maven Avro plugin for generating pojo
								avro-maven-plugin
								avro dependency.
                    HelloWorld
                </Kafka>
            </Servers>
            <Framework>
                <Spark>
				About - all the following notes actually belong in detailed CS. Here it should be only DMPM and regex.
				PySpark (Spark Programming in Python) - Source - LearningJournal.
					?Useful in - I/O and Storage and Processing
						1. Data Collection and Ingestion
						2. Data Storage and Management
						3. Data Processing and Transformation
						4. Data Access and Retrieval
					1. Google File System – 2003
					https://ai.google/research/pubs/pub51
						1. HDFS - Hadoop Distributed File System
						vs Data Lake - Vertical scalability, High capital cost.
					2. MapReduce – 2004
					https://ai.google/research/pubs/pub62
						2. Hadoop MapReduce
					Layers
						Infrastructure (Storage) - HDFS, Amazon S3, Azure Blob, Google Cloud, CFS.
						Resource Managers - YARN Kubernetes Mesos
						Runtime - Apache Spark Engine - Most active sources
							Spark Installations or Environment
								Local Mode - Command Line REPL
								Python IDE - Pycharm. Anaconda distribution of python is recommended.
									https://www.jetbrains.com/pycharm/
									https://www.anaconda.com/products/individual
									https://spark.apache.org/downloads.html
									1. Anaconda Platform
									2. PyCharm
									3. Spark Binaries
									4. SPARK_HOME
									5. HADOOP_HOME
								Other Notebooks - Jupyter Notebook.
									set SPARK_HOME
									install findspark -> python -m pip install findspark
									Use it in python notebook
										import findspark
										findspark.init()
										import pyspark
										from pyspark.sql import SparkSession
										spark = SparkSession.builder.getOrCreate()
										spark.read.csv("C:\\Users\\I341365\\Desktop\\di.csv").show()
								Databricks Cloud - Notebooks.
									Single cluster limited to 6GB and no worker nodes
									Basic notebook without collaboration
									https://docs.databricks.com/getting-started/quick-start.html
									DROP TABLE IF EXISTS diamonds;
									CREATE TABLE diamonds USING CSV OPTIONS (path "/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv", header "true")
									?Personal exploration
										DATABRICKS PLATFORM – FREE TRIAL - ML Frameworks, BI, SSO, 14 day trial
										COMMUNITY EDITION - For students and educational institutions
											Import explore data. blank notebook.
											New Notebook, Table, cluster, MLFlow Experiment, Library Import, Documentation.
										Cluster - Create, Edit, Clone, Restart, Terminate, Delete.
											Name, Runtime Version, Zone, Spark.
											Configuration, Notebooks, Libraries, Event Log, Spark UI, Driver Logs, Metrics, Apps, Spark Cluster UI Master.
											4 minutes to spin up.
											Apps, Jobs, Executors, stages.
										History server for past, yarn for present.
								Other options - cloud offerings. Eg - Amazon EMR, Google DataProc, Azure HDInsight, etc. Commercial in nature.
							Execute
								Execution Styles
									Stream processing - data is continuously coming and getting processed - Eg - business users informed about news, or IOT.
									Batch Processing - extract a batch from history and process it. Eg - YT to HDFS to Spark to insights.
								Program Execution
									Interactive clients - spark-shell, Notebook.
									Submit Job - spark-submit, Databricks Notebook, REST API.
								Processing Model
									Spark Client - triggers spark-submit to Cluster Manager (YARN).
									Spark Cluster - YARN receives request. and Creates for Application -> Driver and Executors.
								Spark Local and Execution Modes
									Local Machine (Spark Local JVM) - Spark-shell | Notebook | IDE
										local[n]
										YARN
										Kubernetes
										Mesos
										Standalone
									Interactive Clients - Spark Client triggers spark-shell or Notebook.
										Client Machine - Driver.
										Spark Cluster Slaves have executors.
								Spark Session Configs - Configuring Spark Session
									Sources
										Environment variables
										SPARK_HOME conf spark defaults.conf
										spark submit command line options
											spark-submit --master local[3] --conf "spark.app.name=Hello Spark" --conf spark.eventLog.enabled=false HelloSpark.py
										SparkConf Object
									Precedence for session configs
										Application Code - SparkConf
										Spark-submit sources
											Command Line Options
											Spark-defaults.conf
											Environment Variables
						Spark Core - Scala, Java, Python, R.
							Ingest - Informatica, Talend, AWS Glue, HVR, 
								Spark Data Frame Partitions
									Driver has Spark Session - spark.read.csv() is called.
									YARN Cluster - In Memory data frame partitions created after reading file blocks. Separate Executor JVMs.
									HDFS - the file from HDFS blocks.
							Store (infrastructure already) or process - 
								Transformation and Actions
									Read, where, select, group by, count.
										read.csv()
										select(age, gender, country, state)
										filter(age < 40)
										groupBy(country).count()
										Actions - show()
									Narrow Dependency Transformation - A transformation performed independently on a single partition to produce valid results. Literally union of results. REL - where clause only.
									Wide Dependency Transformation - A transformation that requires data from other partitions to produce valid results. Needs so combining. REL - group by. Solution perform shuffle and sort exchange before group by.
									Spark Actions - Read, Write, Collect, show.
							Consume - File Download, JDBC/ODBC, REST Interface, Search, Data Scientist.
						Spark Libraries - Spark SQL, Spark Streaming, MLlib, GraphX.
							https://github.com/LearningJournal/Spark-Programming-In-Python
				CS - Spark - Source - TutorialsPoint
					Features
						Speed - 100x in RAM and 10x in Disk of hadoop. Done thru reduced Disk I/O. Interin results in memory.
						Supports multiple languages - JAVA, Scala, Python, R.
						Advanced Analytics - MR but also SQL, Streaming, ML, Graph X.
					Deployed on Standalone (Spark on HDFS), YARN / Mesos, SIMR (Spark in MapReduce)
					Components -
						Extensions
							SQL - new data abstraction called SchemaRDD to support structured and semi-structured data
							Streaming - leverages Spark Core's fast scheduling capability to perform streaming.
								ingests data in mini-batches and performs RDD and performs transformations on those mini-batches.
							MLib - distributed machine learning framework. Done against Alternating Least Squares (ALS) implementations.
								9x of Apache Mahout. Now Mahout also has spark interface.
							GraphX - distributed graph-processing framework.
								API for expressing graph computation that can model the user-defined graphs by using Pregel abstraction API.
						Core - general execution engine for spark platform. Provides IN-Memory computing and referencing datasets.
					RDD - ...
					Installation - ...
						Client environment -
							No Set up Spark - Website based.
							cli - Spark shell, spark submit
							Languages client API
							Notebooks
							Cluster Access - Zepplin Notebook, SSH cli access, etc.
							CF SJS (Spark Job Server) Access - APIs exposed to submit job.
					Core Programming - ...
					Spark Deployment - ...
					Advanced Programming
						contains two different types of shared variables ->
						Broadcast variables − used to efficiently, distribute large values.
							val broadcastVar = sc.broadcast(Array(1, 2, 3))
							...
						Accumulators − used to aggregate the information of particular collection.
							val accum = sc.accumulator(0)
							sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
							accum.value
							...
						Numeric RDD Operations - ...
				Misc as follows
					CS - Spark Connections ways - Excellent Raw Material for DMPM.
						Local
						SSH into the remote clusters and use Spark shell on the remote cluster.
						downsample the data and pull the data to your laptop.
						Remote notebook on a cloud - zepplin, jupyter supported on cloud - but crashes and gotta download.
						Bridge local & remote spark - seamless with spark standalone but yarn gotta copy etc files to local spark - auth problem
						Jupyter notebook kernel called “Sparkmagic” - sends code to/from Spark cluster thru livy (possible to install thru ambari) - but installation, connection, and authentication issues
							sparkmagic introduces Docker containers to solve the problem of auth, installation, etc.
						Problems - other languages to run on cluster, scheduling the run.
							Set up a remote Jupyter server and SSH tunneling
							Set up a cron scheduler
							Set up Airflow
							Set up Kubeflow and other Kubernetes-based solutions
							Bayesnote is a notebook orchestration platform for Jupyter notebook:
						Others - PySpark Gateway
					CS - Spark applications (Documentation Reading) - Excellent raw material for DMPM.
						run as independent sets of processes on a cluster
						coordinated by the SparkContext object in your main program (called the driver program). ?SparkContext is the framework main driver program which we elicit thru exit call role-reversal (cmp with MapReduce)
						To run on a cluster
							SparkContext can connect to several types of cluster managers (spark standalone, Mesos, YARN) for resource allocation across application.
							Once connected, Spark acquires executors on nodes in the cluster; executors - processes that run computations and store data for your application
							spark sends your application code to the executors.
							SparkContext sends tasks to the executors to run
						application gets its own executor processes in separate JVM from other application's executor processes.
							executor runs in multiple threads with no interference and ability to share with other executor except thru persistence.
							each driver schedules its own tasks.
						Spark is agnostic to the underlying cluster manager - it justs gotta acquire executor processes
						The driver program must listen for and accept incoming connections from its executors throughout its lifetime - so spark.driver.port in network config file.
							driver program must be network addressable from the worker nodes
						driver schedules tasks on the cluster => it should be run close to the worker nodes => Same LAN OR Open RPC to driver and submit operations from nearby.
						Cluster Managers
							Spark - Simple one included with Spark for easy cluster set up.
							YARN - Hadoop2.
							Apache Mesos - General CM which supports Hadoop.
							K8 - OSS auto-deployment, scale, mgmt of containerized apps.
						Submit Applications to Application
							spark-submit script
						Monitoring UI - node:4040 (typically) for tasks, storage, usage.
						Job Scheduling - control over resource allocation both - ...
							across apps (level of CM) - ?resources across applications.
							within apps (if n computations on same spark context) - ?resource usage per application
						Concepts
							Application - User program built on Spark (Driver + Executors on nodes)
							Application jar - user's app as jar - uber jar with dependencies but never hadoop and spark dependencies.
							Driver program - process running the main() function of the application and creating the SparkContext
							Cluster Manager - external service for acquiring resources on the cluster
							Deploy mode - Distinguishes where the driver process runs.
								cluster mode - framework launches the driver inside of the cluster
								client mode - submitter launches the driver outside of the cluster
							Worker Node - Any node that can run application code in the cluster
							Executor - A process launched for an application on a worker node
								The process runs tasks - keeping data in memory or disk storage across them.
							Task - work that will be sent to one executor
							Job - parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (save, collect)
								can see this term in logs.
							Stage - Each job gets divided into smaller sets of tasks called stages that depend on each other
						Submitting Applications - ...
							Bundle dependencies
							Launch apps with spark-submit
								master URL
								Load config from file
							Advanced dependency Management.
				Spark UI
					Spark
						Worker nodes.
						Apps 
							Drivers - SubmitId, Worker Instance, MainClass, Core, Mem, TS, State.
								Running Completed.
							Running - AppId, User, Context, Cores, Mem, TS, Duration, State.
							Completed
					Spark Job Submit - 
						ingestionFlow - type abstract.
						sparkJobConfig
							jobCategory
							ContextProperties - hardware specifications
							additional - batch duration, zk commit counter.
				Regex
					Spark Context Creation 
						conf = new SparkConf().setAppName("App").setMaster("local[*]")
						val sc = new SparkContext(conf)
                </Spark>
            </Framework>
            <Scrum></Scrum>
        </Design>
        <Development>
            <PL>
                <Scala>
				CS - Scala
					scala.reflect.internal.MissingRequirementError: object java.lang.Object in compiler mirror not found.
					Need to have java version point to 1.8 and also, in IDE the selected version has to be 1.8
				object MainObject
					def main(args:Array[String]){
						statements
					}
				Where
					Web applications
					Utilities and libraries
					Data streaming with Akka
					Parallel batch processing
					Concurrency and distributed application
					Data analysis with Spark
					AWS lambda expression
					Ad hoc scripting in REPL
				Features
					Type inference
					Singleton object
					Immutability
					Lazy computation
					Case classes and Pattern matching
					Concurrency control
					String interpolation
					Higher order function
					Traits
					Rich collection set
				statements
					[var|val] variableName[:dataType] = literal
					var data:Int = 100
					dataType -
						Boolean
						Char, String
						Byte, Short, Int, Long,
						Float, Double
						Any
					selective
						standard if-else
						ternary - if (a >= 0) 1 else -1
						match case
							var a = 1
							a match{
								case 1 => println("One")
								case "Two" => println("Two")
								case _ => println("No"
								}
					iterative
						while, etc usual
						for(variable - list){}
							for( a - 1 to 10 )
							for( a - 1 until 10 )
							for( a - 1 to 10 if a%2==0 ){
								 println(a);
							}
							var result = for( a - 1 to 10) yield a
							for(i-result){
								println(i)
							}
							var list = List(1,2,3,4,5,6,7,8,9)          // Creating a list
							for( i - list){                         // Iterating the list
								println(i)
							}
							for(i-1 to 10 by 2)
						foreach - list.foreach(function)
							list.foreach{
								println
							}
							list.foreach(print)
							list.foreach((element:Int)=>print(element+" "))
						break, etc - not part of language but import scala.util.control.Breaks._
					comments standard.
					multiline expression - throw exception sometimes
					Functions
						def functionName(parameters : typeofparameters) : returntypeoffunction
						def functionExample(a:Int, b:Int):Int = {
							if(b == 0)          // Base condition
							 0
							else
							 a+functionExample(a,b-1)
						}
						with default value - parameters with some default value
						def functionExample(a:Int = 0, b:Int = 0):Int = {}
						named parameters - functionExample(b = 15, a = 2)
							functionExample(15,2); functionExample(a = 15, b = 2)
						Higher Order Functions
							functionExample(25, multiplyBy2)
							def multiplyBy2(a:Int):Int = {
								a*2
							}
							def functionExample(a:Int, f:Int=>AnyVal):Unit = {
								println(f(a))
							}
							Function composing
								var result = multiplyBy2(add2(10))      // Function composition
						Anonymous functions
							rocket - var result1 = (a:Int, b:Int) => a+b
							wild card underscore - var result2 = (_:Int)+(_:Int)
							(result1(10,10)); result2(10,10)
						Function Currying
							multiple parameter lists
							def add(a:Int)(b:Int) = {
								a+b
							}
							var result = add(10)(10)
							var addIt = add(10)_
							var result2 = addIt(3)
						Function nesting
							def add(a:Int, b:Int, c:Int) = {
								def add2(x:Int,y:Int) = {
									x+y
								}
								add2(a,add2(b,c))
							}
							def main(args: Array[String]) = {
								var result = add(10,10,10)
								println(result)
							}
						Variable length parameter functions
							def add(args: Int*) = {
								var sum = 0;
								for(a - args) sum+=a
								sum
							}
							var sum = add(1,2,3,4,5,6,7,8,9);
							println(sum);
					Scala OOP
						Object and Class
							class Student{
								var id:Int = 0;                         // All fields must be initialized
								var name:String = null;
							}
							var s = new Student()
						Singleton object
							object SingletonObject{
								def hello(){
									println("Hello, This is Singleton Object")
								}
							}
							SingletonObject.hello()
						Companion Object
							class CompanionClass{
								def hello(){
									println("Hello, this is Companion Class.")
								}
							}
							object CompanionObject{
								def main(args:Array[String]){
									new ComapanionClass().hello()
									println("And this is Companion Object.")
								}
							}
						Case Classes and Case Object
							public and immutable by default
							case class CaseClass(a:Int, b:Int)
							object MainObject{
								def main(args:Array[String]){
									var c =  CaseClass(10,10)       // Creating object of case class
									println("a = "+c.a)               // Accessing elements of case class
									println("b = "+c.b)
								}
							}
							useful also for pattern matching - f match {case CaseClass(a,b)=> //do something case CaseClass2(a)=>//do something}
						Default Primary Constructor
							statements in class body.
							class Student(id:Int, name:String){
								def showDetails(){
									println(id+" "+name);
								}
							}
						Secondary Constructor
							class Student(id:Int, name:String){
								var age:Int = 0
								def showDetails(){
									println(id+" "+name+" "+age)
								}
								def this(id:Int, name:String,age:Int){
									this(id,name)       // Calling primary constructor, and it is first line
									this.age = age
								}
							}
						Constructor overloading
							class Arithmetic{
								def add(a:Int, b:Int){
									var sum = a+b
									println(sum)
								}
								def add(a:Int, b:Int, c:Int){
									var sum = a+b+c
									println(sum)
								}
							}
							object MainObject{
								def main(args:Array[String]){
									var a  = new Arithmetic();
									a.add(10,10);
									a.add(10,10,10);
								}
							}
						Method Overloading with different data types
						this - this(), this.variable
						Inheritance
							class SubClassName extends SuperClassName(){}
							Single, multi-level, hierarchical, multiple, hybrid.
						Overriding
							class Vehicle{          //use final to stop
								val speed:Int = 60  //use final to stop
								def run(){          //use final to stop
									println("vehicle is running")
								}
							}
							class Bike extends Vehicle{
								override val speed:Int = 100        //OR use var in parent and don't declare here
								override def run(){
									println("Bike is running")
								}
							}
						Abstract class
							abstract class Bike{
								def run()
							}
							class Hero extends Bike{
								def run(){
									println("running fine...")
								}
							}
							it has constructor but no object
						Trait
							interface with partial implementation
								trait Printable{
									def print()
								}
								abstract class A4 extends Printable{            // Must declared as abstract class if not gonna implement abstract method
								   def printA4(){
									   println("Hello, this is A4 Sheet")
								   }
								}
							var or val variables both allowed - can have abstract and non-abstract members
								trait Printable{
									def print()         // Abstract method
									def show(){         // Non-abstract method
										println("This is show method")
									}
								}
								class A6 extends Printable{
									def print(){
										println("This is print method")
									}
								}
							Multiple traits
								trait Printable{
									def print()
								}
								trait Showable{
								   def show()
								}
								class A6 extends Printable with Showable{
									def print(){
										println("This is printable")
									}
									def show(){
										println("This is showable");
									}
								}
							Mixins - extends PrintA4 with Print (first class or abstract class and then traits)
						Access Modifiers
							None - protected + package + outside package access
							protected - private + subclass access
							private - class or companion access.
					Exceptions
						try{
							a/b
						} catch{
							case e: ArithmeticException => println(e)
							case ex: Exception =>println(ex)
							case th: Throwable =>println("found a unknown exception"+ ex)
						} finally{
							println("Finaly block always executes")
						}

						throw new ArithmeticException("You are not eligible")
						throws keyword or throws annotation
							class ExceptionExample4{
								@throws(classOf[NumberFormatException])
								def validate()={
									"abc".toInt
								}
							}
						Custom Exception
							class InvalidAgeException(s:String) extends Exception(s){}
							class ExceptionExample{
								@throws(classOf[InvalidAgeException])
								...
							}
					Collection
						Array
							var arrayName : Array[arrayType] = new Array[arrayType](arraySize);     //new Array[Int](2)
							var arrayName = new Array[arrayType](arraySize)
							var arrayName : Array[arrayType] = new Array(arraySize);                //new Array(5)
							var arrayName = Array(element1, element2 ... elementN)                  //Array(1,2,3)
							arr(2); loop for all; arr.foreach((element:Int)=>println(element))      //ops
							var arrayName = Array.ofDim[ArrayType](NoOfRows,NoOfColumns)            //Array.ofDim[Int](2,4)
							var arrayName = Array(Array(element...), Array(element...), ...)        //Array(Array(1,2), Array(3,4,5))
							Eg -
							var arr = Array(Array(1,2,3,4,5), Array(6,7,8,9,10))
							var arr = Array.ofDim[Int](2,2)
							var arr1 = Array(Array(1,2,3,4,5), Array(6,7,8,9,10))   // Creating multidimensional array
							var arr2 = Array(Array(1,2,3,4,5), Array(6,7,8,9,10))
							var arr3 = Array.ofDim[Int](2,5)
							for(i- 0 to 1){               // Traversing elements using loop
							   for(j- 0 to 4){
									arr3(i)(j) = arr1(i)(j)+arr2(i)(j)
									print(" "+arr3(i)(j))
								}
								println()
							}
						String
							Immutable - s1 = "Something " + s1
							equality works - s1 == s2 is true if same strings.
							methods
								equals (==)
								compareTo - lexicographically
								concat (+)
								substring(startIndex, endIndex)
							String interpolation
								println(s"value of pi = $pi") - s for string - allows us to pass variable in string object
								println(f"This is $s1%s, scala version is $version%2.2f") - f for formatting
								var s1 = "Scala \tstring \nexample" - println interprets \t and \n specially.
								var s2 = raw"Scala \tstring \nexample" - println won't interpret \n as newline
						Tuple
							var tuple = (1,5,8,6,4)                     // Tuple of integer values
							var tuple2 = ("Apple","Banana","Gavava")        // Tuple of string values
							var tuple3 = (2.5,8.4,10.50)                // Tuple of float values
							var tuple4 = (1,2.5,"India")                // Tuple of mix type values
							println(tupleValues._1) // Fetching first value
							println(tupleValues._2) // Fetching second value
							tupleValues.productIterator.foreach(println)
						Others
							Traversable
								Iterable
									Set - HashSet, BitSet, ListSet
										SortedSet
										TreeSet - Abstract class
									Sequence -
										IndexedSeq - Vector, NumericRange, String, Range
										LinearSeq - List, Stream, Queue, Stack
									Map - HashMap, ListMap
										SortedMap - TreeMap
							Traversable - head, init, isEmpty, last, max, min, size, sum, tail, toArray, toList, toSeq, toSet
							Set - Store unique elements without ordering
								val games = Set("Cricket","Football","Hocky","Golf")
								games += "Racing"
								games -= "Golf"
								merge - games ++ alphabet
								contains
								for, foreach
								intersect, union.
								More - SortedSet(5,8,1,2,9,6,4,7,2)
							HashSet - neither maintains insertion order nor sorts the elements
							BitSet - maintains arrays of bits - BitSet(1,5,8,6,9,0)
							ListSet - immutable sets using a list-based data structure
								maintains insertion order. suitable only for a small number of elements.
								create empty - ListSet.empty or ListSet()
							Seq - represents indexed sequences that are guaranteed immutable
								access elements by using their indexes. It maintains insertion order
								var seq:Seq[Int] = Seq(52,85,1,8,3,2,7)
								seq(2)
								contains, copyToArray(xs: Array[A], start: Int, len: Int): Unit
								endsWith[B](that: GenSeq[B]): Boolean - seq.endsWith(Seq(2,7))
								head, indexOf, isEmpty, lastIndexOf,
								reverse - seq.reverse
							Vector - import scala.collection.immutable._
								general-purpose, immutable data structure. Good for large elements.
								var vector = Vector("Hocky","Cricket","Golf")
								vector :+ "Racing"
								newVector ++ vector2
								mergeTwoVector.reverse
								mergeTwoVector.sorted
							List - class for immutable linked lists. Good for LIFO - stack. Ordered with duplicates.
								foreach, random access - true for all.
								list ++ list2
								list3.sorted
								list3.reverse
							Queue - FIFO DS.
								var queue2:Queue[Int] = Queue(1,5,6,2,3,9,5,2,5)
								queue.front
								queue.enqueue(100)
								var dequeueQueue = queue.dequeue
							Stream - Stream is a lazy list - evaluates when required.
								val stream = 100 #:: 200 #:: 85 #:: Stream.empty
								var stream2 = (1 to 10).toStream
								println(stream) - Stream(100, ?)
								stream2.head
								stream2.take(10)
								stream.map{_*2}
							Maps - to store elements as KV pairs.
								var map = Map(("A","Apple"),("B","Ball"))
								var map2 = Map("A"->"Aple","B"->"Ball")
								var emptyMap:Map[String,String] = Map.empty[String,String]
								map("A")
								var newMap = map+("C"->"Cat")
								removeElement = newMap - ("B")
							HashMap - use hash code to store elements
								var hashMap = new HashMap()
								var hashMap2 = HashMap("A"->"Apple","B"->"Ball","C"->"Cat")
								println(hashMap)
								hashMap.foreach {
									case (key, value) => println (key + " -> " + value)
								}
								hashMap+("D"->"Doll")
							ListMap - immutable maps by using a list-based data structure. maintains insertion order
								var listMap = ListMap("Rice"->"100","Wheat"->"50","Gram"->"500")    // Creating listmap with elements
								var emptyListMap = new ListMap()            // Creating an empty list map
								var emptyListMap2 = ListMap.empty           // Creating an empty list map
					File Handling - scala.io
						val fileObject = new File("ScalaFile.txt" )     // Creating a file
						val printWriter = new PrintWriter(fileObject)       // Passing reference of file to the printwriter
						printWriter.write("Hello, This is scala file")  // Writing to the file
						printWriter.close()             // Closing printwriter
						Character-wise reading
							val filename = "ScalaFile.txt"
							val fileSource = Source.fromFile(filename)
							while(fileSource.hasNext){
								println(fileSource.next)
							}
							fileSource.close()
						Line wise reading
							val filename = "ScalaFile.txt"
							val fileSource = Source.fromFile(filename)
							for(line - fileSource.getLines){
								println(line)
							}
							fileSource.close()
						For more, use java.io package to access file methods
					Multithreading - multitasking by using Multithreading - lightweight sub-processes
						About
							create Thread by extending Thread class or Runnable interface
							start() - new to runnable state.
							OS - runnable to running.
							sleep, i/o, lock, suspent, wait - running to blocked or non-running.
								back to runnable after over.
							run() exits - running to termination
						class ThreadExample extends Thread{
							override def run(){
							  println("Thread is running...");
							}
						}
						new ThreadExample().start()
						Runnable
							class ThreadExample extends Runnable{
							override def run(){
							println("Thread is running...")
							}
							}
							new Thread(new ThreadExample()).start()
							Method
								getName()
								getPriority()
								getState()
								isAlive() - if started.
								join() - wait for thread to die.
								run() - if separate runnable object then run.
								setName, setPriority, sleep, yield - pause for other threads to proceed.
							Eg - t1.start()
							t1.join()
							t2.start()
							Eg - t1.setPriority(Thread.MIN_PRIORITY)
							Eg - observe concurrency.
							t1.start()
							t1.task()
                </Scala>
                <JAVA>
                    JAVA - DM (behavioral abstract study - no DFD levels here)
                        Overview
                            Basics DT
                            Operators
                            Control Flow - Regular ones.
                        Systems - File Handling, Multithreading, Sockets, Security.
                            IO
                        OOP - Inheritance, Polymorphism, Overriding
                            Exception Handling
                        ADT - Imp DT and Data Structures - String, Array, Stream.
                            Collections
                        Functional - From JAVA 8
							Concept - Lambda is Interface implementation with 1 method.
								Greeting innerClassGreeting = new Greeting() {
									public void perform() {
										System.out.print("Hello world!");
									}
								};
								greeter.greet(() -> System.out.print("Hello world!"));
								greeter.greet(innerClassGreeting);
							new Thread(() -> System.out.println("Printed inside Lambda Runnable")).run();
							Collections.sort(people, (p1, p2) -> p1.getLastName().compareTo(p2.getLastName()));
							printConditionally(people, p -> p.getLastName().startsWith("C"));
								Internally -
								printConditionally() {
									for (Person p : people) {
										if (condition.test(p)) {
											System.out.println(p);
										}
									}
								}
							Some famous lambdas
								Biconsumer, consumer - function used as parameter which inputs 1 or 2 args and returns nothing.
								Producer - function used as param which consumes no argument and returns something.
								Predicate - function param which consumes 1 arg and return boolean.
								Eg -
								performConditionally(people, p -> p.getLastName().startsWith("C"), p -> System.out.println(p));
								private static void performConditionally(List(Person) people, Predicate(Person) predicate, Consumer(Person) consumer) {
									for (Person p : people) {
										if (predicate.test(p)) {
											consumer.accept(p);
										}
									}
								}
							Exception handling - use a wrapper lambda.
								Eg -
								process(someNumbers, key, wrapperLambda((v, k) -> System.out.println(v / k)));
								private static BiConsumer(Integer, Integer) wrapperLambda(BiConsumer(Integer, Integer) consumer) {
									return (v, k) ->  {
										try {
											consumer.accept(v, k);
										}
										catch (ArithmeticException e) {
											System.out.println("Exception caught in wrapper lambda");
										}

									};
								}
								process(int[] someNumbers, int key, BiConsumer(Integer, Integer) consumer) {
									for (int i : someNumbers) {
										consumer.accept(i, key);
									}
								}
							Method Reference - same as lambda picked from some class method.
								Eg -
									Thread t = new Thread(MethodReferenceExample1::printMessage);
									printMessage() {
										System.out.println("Hello");
									}
								Eg
									performConditionally(people, p -> true, System.out::println);
									performConditionally(List(Person) people, Predicate(Person) predicate, Consumer(Person) consumer)
							streams
								people.stream()
									.filter(p -> p.getLastName().startsWith("C"))
									.forEach(p -> System.out.println(p.getFirstName()));
								long count = people.parallelStream()
									.filter(p -> p.getLastName().startsWith("D"))
									.count();
                        Misc
                            Important Keywords
                            vs CPP
                            Methods
                            Constructors
                            lambda
                        Libraries - Math, Lang, Time, Network, Image, Advanced, Tuple, Wrapper.
                        Uncat - Internals - Package, Reflections, Garbage Collection, NIO.
                    JAVA PM
                        JAVA Source Code - create JAVA file
                            Main class with static method - containing statements
                            Class Orchestration - OOP concepts over classes.
                                Design Patterns
									Strategy - Class behavior at runtime.
										Observer - trigger a behaviors in subscribers.
										Template Method - abstract behavior extracted out.
										Command - commands to behavior mapping.
										Mediator - middleman for communication.
										Memento - state recall.
										State - behavior changes with state.
										Structural
										Adapter - Make class behave like other.
										Facade - representative class for encapsulating all behaviors.
										Composite - Make object and composition behave similarly.
										Decorator - add combination permutation of similar behaviors.
                            All Class Logic - DSA over statements.
                        JAVA Compile - javac command creates bytecode (.class files - name same as classes not necessarily file names).
                            classpath, dependencies
                        JVM Run - java process on class file to start the run; also jar.
                            uses JIT
                            classpath
                    JAVA Regex
                        PM - Grammar Syntax
                            Application -> Package*
                            Package -> ProgramFile*
                            ProgramFile -> Program
                            Program -> Imports* class+
                            class ->  AccessControlModifier NonAccessModifier class className [Inheritance] { MemberVariable MemberMethod }
                            MemberVariable -> AccessControlModifier NonAccessModifier DataType VariableName [Initialize]?;
                            MemberMethod -> AccessControlModifier NonAccessModifier ReturnType MethodName ( Arguments ) { MethodBody }
                            AccessControlModifier -> default|private|public|protected
                            NonAccessModifier -> static|abstract|final|synchronized|volatile
                            ReturnType -> DataType|ClassName
                            Arguments -> [DataType|ClassName []? ,]*
                            MethodBody -> VariableDeclaration Statements*
                            VariableDeclaration -> DataType LocalVariable [= Variable | Constant]
                            Statements -> (VariableDeclaration | SimpleStatement | ControlStatement | FunctionCall | IO)+
                            ControlStatement -> DecisionStatement | IterativeStatement | ExceptionBlock
                            ExceptionBlock -> try { Statements } catch(ExceptionClass) { Statements } finally { Statements }
                            SimpleStatement -> ( LocalVariable Operator )* LocalVariable	//ignoring unary and ternary.
                            Operator -> Assignment | Arithmetic | Relational | Logical | Bitwise
                            VariableName, MethodName, ClassName, LocalVariable -> Identifier
                            Identifier -> same rules as CPP.
                            DataType -> PrimitiveDataType | NonPrimitiveDataType | UserDefinedDataType | Collection
                            PrimitiveDataType -> ...
                            NonPrimitiveDataType -> Arrays | String.
                            UserDefinedDataType -> ...
                            Collection -> Set | List | Map.
                        RuntimeBehaviors
                            Inheritance -> extends SuperClass | implements Interface
                            Polymorphism -> exact signature called.
                            Overriding -> Method from SuperClass, Dynamic Method Dispatch.
                            Abstraction | Interface
                            Encapsulation
                            Packages
                            ===
                            SuperClass -> ClassName
                    More - JAVA DMPM - For more languages just plug the delta not everything.
                </JAVA>
                <Python>
                    DM
                        Basics
                        IO - DB also
                            open("test.txt", "wb")
                            test_file.write(bytes("Write me to the file\n", 'UTF-8'))
                            test_file.close()
                            test_file = open("test.txt", "r+")
                            text_in_file = test_file.read()
                        Misc - Variables, operators, functions.
                        Control Flow - eg should be moved to grammar portion later in abstract form.
                            normal
                                    a = b // 5; a**2 #comment
                                    a >> 2
                                    a le; b and b >= c
                            selective
                                    if i lt; 5:
                                            print('less')
                                    elif i == 5:
                                            print('equal')
                                    if substring in string:
                                            print('found')
                            iterative
                                    while (count lt; 9):
                                            print('The count is:', count)
                                            count = count + 1
                                    else:
                                            print('loop finished')
                                    for index in range(len(fruits)):	#iterate over list
                                            print('Current fruit :', fruits[index])
                            functions
                                    def func(self, arg):
                                            self.var = arg
                                    abs(-5)
                            others
                                    ''' multiline comment
                                    # single line comment
                                    \ for next line
                                    ' for word " for sentence """ for paragraph.
                        Data Types and regex
                            num - int, hexa, octa, long, float, complex
                                    arithmetic, logical, relational, etc.
                                    typecast - int("5")
                            bool
                                    logical
                            string
                                    concat - *2, +,
                                    substr - [[m]?:[n]?], long_string[-5:]
                                    replace - str.replace("is", "was")
                                    find - string.find(substring)
                                    split - long_string.split(" ")
                                    length - len(str)
                                    misc - capitalize, isalpha, isalnum
                            list - like flexible array
                                    concat - self.result = self.list11 + self.list12 + [1, 2], *4
                                            grocery_list.append('onions'), insert(1, "Pickle"), remove("Pickle"), sort(), reverse(),
                                    substring - self.list11[2:]
                                    length - len(list)
                                    nesting - list of lists - nested_list[1][1]
                            tuple
                                    concat - *2, etc
                                    substring - self.tuple11[1:3]
                                    length - len(tuple)
                                    typecast - list(pi_tuple)
                            dictionary - like struct
                                    getVal - dict1['one'], dict1.get("Pied Piper")
                                    allKeys - tinydict.keys()
                                    allValues - tinydict.values()
                                    replace - super_villains['Pied Piper'] = 'Hartley Rathaway'
                                    length - len(super_villains)
                        OOPS
                            Inheritance - class Dog(Animal): ...; overriding, overloading.
                                class Foo(Bar):
                                    def baz(self, arg):
                                        return super(Foo, self).baz(arg)
                        Exception Handling
                        Collections
                        Django
                        Data Analysis
                        Numpy
                        Pandas
                        ML
                        GUI
                        Libs - OS, Calendar, Timit, etc.
                        Misc, Applications
                    PM - Grammar
                        ...
                </Python>
            </PL>
			<DSA>
			CS - DSA Approach to problem solving for any puzzle
				Data structures - Organize your data properly to solve with great performance (least effort) and ease.
					Lang bare Capabilities - Arrays, pointers
						Exploring possibilities of combinations and self references.
					Data Structures - ability to reduce problems to data structures.
						Sets, Lists
						Maps, Trees, Graphs, etc.
					Standardizations - ADT, Collections
						Comparisons, operations and Time complexities.
				Algos - Approach taken to solve abstract problem, thus all instances thereof.
					Mantra - Mixture of following
					Housekeeping - keeping track of details while solving.
					Logic
						Take an approach case and prove that wrong.
						Go exhaustive with PnC and reason which is the best.
					Induction
						linear scale
						Geometric scale - DnC
					Backtracking - final solution tryst.
						Brute force - try every candidate solution.
						Greedy - Inductive approach of scaling with best now is best always.
						BnB - Pruning.
						Dynamic Programming - Memoization.
					More
						Linear Programming, Approximation algorithms, etc.

			</DSA>
        </Development>
        <Deployment>
            <GIT_VIM_SHELL></GIT_VIM_SHELL>
            <Maven>
            CS - Maven from LinkedIn Official Learning for DMPM
                maven
                    requisites - java, maven, ide - all path variables.
                    POM - project object model.
                        project management and comprehension tool - original for building.
                        easy, uniform build, provide information, provide guidelines and best practices.
                        support migration.
                        easy to use, great community support, reliable.
                    POM - a set of standards, a project lifecycle, dependency management system, logic for executing plugin goals at defined phases in a lifecycle.
                        project description, unique set of coordinates, project attribs, license.
                        version, authors and dependencies.
                        coordinates = group, artifact, version.
                Default file structures
                    root
                        src, 
                            main
                                java
                                resources
                            test
                                java
                                resources
                        pom.xml
                pom file
                    repositories
                        pluginRepositories - points to plugins used by maven itself.
                        Maven repo - central open source - search.maven.org/#search
                            local repo - at .m2 - all your dependencies and projects in here.
                    profiles - to override configurations
                        this is org specific. like dev org, test org, etc.
                    details - group artifact version.
                    reactors and parent
                        provided versions dependencies and plugins - specify no more properties version and dependencies.
                        reactor is sequence the package sequence.
                        useful for making shared client libraries for web services for eg.
                        observe reaction build sequence b4 starting.
                    properties
                    dependency
                        details and scope is compile by default
                    build - plugins to build your project.
                        plugins - similar to dependency but more coordinates.
                            eg - use java 11.
                        eg - compiler and 3rd party.
                        compiles to bin and moves to target directory and packages it.
                    report - report info about project.
                        surefire report for test coverage.
                        mvn clean package site
                        open target/site/index.html
                    archetype - powerful constructs although not core.
                        project template using maven.
                        specified when starting - useful for making consistent artifacts.
                        speed to market, consistent standards.
                        can create your own for boiler plates.
                Lifecycle - 3 by default - build plugins are of all the lifecycles.
                    clean - cleaning
                    default - main lifecycle
                        Phases in it - to be executed in order.
                            validate, compile, test, package, verify, install, deploy.
                            phase made up of goals.
                            goals are individual tasks in phase.
                            triggered individually - mvn dependency:analyze - specific goal.
                            avoid running full phases.
                        Build plugins are part of all lifecycles - 3 types of them - core, packaging, tools.
                            tools - variety of uses, release, signing, dependency.
                                dependency plugin
                                    see usage - lots you can check.
                                enforcer plugin - force usage like specific java version.
                                rules can be set inside executions section of pom with rules.
                                jarsigner - PKI encryption signing. keystore certificate.
                                    signed jars for using application servers - for security.
                                release plugin
                                    most used. Make it and release it - preparing project for next iteration.
                                More
                                Scope
                                    default - compile.
                                        dependency is always available.
                                        dependency is propagated. when used in dependency - transitive.
                                    provided - similar to compile. won't see dependency in war but at runtime we assume it.
                                        only useful during compilation but not packaged cuz runtime will have these.
										difference is in the classpath listed in the MANIFEST.MF file included in the jar if addClassPath is set to true in the maven-jar-plugin configuration. 'compile' dependencies will appear in the manifest, 'provided' dependencies won't.
                                        common in enterprise but not in cloud - cuz violates 12 factor principle.
                                        not transitive.
                                    runtime - useful when multiple versions of api.
                                        only for execution. not needed for compilation.
                                        only seen in runtime and test classpaths.
                                    test scope - reduce size - needed only for testing. Test compilation and execution classpaths.
                                        unit test frameworks. not transitive.
                                    less common - system - similar to provided but you gotta specify the location.
                                        import - applies to pom files.
                                Transitive dependencies
                                    Dependency of dependencies - only care for what you know not further dependencies.
                                    closest version - degrees of separation from the root.
                                    dependency management section beats closer version.
                                    scope plays a role.
                                    local definitions rules them all.
                                    Tricks
                                        only declare what you need.
                                        validate scope.
                                            use dependency analyse.
                                        consider parent pom to control version.
                                        always declare when risk of breaking.
                                        always declare when risk of security.
                                Dependency management
                                    for orgs to control actual dependencies within it.
                                    paste properties in parent. dependency management in parent.
                                    remove scope and versions from child project.
                                    similarly for plugin management.
                                Dependency plugin
                                    mvn dependency:analyze - tells warnings of what you use in code and get thru transitive dependencies so overly dependent on them not changing things.
                                    mvn dependency:resolve - lists all declared dependencies linearly.
                                    mvn dependency:tree - to show something on our classpath.
                            core most used - jdk to make bytecode, installation to put in m2.
                                deployment to push remote, validation for validating source code.
                                compiler - does compilation of code.
                                    mvn compile and test-compile
                                    configuring it - section. under pluginManagement in the parent pom.
                                deployment - use distributionmanagement tag to push to remote coordinates.
                                    7 hands you a jar file - create a pom from that.
                                    deploy:deploy-file
                                resources - eth you reference is put into your package - css, js, etc from resources.
                                surefire plugin - convert to site doc.
                                    to execute junit tests. halt when it fails.
                                failSage for integration testing.
                                useful for large scale CICD then necessary.
                            packaging - compiled bytecode and package s.t consumable by jvm.
                                jar is default packaging plugin to make jar file which can be loaded by class loader of the jvm.
                                war by web application server.
                                ear - enterpise; shade - for uber jar - more to do with dependency then packaging or build.
                                when you trigger package this internally gets executed.
                                war file - web archive goes into web application server.
                                jar plugin
                                    default plugin.
                    site - documentation generating
                Maven Multi-module
                    IDE Integrations - Maven integrations with IDE from certain version - plugins b4.
                    Inheritance - configuration from parent. parent child.
                        doesn't quote its own coordinate other than artifact.
                        defined from topmost level downwards.
                        multimodule has parent child relationship but also modules tag in the parent.
                        Any child dependencies are defined in parent object (POM)
                        parent can specify commonly used dependencies.
                            inherited projects automatically get them.
                            submodule identifies all projects in the parent project and inheritance is vv.
                    Reactor - when using multi-module it is used
                        collects all available modules.
                        sorts the projects into the correct order.
                        builds in the order.
                        create from IDE - as Maven, parent project.
                        ?it's layered way of modules not packages in the project.
                        IDE - dependencies pulled automatically so just run them in sequence of dependency and parent in last.
                    Maven Profiles
                        provide ability to customize a particular build.
                        can be written for dev, test or deploy.
                        separate envt for different stakeholders.
                        profie provides an alternative set of values.
                        eg - profiles, profile, id for production, build, plugins, plugin, coordinates.
                            config - debug to false.
                    Writing plugins
                        all work is done by plugins - plugin execution framework at heart.
                        every plugin is artifact that has n Mojos - Maven Plain old java objects.
                        Mojo is a goal in Maven - compiler:compile goal is a mojo.
                        3 parts to plugin descriptor - top level config coordinates.
                            declaration of mojos.
                            declaration of dependencies.
                        maven-archetype-mojo to create a plugin.
                        default code comes - change it to make your code.
            Regex
                mvn -v
                commands
                    clean - remove the bytecode files (to ensure deleted source classes are removed)
                    validate - check schema fine.
                    compile - compile sources.
                    test - run tests
                    package - make jar, war, etc.
                    verify - ???
                    install - into local repo.
                    deploy - into central
                Options
                    -DskipTests
                    -o - offline
                Dependency related
                    dependency:resolve
                    dependency:tree - very important for nesting.
                Plugin-specifics
                    mvn eclipse:eclipse
            </Maven>
        </Deployment>
		<Maintenance>
		Incident Solving Context Flow Diagrams
			L0 - Is user's concern legitimate - Business Level - 
				Feature - do we support? 
					Yes - go to L1.
					No - Put in a request for future.
			L1 - Functionality Level
				What was the problem - Click path or steps that show a problem
				Explore Expectation vs Actual. Refer changes or History for different transactions over a business scenario.
				Was this expected?
					Yes - Inform customer with consultation on how to use things. 
					No - but there is instant solution - Workaround.
					No - and We need to dig the transaction - Go to L2.
			L2 - Architectural Level.
				Map UI or User State with the persistence State. Is persistence or utils already in expected state?
					This means you gotta know the Tables and its state transition diagram at persistence level - FSM.
					Yes - We need to trace the logic - Go to L3.
					No - Somewhere the persistence has been modified against expectation.
						Immediate fix - reinstate the persistence or util.
						Permanent Fix - Take steps to ensure bug fix.
			L3 - Implementation Logic Level
				Is it reproducible or simulate-able.
					Yes - debug the application with it thru debugger or logs.
					No - Try from your end to replicate or simulate the user conditions refering to transaction history over business scenario.
						Still No - Give a workaround alternative to customer so that business scenario continues and ask him to reproduce.

		</Maintenance>
    </DevOps>
    <DataScience>
        <Tools>
            <Excel></Excel>
            <Tableau></Tableau>
            <IDE>
                RStudio, Spyder, Jupyter, pycharm
            </IDE>
            <Hadoop></Hadoop>
			<RLibs>
				Text mining in R
					frequency; correlations
					classification naive bayes, 
					sentiments by classification algos; sentiments by function (positive - negative words)
					Topics by frequent unique terms LDA algo.
					Network analysis
					clustering
					NLP - Regex, Processing, Named entities, patterns, topic modeling
					feature extraction, text classification.
					words similarity like king and man. predict next word, spam classification using cnn.
			</RLibs>
            <Tensorflow></Tensorflow>
            <Keras></Keras>
        </Tools>
    </DataScience>
</CS>