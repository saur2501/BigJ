<?xml version="1.0" encoding="UTF-8"?>
<UtilsForAppDev>
    <About>
        My Defn - The softwares that are gonna be consumed by technical users of CS.
        Focus - how are these softwares made (like the case with Systems Software section also)
        Studies the workings of libraries and frameworks that will be used by application programs that directly serve the end users.
        We are gonna consume services from this Utils. These are not just apps as eg.
        The scope covers headers for different feature types - starting from types of persistence, geography based features, ML based, etc.
        This is different from official definition of https://en.wikipedia.org/wiki/Category:Utility_software_types.
        Email client isn't util cuz it can be easily built using libs and frameworks.
        Eventually this should merge into CS_DM_PM
    </About>
    <HighLevelViewOfCapabilities>
        About - all the CS capabilities at our disposal to make Apps - not only important for high level picture of an architect but also for entrepreneur to leverage.
        Front end - DS capabilities (D3, etc), bootstrap looks, dynamic content, CSS looks; 
                speech recognition and multimedia (make sound), NLP.
                IOT - any sensing and actuating 
                        ML Applications - includes object detection.
        Logic - PL, OOAD, DP, DSA.
                Big Data - MR, Spark, Hadoop Libs.
                Frameworks - for Development ease.
                DS - AI, Stats, ML - Pattern recognition.
                        LA (for programming util, Gaming world, ML util, etc)
                Data Security (GDPR also), UXD, Performance Benchmarking, etc.
                CN - Coordinate or communicate with other machines.
                IO - TOC thru regex or grammar, NLP, connectors.
        Persistence - Data Modeling, DBMS, NoSQL, Hadoop.
        Environment - OS, Platform, IDE, Shell, Deployment, Distributed.
        Understandings - Discrete Maths Logic, set theory (functions and relations - graph theory, recurrence, PnC)
            Offshoots - Probability and stats, Algebra (or group theory, Lattice, closure, abstract world governing laws - quadratics, linear equations, recurrence pattern, Scaled to linear algebra, etc)
    </HighLevelViewOfCapabilities>
	<IDE>
		<DevelopmentSoftwares>IDE, Parsers and compilers</DevelopmentSoftwares>
	</IDE>
	<FrontEnd>
		Browser - how it works.
		Extensions.
	</FrontEnd>
	<Backend>
		Web Servers - how it works. (App Servers, DB Servers)
		Web Frameworks - Express, ASP, JSP, etc.
	</Backend>
	<ExecutionEngines>
		<DistributedComputing>Eg - hadoop.</DistributedComputing>
	</ExecutionEngines>
	<Platforms>
		<Shell>

		</Shell>
	</Platforms>
	<Persistence>
    <DBMS>
        <DataModelingConcepts>
            <Evolution>
                What?
                    Database is a structure repository for data
                        It is comprised of data and meta data corresponding to the data
                        The repository structure (schema) complies with a particular data model
                        DBMS- is the set of softwares that interact with OS to enable CRUD operations and managing data.
                How?
                    stored in flat files in Disk but lotta structuring done and facade created to enable integrity constraints.
            </Evolution>
            <Modelling>
                    <What>
                        Models are just partial representation of some aspects of reality.
                            So, mapping from 1 to another is an effort.
                        REL representation of a person or thing, typically on a smaller scale
                            something used as an example- a simplified description
                        <Types>
                                Real
                                        Physical aspect to it.
                                        aircraft,building model.
                                Abstract
                                        Nothing physical to it.
                                        Eg- mathematical models to understand sth
                                                financial forecasting.
                                                weather forecasting.
                                                Data Model.
                        </Types>
                        <Characteristics>
                            Less Expensive
                            Prototype and Design
                            Not real
                            Visual representation
                            Effective/not effective
                            smaller scale
                            set of rules?
                            Different layers of abstraction- model corresponding to each one of them
                            Different from real
                            Easy to use- simulator vs being in cockpit
                            can occupy less space- paper model of building
                            can be a reasonable substitute st- simulator to learn driving
                            is a poor substitute- real has scale,cost put in,eth at best
                            Less professional handling reqd.
                            can be thrown away- penanlty of mistake not huge.
                        </Characteristics>
                        <Eg>
                                Model for dress
                                Building Model(2-D,3-D,Graphic)
                                aircraft Model(simulator,3-D,chart for components)
                                Financial Model,Weather forecast Model
                        </Eg>
                        <Summary>
                            Modelling is a process to abstract out from reality ;
                            see real system- nth understood!!
                            Imperative to tackle large problems.
                            Visual- Reality is presented in pictorial way.
                        </Summary>
                    </What>
                    <WHY>
                    Why Modelling in first place?
                            Real Entities may not be available- Dinosaurs!
                            Real are expensive to make- aircraft
                            Volumnous- Database
                            Real entity doens't exist then scale it
                            Less effort to make than real one.
                            Partial picture picked up- ignoring other aspects/layers.
                                    brainstorming on some aspects
                            Cost estimation
                            To get an overview/feel of product
                    </WHY>
                    <Viz>
                        <SDLC> Modeling of softwares
                        In SDLC- what you will present to the manager deliverable
                                Analysis/Requirements
                                        Business blue-print
                                        Use-cases
                                        Structured system analysis and design
                                        DFD
                                                Set of processes that affect data
                                                eg- SEZ
                                Design Models
                                        Structured Chart
                                                Hierarchical Modular form of system.
                                        State Diagrams
                                Coding- get downto the real thing- no escape from real
                                Production/Execution- Tell me about real system- see yourself- wires and current!!?
                                        Call Graphs- which fn calls which function
                                                static- if fixed
                                                dynamic- at particular runtime- which function gets invoked based on conditional statements!
                                                Check dead-code - if in general then there was some prob b4.
                                                hotspots of code- need to optimize
                                        Coverage Analysis
                                                fn testing
                                                50 fn tested thru this
                                        Performance Charts
                                                A chain of fn calls (time spent in each func-bottleneck)
                                        Infrastructure Model- Used by IT
                                                For IT, impossible to understand without models.
                                                Even IIITB with <1000 studs. Need dig to make decisions
                                                Network Diagrams.
                        </SDLC>
                        <DM>Refer next</DM>
                    </Viz>
            </Modelling>
            <DataModel>
                <What>
                    Defn- It is abstraction using notation, concepts to describe data
                        Notations- vocabulary-terms, notations, etc
                        Describe data- sufficient vocab/constructs to account for data behavior
                    Eg- 1 approach (RDBMS)
                            REL reality is messy
                            put all the related data in a row- columns fixed for different fields- label put on top.
                    <Schema>
                    Schema- it is specific Data Model's instance for a problem(structure).
                            Schema Evolution- Changing the schema of LIVE database.
                                Data name change
                                Data type change
                                Adding new data items
                                Removing data items
                            Data is instantiation of schema.
                            DB Design realized thru schema.
                    </Schema>
                    @DM is vocab/grammar, Schema is sentence, mapping is translation.
                    <UniversalDataModel>
                            Not universal Schema- for all apps, to describe their data
                            1 model that can successfully explain data to e1 is not possible
                                    Stakeholders Needs are different
                                            CEO/Business owner seeing data/bits- He wants to make sense out of it- not tables- maybe hierarchical structure.
                                                    show me pie-charts of high level.
                            Consider different cases and wonder if 1 can fit all
                                    Employee I/f
                                    Company hierarchical structure
                                            tell all people working under empId (of CEO)
                                    Network Infrastructure
                                            it is a graph. try with RDBMS
                                    Engine Desing
                                    Building Plan
                                    Road Infrastructure
                                            GIS
                                    Genetics
                                    Rich Media
                    </UniversalDataModel>
                </What>
                <Why>
                    Chaos of data we want to save in DB or so. So many names values, empId values all scattered.
                    Make sense out of it! Which elements belong to which- which values are interrelated
                </Why>
            </DataModel>
            <DataLifeCycle>
                    starts off when you Need Data (in your app)
                    Evaluate Business Processes- Logic we put to make sure that requirements are met.
                    Determining needed Data- Get data details
                    Gathering needed Data- find out methods like web service, keyboard, files, etc.
                            who will provide (which actor)
                    Storing Data- Local/Global Structures, DDL.
                    Using Data- DML
                    Delete Obsolete Data- Data has lost its significance-
                            eg- OTP, password,  Sesion Id, some purchase of 60s.
                    Archiving historical Data- Separate active/inactive data- doing analysis on them.
                    Eg- RDBMS has no vocab for archival- nowhere in DDL create for 1 year.
                    Definite outcomes at the end of every stage of Data.
                    If single DM can't do all the stages- jump from 1 Data Model to another, in a seamless manner (mapping rule be sound)
            </DataLifeCycle>
            <DBLayers>
                    Physical- All fields, files, records- how things are stored on PC.
                    Logical- relational- rows, cols, etc.
                    External- the view of a table made accessible to stakeholder.
                            Vertical and Horizontal sense- accounting, marketing, inventory people need to see relevant cols.
                            In RDM, views are nth but named select query. Remember its syntax.
                            External Schema is a restriction on logical schema.
            </DBLayers>
            <Variety>
                Structured data
                    Data is neatly grouped and organized
                    Every piece of data has a unique meaning associated with it
                    Data is byte addressable- you can tell where this byte belongs.
                    Example: all traditional databases
                Unstructured data
                    Data is represented without any visible structure to it
                    Example: A long paragraph of text, resume.
                Semi-structured data
                    A mix of structured and unstructured data (some parts are byte addressable; others are not)
                    Example: A book organized into chapters, sections and paragraphs; html tags.
            </Variety>
        </DataModelingConcepts>
        <Types>
            <DataModelTypes>
                <Traditional>
                    If we forget history we are condemned to repeat it.
                    Types based on way of organizing the data.
                    <PaperBasedSystems>
                            aka FileCabinetDatabases
                            Why? 
                                Benefits
                                    Quick to establish- shoebox or milk crate to get started.
                                    easy to use- no professional training reqd.
                                    Cheap!
                                Limitations
                                    Vulnerable- not secure. Fire, theft, misplacement,spilled coffee.
                                    Difficult to query- use cart catalog.
                                    Difficult to transfer data- make photocopies, mail hardcopies, organization of logical data sets tough (context)
                                    Scalability limitations- limited to ppl's fability and economy was regional.
                            How? Evolved- how did we? 
                                Why not use computers to improve data management?
                                IBM large no of transaction- more volume of transactions like companies IBM.
                    </PaperBasedSystems>									
                    <FlatFile>
                            Flat file Data model- Data is stored in numerous files like office- data repeats in more than 1 file- redundancy and inconsistency.
                                    What? Microsoft Excel vs fear of access- easy just add a field and stupid easy- lot of stuff quickly.
                                        Eg- Studs and grades- Cust order diff products.
                                    Why? 
                                        Ideal for small data.
                                        !Y
                                            Contains redundant data- duplicated- some upper case, lowercase. They don't look up b4 entrying.
                                            If many people access same excel, no1 responsiblity
                                            date formats, Spelling mistakes for names; 
                                            Same entity with slight changes for other relationships
                                                can filter but do you think about 2000 record in there (1000 repititions).
                    </FlatFile>
                    <Hierarchical>
                            What?
                                IBM had 1st attempt to model hierarchical relationships in ~1960-1970s
                                    meant to overcome probs of flat file approach
                                @We compare hierarchical filesystems with a tree structure of directories, 
                                        vs flat filesystems that have only one place that contains all files ~ single directory with no subdirectories.
                                Envisage- Every record type has to come under some entity and shares 1:N relp with it.
                                        Top down way of viewing business entities. Very organized.
                                                1 drawer/DB- each record type has 1 owner/parent and records linked like org chart.
                                                Eg- Which branch John White works- dive vertically to find out.
                                        for M:N, have elements duplicated or keep pointers to actual data under each of them.
                                        Vocabulary- ROOT in DDL, DML- no query language but host language (like cobol,PL1) given keywords/constructs to specify "what"
                                            Terms- Record Types ~ struct, Record ~ instance, PCR type ~ 1:N relp, PCR type instance.
                                        Has a tree like structure where 1 parent may have multiple children. But child has 1 parent (1:N)
                                        Series of DB are grouped together to form a family tree. Parent DB, Child Databases.
                            Eg- 1 Dept has many Studs,Profs- courses.
                            Eg- Hospital has many wards and Units. Ward has many patient and Doctors under it. Patients have many phases under them- Symptom,diagnoze,treatment.
                            Why? Save time spent searching the file. Tree saves the time.
                                Permissions
                                Flatfiles useful for embedded- small n simple- limited files to handle.
                                Benefits-
                                        Relative easy to understand
                                                conceptually very similar to File cabinet system
                                                Easy to migrate to electronic system.
                                        Relatively easy to develop and get started.
                                                Many entities or obj of similar structure
                                                easy to expand data storage folder.
                                        Profit generating due to speed.
                                                Data processing team- how much our sales improved- next day job. 15 sec now.
                                Disadvantage
                                        Difficulty in locating data (Search for Data is difficult and takes time)
                                                Lousy performance with large data collection
                                                        inefficient navigation in addition to bloated data
                                                        bogged down- big computer but referencing data same time.
                                                                1 pizza and all high school boys running to get it.
                                                                ->many ppl keep data local for easy access- so not updated frequently.
                                                                        leads to data inconsistencies.
                                                                Imagine 300 branches- each file searched 1 by 1 to find where John white works.
                                                                Set of records on employees and with branches- Imagine Londer branch information with employee.
                                                                        Employee centric data and Branch centric information. Multiple copies of same data in different order (bloated).
                                                Very difficult to modify
                                                        Where on disk is data located. vs now, how cares DBMS will manage that.
                                                        easy only if wanted to replicate file cabinet storage.
                                        Specialized skills required to obtain information
                                                Inefficient to speed of biz.						
                                                        Large staff required to maintain system.
                                                Less efficient system maintenance- Ppl reqd to look for and clean up mess.
                                                        Redundant data stored 
                                                        Inconsistent data- 
                                                        Prone to anomalies and bad data
                                                Difficult to archive data due to dependent parent child relp of DS.
                                                Very expensive
                                                        Experts controlled access to data.
                                                        Millions $ on mainframes- only fortune 500s could.
                                                                Inflation so 10s of Million $ now.							
                                        Difficult to account for complex relationships
                                                Paper based systems- business adapting themselves to way data is organized. vs now biz itself and data acc.
                                                Devp had to be able to predict all possible access patterns in advance and design the DB acc.
                    </Hierarchical>
                    <Network>
                            <What>
                                Introduced by IBM in 70s.
                                Mathematical- Graph like structure. Less structure cmp to hierarchical.
                                Eg- 1 user can have many books so user table or data structure- each user record has a link to LL which is pointers to book records.
                                Why? Designed to reduce prob of hierarchical.
                                        Allows more links betw the child databases. Searching became easier.
                                        Child can have more than 1 parent.
                                        Multiple connections for each entity.
                                        Not usually balanced. Not top-down by nature.
                                Keywords used for query for COBOL programs but not rich.
                                        No formal query language
                                        ptr- translated to block pointers.
                            </What>
                            <Why>
                                Advantages
                                        Still relatively easy to understand- conceptually same as b4.
                                        Able to adapt to business processes- capturing more complex relp.
                                Limitations- Same as hierarchical
                                        Slow to adapt or modify.
                                        Heavy reliance on experts
                                                Need to be DB expert to use it. So not for general public real life apps.
                                        Bloated data/anomalies.
                                        Expensive.
                                        No physical data independence (eth is hard coded). 
                                                Programmer has to think in terms of record,hardware.
                                                Physical data and logic are tightly bound. 
                            </Why>
                    </Network>
                </Traditional>
                <Relational>
                    <IITKGP>
                        so called- RDBMS
                            Relational- Relations or tables- related fields or attribute values
                            Database- Collection of data (of an information system)
                                Eg- Bank, library, Railways
                            Management System- Access, Process the data (query and update)
                        <Evolution>
                        Early days- application programs doing CRUD storing on files
                            Problems
                                Disorganized development- Data Isolation
                                Data Redundancy => Data Inconsistency

                                Concurrency
                                    Improper interleaving of application processes cause problems
                                    P1 checks seat is available- writes ticket number on seat or seat no on ticket
                                        B4 decrementing no of seats (in same or other file)
                                    P2 arrives- checks remaining seats and writes seat number on another ticket!!
                                    ??while 1 process uses a file- it locks the inode- will 2nd process get to access?
                                Security
                                Integrity Constraints
                        </Evolution>
                        <ACID>
                        RDBMS- 4 properties of transaction manager (ACID)
                            <Atomicity>
                            Atomicity- commit all or none- In a transaction, either commit all of the information pieces or none
								How - Commit and Rollback.
                                REL not that seat is decremented but seat number not allotted to ticket.
                                    Challenges- Power failure, errors, crashes, ?1 successful operation and not the other.
                                ?Success thru commit, rollback system- not written to DB until whole transaction is complete.
                                Eg- account transfer- either increment B and decrement A by 5000 or do nth at all.
                                <LogBasedRecovery>
                                Log Based Recovery
                                    On any DB change- write a tuple to logs-
                                        (T1,Starts)
                                        (T1,X,OLD value, New value)
                                        ...
                                        (T1,ENDS)
                                    At any stage power fails, we know what to restore and to what value.
                                        Eg- (T, starts)
                                            Transaction Starts....
                                            Seen:Writes
                                            Log:(T,A,1000,1500) //power fails after this.
                                            (T,B,2000,1500)
                                            Write into buffer for writing.
                                            Seen: Transaction Commit- Output Log records to stable storage.
                                            //Output block of data to disk
                                            //move all log record of this Transaction to stable store
                                            (T, commits)    //moved to stable storage
                                            Transaction Commits
                                            [checkpoint 1]  //move all log records to disk, move all blocks to disk, output a log record (checkpoint (no)) onto stable storage
                                        If Power fails for Deferred write- Writing happens in the last
                                            1. Redo- All the log values from a Checkpoint to last commit- are redo- write the values from log to disk.
                                                Idempotent- do them 100 times same results
                                            2. The transaction w/o (T,commit) can be ignored cuz not committed and hence not written in DB also.
                                                restart those transactions.
                                            3. Crash on recovery- then redo the process again.
                                        Immediate DB Modification
                                            Must for long run transactions
                                            Write is written to disk immediately.
                                            1. Redo all transactions from checkpoint to transactions with committed state
                                            2. Unto transactions which wrote sth to disk but didn't commit

                                        1. restore A to 1000 and mark transaction failed cuz we don't know anything after that.
                                        2. Redo (T) after failure
                                    Before output a data block B1- output all log records pertaining to B1 to disk.
                                </LogBasedRecovery>
                                <Checkpointing>
                                Checkpointing
                                    output all log records from MM to disk
                                    output all modified data blocks from MM to disk
                                    Output a log record Checkpoint onto stable storage.
                                    //so that we know where our system failed and carry on transactions from there.
                                </Checkpointing>
                            </Atomicity>
                            <Consistency>
                            Consistency- system from 1 valid state to another
                                <IntegrityConstraints>
                                Integrity Constraints- 
                                    <DomainConstraints>
                                        definition of domains of each attribute
                                            eg- acc_no int/varchar(10)
                                        conditions to hold for the values written for them
                                            Eg- NOT NULL
                                            gross_salary=deductions+salary_paid
                                        check anytime- using assert:
                                            assert yr_constraint on book : yr_pub > 1800 and yr_pub < 1996
                                            RIC- assert borrow_constraint on insertion to borrow : exists (select * from book where borrow.acc_no = book.acc_no) and ..
                                    </DomainConstraints>
                                    <EntityConstraints>
                                        Entity Constraints
                                            primary key (acc_no) in last of create table statement
                                    </EntityConstraints>
                                    <ReferentialConstraints>
                                    Referential Integrity Constraints (subset dependency)
                                        book(acc_no, yr_pub, title) and borrow(acc_no, card_no, DOI)
                                            Desirable- PI (FK) borrow = PI (PK) book
                                        case 1: borrow shows a book acc_no=9999 but no entry in book
                                            solution- 
                                                Insertion check in borrow- corresponding pk exists.
                                                    t1 inserted, t1[fk] = PI (PK) r1
                                                deletion check in book- no corresponding fk exists.
                                                    t2 deleted, SIGMA (t2[pk]=fk) r2 = PHI
                                                update = delete + insert
                                        case 2: vv is ok
                                        foreign key (acc_no) referencing (book)
                                    </ReferentialConstraints>
                                </IntegrityConstraints>
                                <Normalization>
                                Normalization- Solves redundancy problem!
                                    <FunctionalDependencies>
                                    Using Functional Dependencies- exposes redundancy in system
                                        A -> B where A and B are set of attributes
                                            FOR ALL t1, t2 BELONGS R (any relation)
                                            t1[A] = t2[A] => t1[B] = t2[B]
                                        Target to minimize redundancy- a table where only PK on lhs of FD exists
                                        Eg- 
                                        acc_no -> yr_pub means whenever acc_no is same, yr_pub is bound to be same. 
                                            Tell me acc_no, I can tell you yr_pub (checking table)
                                            say- acc_no determines yr_pub
                                        Eg- flights(fNo, arr, dep, planeType)
                                            seats_free(flightNo, date, seatsAvl)
                                            fNo -> arr
                                            fNo -> dep
                                            fNo-> planeType
                                            written as fNo -> arr, dep, planeType
                                            fNo, date -> seatsAvl
                                        Eg- studName -> address and name, subject -> grade
                                        Eg- acc_no -> yr_pub, title, cardNo, DOI, supp_name, price, DOS
                                            cardNo -> name, address
                                            supp_name -> supp_address
                                        Eg- S_name, DOS -> Price means supplier on a given day is bound to keep a fixed price (on eth)
                                            cardNo -> DOI means a user can issue books on 1 day only.
                                        if acc_no -> yr_pub then acc_no, cardNo -> yr_pub has cardNo (redundant)
                                    </FunctionalDependencies>
                                    <Closure>
                                    Closure of FD
                                        F+ : FD that follow from F
                                            transitive laws and tree structure
                                            Armstrong's axioms to compute
                                                Reflectivity- A SUBSEToF B => B -> A
                                                Augmentation- if A -> B then CA -> CB
                                                Transitivity- A -> B and B -> C => A -> C
                                                It's complete and sound- 
                                                    Sound- theorem output => reality output
                                                    Complete- reality output => theorem can do it.
                                                Union- A -> B and A -> C => A -> BC
                                                    Proof- A -> B then AA -> AB and A->C => AB-> CB => A -> CB or BC by toggle
                                                Decomposition- A -> BC => A->B and A->C
                                                    Proof- A -> BC and B -> B => AC => BCC => BC (BC => B)
                                                PseudoTransitivity- A-> B and CB -> D then CA -> D
                                            Closure of attribute set: for all combinations, find their closure.
                                                A+, C+, (AB)+, etc
                                                Algo(a){
                                                    result = a;
                                                    modified = true;
                                                    while(modified){
                                                        modified = false;
                                                        for all FD A -> B in F{
                                                            if(A IN RESULT){
                                                                Result = Result UNION B;
                                                                if Result changes
                                                                    Modified = true;
                                                            }
                                                        }
                                                    }
                                                }
                                            Cover-if F+ = G+ then F covers G.
                                                Minimal cover or canonical cover.
                                    </Closure>
                                    <DesirableEffects>
                                    Desirable Effects
                                        Motivation- Reduction of redundancy- reduce FD/ table.
                                        Necessary Condition- Lossless Join- additional or lost info on join.
                                            r = PI(R1)r JOIN PI (R2) r JOIN ....
                                            Eg- name -> addr; name, item -> price
                                                S1(name, address), S2(name,item,price)- join on name is no problem
                                                P1(name,item) P2(add,item,price)- join on items is lossy.
                                        Desired Condition- Dependency preservation- make new tables- in separate tables then no hope to maintain FD.
                                            F+ = F1 U F2 U ...
                                    </DesirableEffects>
                                    <viz>
                                    Normal Forms
                                        NF1- all atttributes are simple- atomic, not multivalued (skills), not composite (address comprises of city,street,etc)
                                            Composite- break them down into columns or separate tables with fk here.
                                            multivalued- make them go into separate rows.
                                        NF2- PK -> every non-prime (!candidate key part) attrib (other cols)
                                        NF3- no NPK -> NPK (1 col -> other col)- make a new table and preserve FK.
                                    </viz>
                                    <ValidState>
                                    valid state- a state in which data information doesn't contradiction with obtained from another portion.
                                        Constraints continue to hold.
                                        Using triggers- relevant fields change along.
                                        different from logical correctness of transaction.
                                    Eg- Remaining seats is 19 but on Database 20 seats remain for the date and train.
                                        In 1 table the monthly salary is different annual salary/12 in another place.
                                    Solution- Normalization
                                    </ValidState>
                                </Normalization>
                            </Consistency>
                            <Isolation>
                            Isolation- no messing with other- may cause inconsistency or undesirable outcomes (2 ppl on 1 seat)
                                Solves concurrency problem!
                                1 transaction doesn't mess with another!
                                Concurrent execution is as good as serial execution (serializable)
                                Input (x) - transfer block where X resides to Main Store
                                    read(xi,X); write (X,xi)
                                Output (x)- write the block back to Disk.
                                    happens depending on buffer management techniques
                                <AtomicJob>
                                    Transaction- reads reqd data items once and writes (if updated) once.
                                        States: Active, Partially Committed, Committed, Failed, Aborted (Do- kill,restart)
                                        T: Read(X,xi)
                                            xi += 15
                                            Write(X,xi)
                                            Read(X,xi)
                                            xi -= 15
                                            Write(X,xi)
                                        When transactions execute- there may be some interleaving of processes that may mess up.
                                    Eg- imagine x=500 and y=1000 (DB values in disk)
                                            T1      T2
                                        read(x)
                                        x -= 100
                                        write(x)                    //x=400, y=1000
                                                    read(x)
                                                    t=x*0.2
                                                    x -= t
                                                    write(x)        //x=320, y=1000
                                        read(y)
                                        y += 100
                                        write(y)                    //x=320, y=1100
                                                    read(y)
                                                    y += t
                                                    write(y)        //x=320, y=1180
                                        This is concurrent serialization schedule cuz 
                                            x+y=1500
                                            net outcome is same as serial output.
                                    Vs Eg- imagine x=500 and y=1000 (DB values in disk)
                                            T1      T2
                                        read(x)
                                        x -= 100
                                                    read(x)
                                                    t=x*0.2
                                                    x -= t
                                                    write(x)        //x=400, y=1000
                                        write(x)                    //race condition //x=400, y=1000
                                        read(y)
                                                    read(y)
                                                    y += t
                                                    write(y)        //x=400, y=1100
                                        y += 100
                                        write(y)                    //x=400, y=1100
                                        This is not concurrent serialization schedule cuz 
                                            even if x+y=1500 (it would not have been if values were different)
                                            net outcome is not same as any serial output. T1 then T2 or T2 then T1
                                                320, 1180 or 300, 1200 were valid outcomes of this.
                                </AtomicJob>
                                <Serializable>
                                Abstract out eth but read and write and check where it fails
                                    <ConflictSerializability>
                                    Conflict Serializable Schedule
                                        T1 then T2 interleaving, swapping to make serializable
                                        read(x) and read(x)    //allowed
                                        read(x) and write(x)   //not allowed
                                        write(x) and read(x)   //not allowed
                                        write(x) and write(x)  //lost update
                                        read/write(x) and read/write(y) //allowed
                                        Eg-
                                            T1: Read(x), Write(x)
                                            T2: Read(P), Write(x)
                                            Is Equivalent to
                                            T2: Read(P)
                                            T1: Read(x), Write(x)
                                            T2: Write(x)
                                        Not Conflict Serializable-
                                            T1      T2
                                        Read(x)
                                        Write(x)
                                                    Read(y)
                                                    Write(y)
                                        Read(y)
                                        Write(y)
                                                    Read(x)
                                                    Write(x)
                                        Analysis- Read(y) of T2 can't go below read(y) of T1 so T2 can't succeed T1.
                                            Try for T2 preceding T1- Read(x) of T2 can't come b4 T1, so can't preceed.
                                            There is a cycle- T1 <- ->T2 (so this Schedule of T1 and T2 not conflict serializable)
                                        ?Thomas Write Rule
                                            Read(Q) 
                                                    Write(Q)    //blind write
                                            Write(Q)
                                            IS Equvalent To
                                                    Write(Q)
                                            Read(Q)
                                            Write(Q)
                                            IS Equivalent To
                                            Read(Q)
                                            Write(Q)
                                                    Write(Q)
                                    </ConflictSerializability>
                                    <ViewSerializable>
                                        S1 and S2 are view equivalent iff
                                            For All Input var, Ti reads initial value be it S1 or S2
                                            For All Q, If Ti read(Q) after write(Q) of Tj in S <=>alo in S'
                                            For All Q, Ti performs final write of Q in either S1 or S2
                                        Eg-
                                            T1      T2      T3
                                        Read(x)
                                                  Write(x)
                                        Write(x)
                                                            Write(x)
                                        Is it equivalent to?
                                            T1      T2      T3
                                        Read(x)
                                        Write(x)
                                                  Write(x)
                                                            Write(x)
                                        Analysis- for x, first read by T1, no consumption and last write by T3- so yes, view serializable.
                                    </ViewSerializable>
                                    <Protocols>
                                    Protocols to Ensure Serializability
                                        Lock Based Protocols
                                            Onus is on users to write consistent concurrent Transactions.
                                            Shared Lock- Ti can read on shared lock but can't write
                                            Exclusive Lock- Both read and write allowed.
                                            Eg- 
                                                    T1              T2              T3
                                                Lock-X(P)           Lock-X(P)       Lock-S(P)
                                                Read(P)             Read(P)         Read(P)
                                                P +=50              P -= 20         Display(P)
                                                Write(P)            Write(P)        Unlock(P)
                                                Unlock(P)           Unlock(P)
                                            Lock Compatibility
                                                S   X
                                              S T   F
                                              X T   F
                                            2-phase locking
                                                Growing Phase- Transaction acquires locks but not release them
                                                Shrinking Phase- Transaction Releases locks but not acquire.
                                                Eg- Lock(P)
                                                    Lock(Q)
                                                    Lock(R)
                                                    ...
                                                    Unlock(P)
                                                    Unlock(Q)
                                                    Unlock(R)
                                                Not Deadlock Safe
                                                    Eg-
                                                        Lock-X(P)
                                                        Read(P)
                                                        P -= 100
                                                                        Lock-S(Q)
                                                                        Read(Q)
                                                                        Lock-S(P)
                                                        Write(P)
                                                        Lock-X(Q)
                                                        ...             ....
                                                        Prob is still with growing shrinking
                                                            Lock-S(P)
                                                            Upgrade
                                                                        Lock-S(Q)
                                                                        ..............//think about it- after this stage- deadlock is bound to happen- if requirements are as follows
                                                                        Lock-S(P)
                                                            Lock-S(Q)
                                                            Upgrade
                                                    Solution- Sequence should have been fixed (P then Q then R, etc)
                                                        Tree Protocol used
                                                            A Transaction can lock data item (P or Q) at most once.
                                                            Ti- 1st lock on any data item
                                                            IMPORTANT- Item A can be locked only if parent of A is locked by Ti
                                                            Unlock Data items any time
                                                            Subsequent relocking not allowed
                                                            ?How it works!!

                                            Lock Upgradation/ Downgradation
                                                Lock-S(Q)       //just read
                                                Lock-X(Q)       //read and write
                                                Unlock(Q)       //done with it
                                                Upgrade(Q)      //also add writing (used just b4 writing)
                                                Downgrade(Q)    //reduce write to just read
                                                ?Deadlock Safe- T1 lock-S(P), then upgrade(P), T2: lock-S(Q), lock-S(P) (waiting);
                                                        T1: Lock-S(Q) allowed, upgrade(Q)- not allowed- waiting!!
                                                Eg- 
                                                    Lock-S(P)               Lock-S(P)
                                                    Read(P)                 Lock-S(Q)
                                                    P -= 100                Read(Q)
                                                    Upgrade(P)              Read(P)
                                                    Write(P)                Display(P+Q)
                                                    Lock-S(Q)               Unlock(P)
                                                    Read(Q)                 Unlock(Q)
                                                    Q += 100
                                                    Upgrade(Q)
                                                    Write(Q)
                                                    Unlock(P)
                                                    Unlock(Q)
                                        Time Stamping Protocols
                                            System itself tries to 
                                                detect Possible Inconsistency during execution
                                                Recover from it/ avoid it.
                                            W-Timestamp(Q)- Largest TS value of any transaction that executes write(Q)
                                            R-Timestamp(Q)- Largest TS value of any transaction that successfully executed Read(Q)
                                                TS(Ti) < TS(Tj)
                                            Idea- Smaller Ti (TS) should finish early.
                                            Ti issues a read(Q)
                                                TS(Ti) < W-Timestamp(Q) : Some bigger Tj has already written
                                                    Reject Read(Q)
                                                    Rollback Ti
                                                    Restart
                                                TS(Ti) > W-Timestamp(Q) : Smaller TS has written, no prob- it had to finish b4 us.
                                                    Read is allowed
                                                TS(Ti) < R-Timestamp    : Bigger TS has read Q already, we have prob with write not read.
                                                    Read is allowed
                                                TS(Ti) > R-Timestamp(Q) : Smaller TS has read, no worries, even it had written was not a problem
                                                    Read is allowed
                                            Ti issues a write(Q)
                                                TS(Ti) < R-Timestamp(Q) : Bigger TS has read Q, it will not be serializable. It had to read after our write not before our read.
                                                    Reject Write
                                                    Ti rolled back.
                                                    //i dont agree!
                                                TS(Ti) < W-Timestamp(Q) : Bigger process has written Q before us (again violates our condition)
                                                    Reject Write
                                                    Ti rolled back
                                                    Thomas [Blind] Write Rule- 
                                                        Ignore Write(Q) by Ti and let Ti continue
                                                        Why? write of W-Timestamp was allowed then cuz 
                                                        //I agree only on blind write
                                                TS(Ti) > R-Timestamp(Q) : smaller process has read b4- good!
                                                TS(Ti) > W-Timestamp(Q) : smaller process has written b4- good!
                                        Multiversion Scheme- 
                                            overcomes  ...... case of TS protocol
                                            Each data item has a sequence of data item versions for each transaction
                                                Content- value of version Qk
                                                W-Timestamp(Qk)- TS(Ti) which created Qk
                                                R-Timestamp(Qk)- Largest TS that consumed Qk.
                                            Ti issues Read(Q)- pick the largest TS in W-Timestamp(Q) less than TS(Ti)
                                                so unconditional read is made possible!!
                                            Ti issues a write- 
                                                TS(Ti) < R-Timestamp- Ti rolls back.    (bigger transaction has read b4)

                                        Multiple Granularity
                                            DB - Relation - Rows - Values
                                            Shared, Exclusive, Intent Share (1 of the children in read), Intent Exclusive (IX- 1 of children in write), Shared and Intent Exclusive (1 of children in read and 1 in write)
                                            when shared, all ancestors get IS if nothing
                                            when exclusive, all ancestors get IX if nothing
                                            if already IS, then IX makes it ?IX.
                                            If already S, then IX makes it SIX
                                            so forth.
                                            Lock Compatibility- while in 1 mode and getting a request for other, what to do?
                                                IS  IX  S  SIX  X
                                        IS      T   T   T   T   F
                                        IX      T   T   F   F   F
                                        S       T   F   T   F   F
                                        SIX     T   F   F   F   F
                                        X       F   F   F   F   F
                                            Locking Rules,
                                                Ti may lock Q in 
                                                    S, IS if Parent is in IX, IS
                                                    X, SIX, IX if Parent is in IX, SIX.
                                                    ?anything if parent not locked
                                                    can unlock Q if no child of Q remains locked bt Ti.
                                    </Protocols>
                                </Serializable>
                            </Isolation>                    
                            <Durability>                
                            Durability- data once committed stays with the system.
                                Recovery from failure
                                    Failure types- Logical, System Errors, System crash, Disk Crash. 
                                    Disk Crash- Protection against
                                        Output all log records from MM to stable storage
                                        Output all buffer blocks onto disk
                                        Copy contents of Database to stable store
                                        Output a log record (dump) to stable storage.
                                    Shadow Paging- Maintain a shadow page table in disk before it goes to MM.
                                written to non-volatile storage.
                                Maintaining multiple copies- replica.
                            </Durability>
                        </ACID>
                        <ModelDesign>
                        Model Design- as made by DB architect.
                            <ApplicationPrograms>
                                <GUI>
                                    aka front end.
                                    GUI- Naive users- Menu driven (back end support)
                                </GUI>
                                <SQL>
                                SQL- Conceptual Level View
                                    <DDL> Schema changes.
                                    DBA -> DB Schema (creation)- As made by DB Designer
                                        No Rollback
										<Create>
                                        create table (tableName)(
                                            colName datatype [condition]*
                                            [,colName datatype [condition]*]*
                                        )
										</Create>
										<Update>...</Update>
										<DROP>
                                        drop table (tableName)
                                            deletes schema also
                                            removes table from DB.
                                            rows, indexes and privileges will also be removed
										</DROP>
										<Truncate>
                                        truncate table (tableName)  
                                            deletes data not structure
                                            doesn't occupy the undo space
										</Truncate>
										vs Delete
                                        (DML) delete book     //removes all rows on condition- gotta commit/rollback
                                    </DDL>
                                    <DML> Data Queries or changes
                                        Queries and Intermediate computation may be performed on Main Memory
										<SQLExecutionSequence>
											Cross product of tables.
											Filter rows from there
											aggregate on fields
											filter on those aggregates
											order the results
											display the results
										</SQLExecutionSequence>
										<Retrieve>
                                        Retrieval
											Eg- select acc_no from book where title="CD"
											{t|t.acc_no and t.title = "CD"}
                                            select A1,A2,A3 //6- present in seq
                                            from r1,r2,r3   //1- cross product
                                            where P         //2- filter rows
                                            group by G      //3- aggregate on field[s] (superset of select)
                                            having H        //4- filter on aggregates
                                            order by O1 asc, O2 desc      //5- sequence them; 2nd level ordering on equality of 1st
											<MINUS> use NOT IN.
                                            Eg- Compiler Design books which are not yet borrowed.
												{t|t.acc_no and t IN book and NOT(EXISTS(b)|b.acc_no = t.acc_no)
												select acc_no from book where title="CD" MINUS select acc_no from borrow
                                                select acc_no from book where title='cd' and acc_no not in (select acc_no from borrow)
											</MINUS>
											<INTERSECTION> use IN.
											Eg - accNo of CD books which are borrowed
												{t|t.acc_no and t IN book and (EXISTS(b)|b.acc_no = t.acc_no)
                                            Eg- select acc_no from book where title="CD" INTERSECTION select acc_no from borrow
                                                select acc_no from book where title='cd' and acc_no IN (select acc_no from borrow)
											</INTERSECTION>
                                            <BELONGS> ?use exists
                                                construct is similar to BELONGS to a set.
                                                works similar to nested iteration
											</BELONGS>
											<NestedSelfChecks> The equality expression containing 2 table references is the criteria of 2 loops.
											Eg - select users that share the address with Vijay.
												{t IN users | EXISTS(t2 IN users) and t2.name = Vijay and t2.address = t.address}
                                            Self Join- select U.name from user U, user V where V.name=Vijay and U.address = V.address
                                                2 tables multiply with rows on their side.
                                                vs IN construct- similar functionality- but faster
											</NestedSelfChecks>
                                            <Rank>
												<AllBut> OR Some
												select S.name ,accNo from supp where price > some (select price from supp where name=narosa)
                                                    //could well be select min(price)
                                                    //if all (select ...) = select max(price)...
												selects all but smallest price from supplier Narosa
													{t IN Supp | (EXISTS s(Supp)) and t.price > s.price and s.name = Narosa}
                                                	select T.name, T.acc_no from supp S,supp T where T.price > S.price and S.name="narosa"
													OR {t IN Supp | t.price > price_var and (EXISTS s(Supp)) and price_var = min(s.price) and s.name = Narosa}
														//indication to use price with where clause cuz that's the pivot
												all but costliest book passes thru
                                                    select S.price from supp S, supp T where (S.price < T.price and S.name=narosa and T.name=narosa)
												</AllBut>
												<Best>
													costliest book of narosa
														All keyword - select price from supp where name=narosa and price >= all (select price from supp where name=narosa)
															{t|t IN supp and t.name = narosa and FOR_ALL(t2 IN Supp) AND t.price > t2.price}
														Best as holds same value as max - Use max inner query.
															{t|t IN supp and t.name = narosa and t.price > t2|FOR_ALL(t2 IN Supp and max(t2.price))}
															Thinking pattern > and FOR_ALL means MAX.
																lt; and FOR_ALL means MIN.
																FOR_ALL always have a combo with aggregation functions - when talking about a group - you can think of some set functions.
															select price from supp where name=narosa and price >= (select max(price) from supp where name=narosa)
														Best as Negation of exists
															{t|t IN supp and NOT(EXISTS(s IN supp)s.price > t.price)}
															select price from supp t where not exists (select * from supp s where s.price > t.price)
													Get costliest - Get All but and then MINUS from total set.
														Minus from select price from supp where name=narosa to get 1st.
													Use order by and limit.
														{t2|(FOR_ALL t2 IN SUPP Order(t2.price) and limit(3))}
												</Best>
                                                <Top_N_Query> Ref LMSPost in IIITB.
													Idea - count number of people greater or less than current while making new table.
                                                    Suppose Data in table Top_N is as follows:
                                                        5
                                                        5
                                                        6
                                                    Scheme-1
                                                        Rank of 5=0; Rank of 6=2
														{t|EXISTS(a in Top_N) and t.val = a.val and t.rank = (FOR_ALL(b in TOP_N)(count(b.val < a.val)))}
                                                        Query
                                                            SELECT A.val, (SELECT COUNT(B.val) FROM Top_N B WHERE B.val < A.val) Rank
                                                            FROM Top_N A
                                                            GROUP BY A.val ORDER BY A.val;
                                                    Scheme-2
                                                        Viz Rank of 5 = 0; Rank of 6 = 1
                                                        Query
                                                            SELECT A.val, (SELECT COUNT(DISTINCT B.val) FROM Top_N B WHERE B.val < A.val) Rank
                                                            FROM Top_N A
                                                            GROUP BY A.val ORDER BY A.val;
                                                    //To convert it into a Top_N query we need to add a having clause. 
                                                        SELECT A.val, (SELECT COUNT(DISTINCT B.val) FROM Top_N B WHERE B.val < A.val) Rank
                                                        FROM Top_N A
                                                        GROUP BY A.val
                                                        HAVING RANK=2; 
                                                </Top_N_Query>
											</Rank>
											<Division> OR Subset Of.
                                            Division (subset of) - Opposite of cartesian product.
												Implementation - 2 sub-queries using CONTAINS.
												Eg - Which supplier supply all the books which Vijay issued (they could be supplying more)
													iow, Suppliers for which book titles are superset of vijay issued titles.
													iow, suppliers for which book titles are subset of vijay issued titles
												Alternative 1 - CONTAINS syntax (but not supported)
													{t|EXISTS(s in Supp) and t.name = s.name and (titles given by s) SUPERSET_OF (titles issued by Vijay)}
													{t|EXISTS(s in Supp)(b in Book)(r in Borrow) and t.name = s.name and FOR_ALL(b.title|b.acc_no = s.acc_no) CONTAINS b2.title|(EXISTS b2,r2,u2)(b2.title|b2.acc=r2.acc and r2.card = u2.card and u2.name = Vijay)}
													select S.name from supp S where (
														(select title from book where book.acc_no=S.acc_no)             //set of titles (by every supplier (1 by 1))
														CONTAINS                                                        //superset of
														(select title from book, user, borrow where book.acc_no=borrow.acc_no and user.cardNo=borrow.cardNo and user.name="vijay")  //titles issued by vijay
													)
												Alternative - 2 NOT EXISTS
													Formula -
														swap CONTAINS parts
														replace ALL and contains both with NOT EXISTS
														add some parts aptly.
													iow, name a supplier s.t there exists no title borrowed by Vijay which that supplier does not provide.
													Break it down to understand
														Get all titles borrowed by Vijay
														among these there exists no title OR check for every title
														which a supplier s does not provide - name that supplier s.
													iow,for a given supplier s, we could not find any title borrowed by Vijay which were not supplied by the s.
													Personal tryst -
													select S.name from supp S where
														NOT EXISTS(
														(select title from book, user, borrow where book.acc_no=borrow.acc_no and user.cardNo=borrow.cardNo and user.name="vijay" and where
															NOT EXISTS(
																(select title from book2 where book2.acc_no=S.acc_no and book2.title = book.title)
															)
														)
													)
													3rd alternative - count of books issued by Vijay > count of books given by every supplier (group by)
													Eg - Movie WW2 - There aint no planes in this hangar that I cant fly!
														hangar means list of planes and pilotskills means which pilot can fly which planes.
													SELECT DISTINCT pilot_name
														FROM PilotSkills AS PS1
														WHERE NOT EXISTS
														    (SELECT *
															    FROM Hangar
																WHERE NOT EXISTS
																    (SELECT *
																	    FROM PilotSkills AS PS2
																	    WHERE (PS1.pilot_name = PS2.pilot_name)
																	    AND (PS2.plane_name = Hangar.plane_name)));
											</Division>
											<Exists>
                                            Exists
                                                ?SIMILAR TO IN (WRONG)
                                                for a row's given value- there exists [1 or more ]rows satisfying a condition.
                                                user who has issued at least 1 of narosa's book
                                                    select name from user,... where user.cardNo=borrow.cardNo and borrow.acc_no=book.acc_no and book.acc_no=supp.acc_no and supp.name=Narosa
                                                    number changes then use aggregate count(*) group by.
                                                    select name from user U
                                                        where exists(
                                                            select * from supp,.. where U.cardNo=borrow.cardNo and .....narosa.
                                                        )
                                                    Eg- select name from user U
                                                        where exists(
                                                            select * from supp S, borrow B
                                                                where U.cardNo = B.cardNo and B.acc_no = S.acc_no and S.name=Narosa
                                                        )
                                                        Or select name from user, supp, borrow where...
                                                Eg- for the titles issued by Vijay, are there suppliers (supplying some) other than Narosa, if yes, name them
                                                    Contains
											</Exists>
											<NotExists>
                                            Not Exists
                                                select name from user where not exists (select * from supp S, borrow B where user.cardNo=B.cardNo and B.acc_no=S.acc_no and S.name="allied")
                                                //P(t) is never true for this name- for all rows this doesn't hold
                                                    !P(t) is true.          for no rows, reverse holds.
                                                Eg- borrower issued narosa but none allied
                                                    select name from user T
                                                    where NOT EXISTS
                                                    (
                                                        select * from supp,borrow
                                                        where T.cardNo=borrow.cardNo and borrow.acc_no=supp.acc_no
                                                        and supp.name=allied
                                                    )
											</NotExists>
											<Aggregate>
                                            Aggregate- Avg, max, min, count, sum. 
                                                Select name, avg(price) from supp group by name
                                                select title from book group by title having count(acc_no) > 50
                                                select S1.name from student S1, student S2
                                                    where S1.marks < S2.marks 
                                                    group by S1.name having count(s1.name)=3
                                                supplier with highest max price
                                                    select name from supplier
                                                        group by name having max(price) >= ALL
                                                        (
                                                            select max(price) from supp group by name
                                                        ) 
                                                    select greatest average priced supplier
                                                        select supp_name, avg(price) as cost from supp, book
                                                            where supp.acc_no=book.acc_no
                                                            group by supp_name      AS P
                                                        select max(cost),supp_name from P
											</Aggregate>
										</Retrieve>
                                        <Delete>
                                            delete borrow where cardNo in (select cardNo from user where name=vijay)
                                            delete (table) where rowCondition
										</Delete>
                                        <Insert>
                                            insert into (table) [(col[,col]*)] values(val[,val]*)
                                            insert into book values (23,1999,"DBMS");
										</Insert>
										<Update>
                                            update (table) set (col)=(val) where rowCondition
                                            Eg- update supp set price = price * 0.95 where name=allied and price > 1000
										</Update>
                                    </DML>
									<DCL></DCL>
                                    <PLSQL>  
                                    Application Programs- PL/SQL- procedures, functions, triggers.                                    
                                        create view v as (query expression)- evaluated everytime v is used.
                                        eg- select view books_1919 as (select title, accNo from book where yr_pub=1919)
                                        Updating views- views have no real existence- just a shorthand.
                                            so any update which messes with table conditions are not allowed.
                                            Eg- insert into v values ("narhari",12943); //not allowed
                                            but if request is valid to table then allowed.
                                    </PLSQL>
                                    <API></API>
                                </SQL>
                            </ApplicationPrograms>
                            <DBMS>
                                <DDL>
                                DDL- Data Definition Language
                                    Create- Schema is input to DDL compiler- forms data dictionary (DB Schema)
                                    <ways>
                                        <TopDown>
                                            aka Sematic DM; aka ER; aka UML diagram.
                                        ER- Top-Down Approach (conceptual Modeling or semantic data model)
                                            Entities- Identify entities (person, thing, brand, concept) in you system- objects of interest
                                                Entity Type- Abstract into a class- class for all objects!!
                                                Entity Set- Collection of entities of same type. Eg- {Mohan, Raju}
                                                Attributes- Distinguishable feature/ property of entity.
                                                    Identify their attributes (of interest) to system (remember)
                                                    Mapping from entity set to a domain of values.
                                                Make a relation(tables) for all entities.
                                                    make every tuple, every entity (rows) dinguishable thru PK (primary key- unique for a reln) or better use SK (surrogate keys- no app importance)
                                            Relationships- An association/ Mapping (relation) between entities.
                                                Identify the relationships of interest (interaction details worth recording)
                                                    If R is A X B X C
                                                    R SUBSEToF domain(A) X domain(B) X domain(C)
                                                    Association
                                                        abstract into relationship type.
                                                        Which all entities interact in the relationship- without which a relation is incomplete
                                                            n-ary- ary-ity of the relation
                                                                Ternary Relation- A Customer Opens an account in a branch.
                                                                    can think of a binary relp betwn Customer and Branch!!
                                                                5-ary- A Staff teaches a subject to students in a room at specified period.
                                                                Unary- employee reports to manager (who is an employee)
                                                            Eg- Mohan saw "Little Krishna" movie in Silma theater.
                                                        Identify attributes of Relationship Type (of interest)
                                                            Eg- at what time, etc- attributes that hold only on interaction.
                                                        Identify Cardinary of relation- for the relation
                                                            1 entity relates to how many entities of other and vv.
                                                            m entities relate to- ludicrous way of asking.
                                                            Cases
                                                                1:1- 
                                                                1:n- add an Foreign key (FK) to the n side of relp.
                                                                    Eg- 1 User can borrow many books but 1 book is with 1 user
                                                                        But book is with many user at different time- we are recording an event- for a given time, above holds!
                                                                m:n- Make a new table s.t it has 1:n relationships with the 2 entities.
                                                    Aggregation- there is no name of a relationship except "has-A"
                                                        Eg- Employee has a Job Role.
                                                        Similar to association- Add an FK to the other side.
                                                    Composition- Existence dependencies. Best suited name of relp is "contains"
                                                        Strong/weak entity- weak is existent dependent on strong.
                                                        Eg- Account is existence dependent on customer. w/o customer account can't exist.
                                                        Eg- All RAMs made in a company must go into a computer.
                                                        Add to the FK, NOT NULL constraint to the weak entity. 
                                                        Attribs of relp can move to weak relation.
                                                            Eg- CustId in Account can't be null. Day of account opening can be in account table.
                                                            Eg- RAMId of a PC can't be NULL.
                                                    Inheritance- Best suited name of relp is "Is-A"
                                                        Non-Partitioned
                                                            add type field to discern entries in parent class.
                                                            children contain special fields only- add an FK to parent's PK in children.
                                                        Partitioned
                                                            ?No parent table!
                                                Eg- Supplier supplies book to Library. User borrows book on a certain date from 1 of the many libraries.
                                        </TopDown>
                                        <BottomUp>
                                            aka Normalization.
                                        Normalization- Bottom-Up Approach
                                            Solves prob of Consistency.
                                            Superkey- a set of one or more attributes which (taken taken) allow us to uniquely identify an entity in an entity set.
                                                Eg- filtering criteria (on properties or attribs) that (we are sure) returns unique individual or entity.
                                                Eg- all fields of customer table
                                            Candidate key- A superkey for which no proper subset is superkey.
                                                SUBSEToF
                                            Primary key- CK chosen by DB Designer (can be a natural attribute or surrogate- never sees light of day)
                                        </BottomUp>
                                    </ways>
                                </DDL>
                                <DML>
                                DML- Data Manipulation Language
                                    Tasks
                                        Retrieve
                                        Update/ Insert
                                        Delete
                                    App program -> DML Compiler [<-> Query Processor] -> App Program Object Code -> DB Manager (<->Data Dictionary) -> File Manager -> Data [files]
                                    Query -> Query Processor -> Database manager -> File Manager -> Data [file]
                                </DML>
                                <QueryFoundation>
                                    <RelationalAlgebra>
                                        Defined in terms of operations on relations
                                            Primary
                                                Rename (rho)- (aliasing thru "As")
                                                    Eg- all users who have same address as "Vijay"
                                                Cartesian Product (X)- CROSS PRODUCT (comma)
                                                    take 1 row[ value] and compare that with all rows
                                                SELECT (sigma)- Subset of a relation (where in sql)
                                                    linear traversal thru table
                                                Project (pi)- display to user (Select in sql)
                                                UNION (U)- merging results of 2 relations
                                                    SQL- implemented as OR and UNION
                                                SET Difference/ Minus (-)- tuples in R1 not in R2.
                                                    Eg- All books that are not issued (still on shelf)
                                                    SQL- use "NOT IN"
                                                        Eg- select acc_no from book where title="CD" MINUS select acc_no from borrow
                                                            select acc_no from book where title='cd' and acc_no not in (select acc_no from borrow)
                                            Secondary (derivable)
                                                Natural Join- Cross Product on criteria (select/filter)(comma and pk=fk)
                                                    If R INTERSECTION S = PHI, then r JOIN s = r X s
                                                    Eg- all books issued by "Chaitanya"
                                                    Eg- supplier names of books issued by "Vijay"
                                                    Commutative, associative, etc
                                                    Types
                                                        Self join
                                                        Inner join
                                                        Outer join
                                                Intersection- = r-(r-s) //take out unique rows of r (i.e in r but not in s)
                                                    //then take these rows which are unique to r from r- we get the rows that are common.
                                                    SQL- implemented thru "IN" keyword
                                                        Eg- select acc_no from book where title="CD" INTERSECTION select acc_no from borrow
                                                            //accNo of CD books which are borrowed
                                                            select acc_no from book where title='cd' and acc_no IN (select acc_no from borrow)
                                                Assignment (alias- "as")- using resultant relation in query.
                                                Division(/) or quotient operation- (S SUBSEToF R)
                                                    r / s = rows/entities of r which has relp to each entity of s.
                                                    Eg- PI(accNo,SupplierName)supp / PI (accNo) (SIGMA (cardNo = F53)borrow )
                                                        //all suppliers who could have supplied books that Vijay holds.
                                                        PI (R-S) r - PI(R-S) ((PI (R-S) r X s)-r)
                                                        SQL- ?not exists
                                                THETA-JOIN 
                                            Eg- PI( TITLE (SIGMA( YR_PUB > 1992 (book))))
                                            Eg- a = {1,2,3}, b = {A,B} //write them as tables
                                                a X b = {{1,A},{1,B},{1,C},{2,A},{2,B},{2,C}}   //written as table
                                                    tells what all relp could have been.
                                                    critera of pk = fk => relp that are.
                                    </RelationalAlgebra>            
                                    <RelationalCalculus>
                                        Non-procedural. Uses the concepts of formal logic to express relational queries.
                                        <TupleRelationalCalculus>
                                            s = {t | P(t) is true}  //t is tuple variable, s is a reln; P(t) is a predicate.
                                                selects all tuples t s.t P(t) is true for each of them.
                                            Eg- s = {t|t BELONGS book AND t[yr_pub]=1991}   
                                                //tuples in book table that have yr_pub field = 1991
                                            Eg- s = {t| EXISTS u BELONGS book (u[yr_pub]=1991 AND t[acc_no]=u[acc_no] AND t[title]=u[title])}
                                                //tuples with acc_no and title from book table which has yr_pub=1991
                                            Eg- name and address of all borrowers who issued a book on "14/08/95"    
                                                {t|t[name]=user[name] AND t[address]=user[address] AND user[cardNo]=borrow[cardNo] AND borrow[issueDate]="14/08/95" AND user BELONGS User AND borrow BELONGS Borrow}
                                            Eg- borrowers who have issued a book supplied by Narosa or Allied
                                                {t|t[name]=user[name] AND user[cardNo]=borrow[cardNo] AND borrow[acc_no]=supp[acc_no] AND (supp[name]="Narosa" OR supp[name]="Allied") AND supp BELONGS Supplier AND borrow,user BELONGS..}
                                            ??Eg- Borrowers who have issued a book supplied by "Narosa" but none by "Allied"
                                                {t|t[name]=user[name] AND user[cardNo]=borrow[cardNo] AND borrow[acc_no]=supp[acc_no] AND supp[name]="Narosa" AND FOR ALL supp, (borrow[acc_no]=supp[accNo] => supp[name]!="Allied")...}
                                            Eg- All suppliers with same address as Narosa
                                                {t|t[name]=supp1[name] AND supp1[name]=Narosa AND supp1[address]=supp2[address] AND...}
                                            Eg- Suppliers who supplied some title which Vijay issued
                                                {t|t[name]=supp[name] AND supp[accNo]=book1[accNo] AND book1[tittle]=book2[title] AND book2[accNo]=borrow[accNo] AND borrow[cardNo]=user[cardNo] AND user[name]=Vijay AND ..}
                                            Eg- {t|FOR ALL book BELONGS Book (t[yr_pub]=book[yr_pub] AND t[title]=book[title])}
                                                prints the yr_pub and title is there's just 1.
                                            Vs
                                            Eg- {t|FOR ALL book BELONGS Book (t[yr_pub]=book[yr_pub] => t[title]=book[title])}
                                                book title [for which all vals in book] with same yr_pub should have same title also
                                                in other words- books which have unique title for a given year
                                            Eg- {t|FOR ALL book BELONGS Book (t[title]=book[title] => book[yr_pub]=1991)}
                                                book titles which were all written in a unique year.
                                            ??Eg- Suppliers who supply some/ all titles which Vijay has issued
                                                some-
                                                all- {t|u[name]=supp[name] AND supp[accNo]=book1[accNo] AND u[title]=book1[title]      AND book[accNo]=borrow[accNo] AND borrow[cardNo]=user[cardNo] AND user[name]=Vijay 
                                                      AND book[title]=book1[title] =>   AND ...}
                                            Safety of Expressions- values obtained as a result should be from the domain.
                                                Eg- {t|!P(t)}
                                        </TupleRelationalCalculus>
                                        <DomainRelationalCalculus></DomainRelationalCalculus>
                                    </RelationalCalculus>
                                </QueryFoundation>
                            </DBMS>
                            <FileAccessingSoftware>
                                File Accessing Software (OS)- Physical Level View
                            </FileAccessingSoftware>
                            <DatabaseFiles>
                            Database files- Physical Level View (database, Data Dictionary)- Reside in Non-volatile storage
                                Disk organization- Track, blocks, records, files.
                                    Blocks and records
                                        Fixed length blocking for fixed length records
                                        Variable Length unspanned records
                                        ..................spanned records
                                        Buffer Management
                                            Locality of reference
                                            Block Replacement policy
                                            Pinned & unpinned records
                                    File and Records
                                        1 file, 1 reln (1:n, N:1)
                                        File Types
                                            Heap
                                                Insert- throw inside (stacking)
                                                Deletin- setting a flag to 0/1 (not dragging)
                                                Search- brute force
                                            Sequential files
                                                sequenced on PK
                                                Insert- if space then enter, else in spare block and merge EOD
                                                Delete- 
                                                Search/Find- Block search- get a block- do binary search in the block
                                                Can use pointers (relative!!) for availabilty and filled
                                            Indexed Sequential- maintain an index to fetch apt block
                                                Multilevel Indexing
                                                Spare/Dense Indexing
                                                Primary Secondary
                                            B+ Trees- O(lg n)
                                Seek Time- Time Reqd to place the head on the track
                                Latency- Time reqd to read some units of data.
                                    Inter block gaps in every track.
                            </DatabaseFiles>
                        </ModelDesign>
                    </IITKGP>
                    <mysql>
                        mysql -u root -p - starts the mysql process with administrative rights. to kill the process as usual- ctrl+c.
                        show databases;
                        use database1;
                        show tables;
                        source \home\user\Desktop\test.sql;     //.sql extension is compulsory. read/write/execute priviliges as a user (run as admin)
                            OR don't use database1 and write this line within test.sql that also works.
                        mysql -u yourusername -p yourpassword yourdatabase < text_file- in 1 go. Also append '> outputFile'
                        mysql -u yourusername -p yourpassword yourdatabase- to create a db- b4 credentials
                        mysql -u yourusername -p yourpassword a_new_database_name < text_file
                        open the database- in sqlyog (GUI) or command- see FK, PK- dependency in UML and other rows to familiarize
                            sql workbench- allows this thru reverse engineering tool (menu bar -> database -> reverse engineer -> next -> schema -> next)
                        start querying in mysql client.
                        where
                            using case insensitive comparison between data thru upper(s1) = upper(s2)
                            select [from] from table;   //using keywords in queries.
                            sql yog- select text and f9 to execute or ; separate execute
                    </mysql>
                        Basically Miscrosoft Access/Sql Server.
                        Logical Structure of data- Efficient- Reduces redundant data. (s/w take care)
                        PK to unique identifier
                                Table is foundation of our database. 
                                Field identifies sth about an entity (record).
                                only 1 occurence of record entity. Why would I want this? Make edit once.
                                        She changes her name- go to particular entity and make that change- once and that's it- reflective of my entire DB.
                        Data is organized in form of 2-D tables. Eg- Oracle,DB2,mysql,etc.
                        Codd Rules & Normals Form
                                When introduced it became a buzz word and e1 started using it which Codd didn't like so gave 13 rules paper for approval.
                                Rule 0- Able to manage DB entirely thru relational capabilities (no facade over hierarchical).
                                Rule 1- Logical Elements based on relations- no term like record type.
                                Rule 2- Get Datum from table, col, PK only. Guarranteed access rule.
                                Rule 3- Null values- not 0, not empty string. Missing i/f, inapplicable
                                        Indpendent of data type.
                                Rule 4- Online Catalogue- Physical data independence.
                                        Data dictionary is being refered for structure of data- it is stored as table. Standardized where to look for.
                                        How is it stored in Oracle? see the catalog tables of these DBMS.
                                Rule 5- Data sublanguage
                                        All DB must have data sublanguage to support.
                                                DDL,View Defn, AUthorization not only select query as traditional claimants.
                                Rule 6- 
                        Limitations
                                Not expressive
                                vocabulary is constraining.
                </Relational>
                <SemanticDM>
                    What?
                        They came because of limitations of hierarchical and Network have limitations.
                            User be shielded from physical (data independence). 
                            Supports rich constructs.
                            Types-	
                                    Static Modeling- Connected system.
                                    Dynamic Modeling- How behavior in run. Eg- Activity diagram, Sequence dig.
                    Why- Benefits
                            Economy of expression- One arrow means a lot in UML.
                            Integrity maintenance rules.
                            Modeling flexibility for different stakeholders.
                </SemanticDM>
                <XML>
                    <What>
                        A very simple mark-up language
                        Provides hierarchical structure to the data
                        Not restricted to just one type of meta data
                        Can be extended to capture many different types of semantics
                        so-called
                            Meta-data information included along with the data is called mark-up
                            Different markup langs have different metadata.
                                Formatting meta-data
                                Structural meta-data
                                Computational meta-data
                    </What>
                    <Why>
                        Data Interchange
                            Gives better flexibility on the structure of the data
                            Platform independent
                            Non-proprietary format
                            REL 
                            <unrelated>
                            Other forms of data interchange
                                With-in the same application
                                    Function arguments
                                With-in the same server
                                    Shared memory
                                Across the servers
                                    Data files
                                    HTTP data
                                    Remote procedure calls (web services)
                            </unrelated>
                        Content management
                            Separation of content from format
                            Content re-purposing
                    </Why>
                    <wellFormed>
                        Start tags should have matching end tags
                        Tags should be non-overlapping
                        There should be a single root
                        XML naming rules apply
                        XML is case sensitive
                        White space is preserved in data
                        <Elements>
                            Elements are extensible
                            Elements can be ordered in a particular sequence
                            Entity references- &lt for < , &gt, &amp, &apos, &quot
                        </Elements>
                        <Attributes>
                            Helps separate the data from meta-data
                            Generally uses less space than elements
                        </Attributes>
                        <namespace>
                            To prevent name clashes
                            A named bag of names
                            Used as a prefix to element names
                            Namespace prefix can be explicit or set to a default value
                            <defaultNamespace>
                                Similar to regular namespaces but prefix need not be specified
                                Parser assigns the prefix internally
                                There can be only one default namespace assigned in an XML document
                            </defaultNamespace>
                        </namespace>
                    </wellFormed>
                    <validXML>
                            <Why>
                                Increases application complexity
                                Same semantics but multiple formats
                                Redundant implementation of business rules (for each format)
                                Application may not be able to fulfill requests if all the data is not available
                                Service requesters would not know what format to use and hence not use the service
                            </Why>
                            <How>
                                <schemaRules>
                                    Any xml instance is validated against rules stated in schema- to declare is valid or not.
                                    <Why>
                                        They help establish a standard vocabulary between the caller and callee
                                        Even for standalone applications, schemas help focus on functionality instead of validation
                                    </Why>
                                    It is optional(change is easy). supports complex,user-defined Datatypes unlike RDB.
                                        <header>
                                                <schema xmlns="http://www.w3.org/2001/XMLSchema"
                                                        xmlns:sample="http://www.iiitb.ac.in/name"
                                                        targetNamespace="http://www.iiitb.ac.in/name"
                                                        elementFormDefault="unqualified">
                                                        <!-- Eth goes within -->
                                                </schema>
                                                <instanceXML>
                                                        <contacts xmlns="http://www.iiitb.ac.in/name"
                                                                xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                                                                xsi:schemaLocation="http://www.iiitb.ac.in/name name5.xsd">
                                                        </contacts>
                                                </instanceXML>
                                        </header>
                                        <Types>
                                                <dataTypes>
                                                        int,string,etc.
                                                </dataTypes>
                                                <simpleType name="st1">
                                                        <restriction base="integer">
                                                                <!-- facets-->
                                                                <ORCategory>
                                                                        <forString>
                                                                                <OR>
                                                                                        <enumeration></enumeration>
                                                                                        <pattern>
                                                                                                a.c\d*\*[ab?c]+b(a|c)d[A-Za-z]{7}
                                                                                        </pattern>
                                                                                        <length></length>
                                                                                        <maxLength></maxLength>
                                                                                        <minLength></minLength>
                                                                                </OR>
                                                                        </forString>
                                                                        <forInt>
                                                                                <OR>
                                                                                        <minInclusive></minInclusive>
                                                                                        <maxExclusive></maxExclusive>
                                                                                        <totalDigits></totalDigits>
                                                                                        <fractionDigits></fractionDigits>
                                                                                </OR>
                                                                        </forInt>
                                                                </ORCategory>
                                                        </restriction>
                                                        <list itemType="int"/>
                                                        <union memberTypes="int st1"/>
                                                </simpleType>
                                                <complexType name="ct1" mixed="true">
                                                        <OR>
                                                        <sequence minOccurs="0" maxOccurs="unbounded">
                                                                <!-- simple/complex element declaration -->
                                                        </sequence>
                                                                <!-- attributes,attributeGroups inside complexType tag -->
                                                        <all>
                                                        </all>
                                                        <choice>
                                                        </choice>
                                                        </OR>
                                                        <Eg>
                                                        </Eg>
                                                </complexType>
                                        </Types>
                                        <elementDeclaration>
                                                <element name="n1" type="st1"/>	<!-- ct1,int,etc ; can be anonymously laid also -->
                                                <element ref="g1"/>			<!-- even simple root level element accepted -->
                                                <attribute name="a1" type="int" use="optional"/>	<!-- prohibited,required -->
                                                <attributeGroup ref="ag"/>
                                        </elementDeclaration>
                                        <groupDeclaration>
                                                <group name="g1">
                                                        <sequence>
                                                                <!-- element declarations -->
                                                        </sequence>
                                                </group>
                                                <attributeGroup name="ag">
                                                        <!-- attrib declarations -->
                                                </attributeGroup>
                                        </groupDeclaration>
                                        <extendingTypes>
                                                <simpleContentType>
                                                        <element name="n1">
                                                                <complexType>
                                                                        <simpleContent>
                                                                                <extension base="int">
                                                                                        <attribute name="units"/>
                                                                                </extension>
                                                                        </simpleContent>
                                                                </complexType>
                                                        </element>
                                                </simpleContentType>
                                                <complexContentType>
                                                        <complexType name="ctm">
                                                                <complexType>
                                                                        <complexContent>
                                                                                <extension base="ct1">
                                                                                        <!-- <sequence> elts </sequence>, etc -->
                                                                                </extension>
                                                                        </complexContent>
                                                                </complexType>
                                                        </complexType>
                                                </complexContentType>
                                        </extendingTypes>
                                        <keys>
                                                <ID-IDREF>use type="ID" in root level and type="IDREF" anywhere to refer to one of them</ID-IDREF>
                                                <Key-Selector-Field>
                                                        <key name="k1">
                                                                <selector xpath=""/>
                                                                <field xpath="Author"/>
                                                        </key>
                                                        <keyref name="kr1">
                                                                <selector xpath=""/>
                                                                <field xpath="relative"/>
                                                        </keyref>
                                                </Key-Selector-Field>	
                                        </keys>
                                </schemaRules>
                                <designStrategies>
                                        <RussianDoll>
                                        </RussianDoll>
                                        <salamiSlice>
                                        </salamiSlice>
                                        <venetianBlind>
                                        </venetianBlind>
                                        <gardenOfEden>
                                        </gardenOfEden>
                                </designStrategies>
                            </How>
                    </validXML>
                    <XPath_XQuery>
                    doc("file1.xml")
                    <path/>
                    <predicates>
                            Conditional
                                    [starts-with(name,'F')]
                                    [num>50 and num<100]
                            IN clause- [@dept=('ACC','WMN')]
                                    flwor- where $prod/dept = ('ACC','WMN')
                            Aggregate function
                                    sum,count,abg,max,min, round, ceiling
                            string function
                                    substring,concat;upper-case,lowercase; trim,replace,length
                            positional predicates
                                    //prod[4]
                                    //prod[last()-1]
                                    //prod[position()<3]
                                    (//prod/name)[1]			//1st name in xml
                                    //prod/name[1]				//1st name of all products
                            Join
                                    let doc1="1.xml"
                                    let doc2="2.xml"
                                    for $item in doc1//prod
                                    ,$prod in doc2//item
                                    where $item/@num=$prod/num
                                    return <$item/>
                    </predicates>
                    doc("1.xml")//product[@dept]	//go to product tag anywhere along hierarchy which has dept attrib.
                    eg- doc("1.xml")/catalog/product[@dept="PTU" and num>50]
                    <flwor>for $prod in doc("1.xml")//product
                            let $p=$prod/@dept
                            where $p = 'ACC'			//could have done without let also
                            return <x>$prod/name</x>
                    </flwor>
                    </XPath_XQuery>
                    <API>
                        <DOM>
                            Document Object Model (DOM) is a programming interface for XML processing
                            Loads the XML document as a tree into the memory
                            APIs are available to traverse the tree and perform operations (read, create, modify, delete)
                        </DOM>
                        <SAX>
                            Simple API for XML (SAX) is an event-driven programming interface for XML processing
                            Parser fires events (calls to application functions) when parsing the XML
                            Application writes handlers for the various events (e.g., startElement)
                            Does not load the entire XML into the memory
                        </SAX>
                    </API>
                </XML>
                <OODB>
                        <FeatureOOPL>
                                <Encapsulation>
                                        wrapping func- control their access (restricted by setter/getter)	
                                        Thinking in terms of what (interface and effect) and not how (implementation)
                                        @Car gives us interface to control the car. The engine may be replaced from Diesel to CNG.
                                            Flexibility to change implementation details for better performance.
                                        @Reservation System- Think Data and operation= seat and reserved; Determine Available Seats, Reserve a Seat, Cancel a reservation, Find a block of available seats.
                                </Encapsulation>
                                <Polymorphism>
                                        H/w does it. Now, programs can do it.
                                </Polymorphism>
                                GUI components ask for properites- OO approach.
                                More expressive than ER cuz same ecosystem.
                                OO World is more natural than RDBMS. Objects/Entities/Tuples don't evolve in RDB, no methods.
                        </FeatureOOPL>
                        <Persistence>
                                Java Persists thru 
                                    Object uniquely Identified thru OID.
                                Objects clustered in blocks- all interconnections get loaded together.
                                    cuz disk is a killer.
                                <SharablePersistence>All apps can access the data</SharablePersistence>
                                <AccessMechanisms>
                                        <AssociatiiveAccess>Similar to DataProperties of Semweb.
                                                Traditional ways
                                                Eg- Relational- retrieve all accounts with balance less than 1K. or branch is BLR.
                                        </AssociatiiveAccess>
                                        <NavigationAccess>Similar to ObjectProperties of Semweb.
                                                Navigate thru relp to get i/f
                                                Eg- Branch of all people with amount less than 1K.
                                                Join in RDBMS and dot operator in PL.
                                        </NavigationAccess>
                                </AccessMechanisms>
                                <Implementations>
                                        <ProgrammingCentric>
                                                Persistence extensions are added to OO Language.
                                                <ByInheritance> PObject class is inherited by whichever class that wants serialization.
                                                        God Class to handle all persistence.
                                                                functions like load, save.
                                                        Any class to handle persistence inherits and overrides it.
                                                                All instances of a type will become persistent without worrying mechanisms.
                                                                Transparent Persistence.
                                                                Demerits
                                                                    Pollutes my design. Fundamental nature of my class changed.
                                                                    3rd party libraries (like collections) called STL(standard template libraries)
                                                                        can't be written- JVM don't know mechansim
                                                                        solution- vendor does needful.

                                                                    Concurrency control mechanism is better.
                                                                    Better performance in update intensive queries.
                                                        locking objects locks path.
                                                        Eg- Versand.
                                                </ByInheritance>
                                                <StorageClass>
                                                        like register int i; persist int i;
                                                        many levels of access- decide your level of access- segment level mostly used.
                                                            retrieves entire segment containing the object.
                                                            Use it, transaction object comes with customer if stored there.
                                                        Probs
                                                                Locking a cluster, other objects suffer.
                                                                [Decision] per instance basis, whether it goes in persistence or not.
                                                </StorageClass>
                                                <viaAPI>
                                                    Like JDBC, we have OODB API.
                                                    Object container stores on HDD.
                                                    can't dethrone RDBMS so used for cache in middleware.
                                                </viaAPI>
                                        </ProgrammingCentric>
                                        <DataCentric>
                                                <ORDBMS>
                                                    RDBMS core and OO extensions.
                                                        Java constructs like inheritance are supported in schema.
                                                    Methods in user space; Only attribute values of Objects go inside DB.
                                                        extended navigational access. Has an object identity.
                                                        violates 1NF to allow nested table,etc.
                                                            DB type system include any user defined data type. SQL extensions to define Schema.
                                                            use DB language to provide behavioral implementation for objects.
                                                            No Paradigm shift- investment preserved and possible to navigate to OODB.
                                                    Oracle burnt its hands on it.
                                                    ORDBMS Refer- https://docs.oracle.com/cd/B14117_01/appdev.101/b10799/adobjint.htm
                                                </ORDBMS>
                                                <ORM>
                                                    UML to DDL. Hibernate Programming approach.
                                                        Frameworks create a bridge of communication.
                                                        No manual connections/SQL Query. I use objects only to interact with DB.
                                                            Less Frequent SQL but st necessity and room for that.
                                                        Conversion done on the fly.
                                                            Static Mapping- to generate underlying table definitions (schema) from class definitions.
                                                            Dynamic Mapping- converting request into rows and result set into objects on fly.
                                                        Freedom from SQL injection attack.
                                                        Ease of changing back-end database.
                                                        Saves time especially when project has a scale to it.
                                                    Lack of standardization- OODB doesn't become famous.
                                                    Impedance Mismatch exists and needs to be addressed
                                                        Inheritance- not there in RDBMS (highly proprietary if at all)
                                                            Parent class referring child object. No "pointer" to child object.
                                                        Granularity- 1 table may have many classes corresponding and vv.
                                                        Identity and Equality- 2 apps use the same record- which persists?
                                                        Association- No directionality in RDB, Class User has address Attrib- 
                                                            user can navigate to address but 
                                                            for address to navigate to user- we have to put user reference there.                                                                        
                                                        Data Types- Varchar vs String.
                                                        Probs already as a programmer + Translating your restrictions into DB terms.
                                                            Take your object throw on disk and go home!
                                                    Eg- Hibernate uses ORM technique.
                                                </ORM>
                                        </DataCentric>
                                </Implementations>
                        </Persistence>
                        <MappingStrategies>                                              
                                Seamless means- no loss of data. Transparent to User. Impedance Mismatch.
                                Enterprise applications rarely employ only one data model
                                Different models are employed for different purposes
                                Eg- 
                                    OO model for programming
                                    Relational model for persistence
                                    XML for data interchange
                                Goal of mapping is to allow movement of data across these various models in a seamless manner
                                Mapping must preserve the data (loss-less)
                                Mapping must maintain consistency of relationships (to the extent possible)  
                                <Types>
                                        <Static>
                                                Rules to convert 1 schema to another.
                                                Eg- UML to RDBMS. Customer Class in OO to customer table.
                                        </Static>
                                        <Dynamic>
                                                Convert at runtime at instance level memory data (RAM).
                                                Instances of 1 Data Model into that of another (based on schema).
                                                Eg- Checks out translation of customer class and figures out a way to convert.
                                                Although we make up our mind for DM at start of project but projects needs may necessitate different DM or exchanges,etc.
                                        </Dynamic>
                                </Types>
                                <Specifics>
                                        <OO2OOPL>
                                                <StaticMapping>
                                                        OO uses UML and has 1to1 mapping to OOPL for most constructs.
                                                        No FK here. Don't say ID.
                                                        for 1:M Instructor:Courses- 1 side has a complex object with other object as member; other will have collection of other objects.
                                                                this is bidirectional- so that we don't lose i/f like all subjects instructor teaches.
                                                        Responsibility lies of Programmer's shoulders to maintain consistency.
                                                        <Directionality>
                                                                <uni>
                                                                        able to access "head" class from "tail" class- arrow mark in association.
                                                                        Taking a stand that our navigational access in 1 direction and not the other.
                                                                </uni>
                                                                <bi>
                                                                        Both side information is preserved- access i/f in either direction.
                                                                </bi>
                                                        </Directionality>
                                                        <Composition>
                                                                Existential dependence.
                                                                In Java, we control scope of variables not lifetime(that's garbage collector's job)
                                                                        so make instance of child class inside of the parent class makes sure that access is lost along with parent class.
                                                                        making instance outside and attaching thru reference variable passing is not a good design- lingering child.
                                                                Another idea may be to use anonymous instantiation in constructor or set reference variable to null. after passing.
                                                        </Composition>
                                                        <Aggregation>
                                                                works similar to association.
                                                        </Aggregation>
                                                </StaticMapping>
                                                <DynamicMapping>
                                                        UML has no schema-instances, No programming envt. So makes no sense.
                                                </DynamicMapping>
                                        </OO2OOPL>
                                        <OO2R>
                                            Leads to Semantic degradation- information lost.
                                                OO program requests for an object- mapping file translates into sql and result is delivered back as objects as per mapping file.
                                                    Does hibernate support Inverse Relationship.
                                                    JDBC- someone has taken the trouble to standardize it.
                                                Mapping rules similar to ER mapping rules.
                                                    object class to Relational table
                                                    class attribs become relational columns.
                                                    Inheritance
                                                        Non-partitioner Sub-classes
                                                            OR-3a: Create one table for the super-class and one table each for every sub-class
                                                            OR-3b: Include a discriminant column in the super-class table
                                                            OR-3c: The PK of the sub-class tables become FK as well and they refer to the super-class table
                                                        Partitioner Sub-classes
                                                            If A has two sub-classes B and C, then the following holds true:
                                                                B V C = A
                                                                B  C = 
                                                                OR-4a: Create one table for each of the sub-classes
                                                                OR-4b: Include the super-class attributes as columns in each of the sub-class tables
                                                    Associations
                                                        OR-5a: One:Many
                                                            To the table corresponding to the many end of association, add a foreign key that refers to the one end of the association
                                                        OR-5b: One:One
                                                            To table corresponding to either end of association, add a foreign key that refers to the table at the other end
                                                        OR-5c: Many:Many
                                                            Create a new table corresponding the association
                                                            To the new table, add respective foreign keys that refer to the tables at both the ends of the association
                                                    Aggregation and Composition- same lines
                                                        General associations and aggregations use shallow delete (set corresponding foreign key to NULL or simply delete the row itself)
                                                        Composition uses cascading delete (can be implemented using triggers too)
                                        </OO2R>
                                        <R2OO>
                                            <Static>
                                                Useful for migration.
                                                    Relevance is lost.
                                                    Metadata of OOPL is class diagram.
                                                useful modernizing existing relational applications.
                                            </Static>
                                            <Dynamic>
                                                Hibernate like tools.
                                            </Dynamic>
                                        </R2OO>
                                        <XML2OO>
                                            OO programs need the ability to easily parse and process structured data present in XML.
                                            <Static>
                                                XO-1: Map every complex element to an object class
                                                XO-2: Map every simple element but having attributes to an object class
                                                XO-3a: Map element attributes to attributes of the corresponding class
                                                XO-3b: Map simple elements to attributes of the class corresponding to the enclosing element
                                                XO-4a: Map all embedded complex elements to the association relationship (rather than inheritance or aggregation or composition)
                                                XO-4b: Choose the multiplicity of the association based on maxOccurences value of the embedded elements
                                            </Static>
                                            <Dynamic>
                                                Many libraries, frameworks, tools available- JAXB,Castor,XMLBeans,JDOM,etc.
                                            </Dynamic>
                                        </XML2OO>
                                        <OO2XML>
                                            Y? OO programs need the ability to easily convert object data into XML format for sharing it with other systems 
                                            <Static>
                                            Causes semantic degradation- inheritance, association, etc.
                                                OX-1: Map every object class to a complexType element
                                                OX-2: Map all scalar class attributes to either simpleType elements or attributes
                                                OX-3: Map all collection class attributes to elements with multiple occurrences
                                                Inheritance 
                                                    Non-partitioned sub-classes
                                                        OX-4a: Create one complex element for the super-class and one complex each for every sub-class
                                                        OX-4b: Embed the sub-class complex element as part of the super-class complex element
                                                        Eg- Person instance can be student as well as researcher. A person maybe nothing also. So create a person complex element and minOccurs,maxOccurs,choice for stud,researcher,etc.
                                                    Partitioned sub-classes
                                                        OX-4c: Create one complex element for the super-class and one complex each for every sub-class
                                                        OX-4d: Embed the super-class complex element as part of the sub-class complex element
                                                        Eg- Person can't exist independently. It has to be either a student OR a teacher OR a researcher but not > 1.
                                                            making a person object (which doesn't exist independently) doesn't necessitate putting exactly one of subclass' instance in them. So, make instances of subclasses and give details of superclass elements like PersonName as a complexType Person.
                                                Association
                                                    OX-5a: One:Many
                                                        Embed either one of the complex elements inside the other complex element taking care of the maxOccurs attribute appropriately
                                                    OX-5b: One:One
                                                        Embed either one of the complex elements inside the other complex element
                                                    OX-5c: Many:Many
                                                        XML does not support Many:Many relationships directly
                                                        Use of key and keyRef is the only option
                                                        Create a new complex element having keyRef elements pointing to complex elements corresponding to each end of the relationship
                                                Aggregation and Composition
                                                    OX-6: Inside the complex element corresponding to the container object, embed the complex element corresponding to the component
                                            </Static>
                                            <Dynamic>
                                                Generic dynamic mapping from OOPL to XML is a topic for deeper investigation
                                                Home-grown solutions are developed that is specific to each application
                                            </Dynamic>
                                        </OO2XML>
                                        <XML2R>
                                            <Static>
                                                Complex Element- Table	Create a table for every complex element in the XML
                                                Simple Element (Multiple occurrence)- Table	Create a table for every simple element that can occur multiple times
                                                Do not share child tables- If types or complex elements are reused as children of multiple parent elements, create on table for each such occurrence
                                                Simple Element (Single occurrence)-> Column- For every simple element, create a column in the table corresponding to the parent of the simple element
                                                Attribute-> Column- For every attribute, create a column in the table corresponding to the element of which it is an attribute
                                                Introduce surrogate keys for all tables- In every table, add a column that acts as a surrogate key
                                                Multi-Occurrence Child: Introduce foreign key in child-	For every child element (simple or complex) that can occur multiple times, introduce a foreign key in table corresponding to the child element
                                                Single-Occurrence Child: Introduce foreign key in parent- For every single-occurrence complex child, introduce foreign key in the table corresponding to the parent
                                            </Static>
                                            <Dynamic>
                                                Side Note- XML can be supported by SQL as a datatype.
                                                Dynamic mapping typically provided off-the-shelf by database vendors using proprietary libraries and utilities
                                                Examples
                                                    Oracle XSU (XML-SQL Utility)
                                                    IBM XML Extender for DB2
                                                    MS SQLServer uses SQL-92 extensions
                                            </Dynamic>
                                        </XML2R>
                                        <R2XML>
                                            <Static>
                                                R2X mapping is straight forward
                                                    Resultant XML structures are generally quite flat without any semantic richness that are generally associated with XML
                                                The mappings are not commutative 
                                                    (i.e., if you do X2R on an XML schema and then do R2X on a the resultant relational schema, you will NOT get back the original XML schema)
                                                G1: Create a complex element for every TABLE
                                                G2: Create either an XML attribute or a simple element for every COLUMN in the table


                                            </Static>
                                        </R2XML>
                                </Specifics>
                        </MappingStrategies>
                </OODB>
                <NoSQL>
                    Comes with gigantic volumes of data (webapps)- RDBMS slogs
                    soln- scale up the hardware
                    soln2- scale out- distribute the DB on multiple hosts
                            No schema (handles structured/ unstructured data, Big/Small Data(TB to PentaBytes))
                                    Redundancy and inconsistency
                            ACID no guarrantee. CAP guarranteed
                            Highly specialized (performance), less functionality (not challenge to RDB)
                            Address probs at app level		
                                    No Joins (for performance)
                                    No Rollback- Not for complex transaction- eg- Banks
                                    No constraint support- val1 = val2+val3
                            Types
                                    Key-Value (eg- Redis)
                                    Tabular (big Table of Google)
                                    Document Oriented (MongoDB,CouchDB)
                            Query Language (Other than SQL)
                    <SomeEg>
                        <Cassandra>
                            documentation works fine
                            sudo service cassandra start/stop, nodetool status
                            sudo apt install python-pip, pip install cassandra-driver, export CQLSH_NO_BUNDLED=true in shell, ./cqlsh, SELECT cluster_name, listen_address FROM system.local;
                            Installed 2.8 version (latest version didn't work) of cassandra using datastax- http://localhost:8888/opscenter/index.html for metrics and cql shell available, start the cql shell AND then the java code works.
                            using nodetool tablestats tlg_v_0- for numbers for each table inside the DB.
                                    the service is set to start automatically at reboot- checked from services.msc- the Datastax DDC server was set to automatic. 
                                            cqlsh is just its client.
                                            JVM_OPTS="$JVM_OPTS -Djava.rmi.server.hostname=127.0.0.1
                                                    #-Xloggc:${CASSANDRA_HOME}/logs/gc.log"
                                            set the nodetool path for access in command prompt.
                                    nodetool flush
                            cassandra datastax version 3.5 wasn't starting and giving exception as 'Exiting due to error while processing commit log during initialization'
                                solved by deleting the commitlog folder's contents- that's it- restarting made it work.
                        </Cassandra>
                        <HBase>
                                Version- 1.1.2.2.4.0.0-169 (check version inside hbase shell)
                                hdfs- running- if snamenode not running- that's ok.
                                hbase- service start from webui- takes about (5-8 mins) with 11GB RAM, 4 CPU.
                                confirm- http://192.168.237.131:16010/master-status
                                hbase shell
                                HBase- Key, Value Pairs, Group into nodes not by country- why? equal distribution.
                                        why hashing- lesser time and increase randomness.
                                        Process of finding max mark across sems- stored on 4 nodes equally
                                                Mapper- All nodes calculate their max.
                                                Reducer- calibrate their results for final outcome.
                                JDBC Code
                                        Create table exception- org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
                                                add domain resolution- 127.0.0.1   manuzhang-U24E
                                                surprisingly,even after removing, the exception didn't come later.
                                        Socket closed
                                                ensure hbase service is running from ambari, zookeeper is running- domain names in /etc/hosts file and VM and property.conf is appropriate
                                                try create table in hbase shell
                                size of hbase table
                                        Space occupied by hbase
                                                go to: http://192.168.237.137:50070 port - utilities menu- browse the file system- to browse the hadoop file system.
                                                check- /apps/hbase/data/data/default/TLG_Wide- this location do- hadoop fs -du -h /apps/hbase/data/data/default/- it lists size occupied by all the files/ tables.
                                                Ensure that you run flush command after insertion to ensure memstore data is moved to the disk
                                                                                                                    flush 'tablename'
                                                                                                                    flush 'REGIONNAME'
                                        hadoop fs -du -h /apps/hbase/data/data/default/- it lists size occupied by all the files/ tables.
                                        hbase
                                                create 'TLG_Wide', 'TagsWide'
                                                describe 'TLG_Wide'
                                                use TLG_Wide
                                                describe databases;
                                                describe 'TLG_Wide';
                                                put 'TLG_Wide', '1', 'TagsWide:ValueFloat', '80.4'	//even rowkeys weren't necessary here.
                                                count 'TLG_Wide'
                                                list 'TLG_Wide'
                                create 'D4', {NAME=>'T4', VERSIONS=>1}
                                get 'TLG_WideC', 'fe6ea54e-d1be-4df7-af1b-cc85274662bb+1492021800000', {COLUMN => 'TagsWide:TagsTimeStamp', VERSIONS => 3}- get versions data
                                create 't1', { NAME => 'cf1', COMPRESSION => 'SNAPPY' }
                                describe 't1'
                                {NAME => 'user', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'FALSE',BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'false'}
                                alter 'example_table', {NAME => 'example_family', REPLICATION_SCOPE => '1'}
                                Properties importance
                                        if not conf.set("hbase.zookeeper.property.clientPort", "2181");	//it uses default- which may not be correct
                                                admin.tableExists(tableName) fails
                                                or insertion- currenttable.put()
                                                retrieve- Unable to get data of znode /hbase-unsecure/table/TLG_Wide because node does not exist (not an error)
                                        conf.set("hbase.zookeeper.quorum", "192.168.237.137");
                                                Retrieve- Connection connection = ConnectionFactory.createConnection(conf); throws exception
                                                        java.net.ConnectException: Connection refused: no further information
                                                        java.nio.channels.ClosedChannelException
                                                        java.net.ConnectException: Connection refused: no further information
                                                insertion- same
                                                table creation same
                                        conf.set("zookeeper.znode.parent", "/hbase-unsecure");
                                                Table Creation- admin.tableExists(tableName) gives null pointer error
                                                retrieve- Result r = currentTable.get(g); stops responding (not error)
                                                Writing- currentTable.put(p); throws exception of null pointer.
                                        Ignore following exceptions on connection
                                                java.io.IOException: HADOOP_HOME or hadoop.home.dir are not set.
                                                java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
                                pom.xml
                                        Exception in thread "main" java.io.IOException: java.lang.reflect.InvocationTargetException...
                                        Caused by: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.s3.S3FileSystem not found
                                        because phoenix also needs to be included in pom.xml
                                                                                            Later observation- Removing phoenix dependency does no harm- in fact in 1 case I had to ensure that pom.xml doesn't have phoenix to have application executed
                                Running BMS for HBase, do remember to set sandbox Domain IP in /etc/hosts file.
                                        no error- only 0-6 calls/sec.
                                Observations in jdbc
                                        Table already exists? then no error message but stops
                                        Minimal hbase-site.xml required
                                                <configuration>
                                                        <property>
                                                                <name>hbase.rootdir</name>
                                                                <value>hdfs://sandbox.hortonworks.com:8020/apps/hbase/data</value>
                                                        </property>
                                                        <property>
                                                                <name>hbase.hbase.zookeeper.quorum</name>
                                                                <value>192.168.237.140</value>
                                                        </property>
                                                        <property>
                                                                <name>hbase.zookeeper.property.clientPort</name>
                                                                <value>2181</value>
                                                        </property>
                                                        <property>
                                                                <name>zookeeper.znode.parent</name>
                                                                <value>/hbase-unsecure</value>
                                                        </property>
                                                </configuration>
                                        Must add in code 
                                                conf.set("hbase.zookeeper.quorum", "192.168.237.140");
                                                doesn't work with this in hbase-site.xml- takes localhost for no reason!!
                                                        Gives following error
                                                                Error- Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProvider
                                                                        Error comes on hbase.rootdir property not included
                                        Wide Column
                                                Object fieldDataValue = table.getTypeOfTable().getValueForField(col);
                                                p.addColumn(columnFamilyBytes, Bytes.toBytes(colName),
                                                                fieldDataValue.toString().getBytes(Charset.forName("UTF-8"))); // convert2Bytes(col, fieldDataValue)
                                        Tall Column		
                                                Object fieldDataValue = table.getTypeOfTable().getValueForField(col);
                                                p.addColumn(columnFamilyBytes, Bytes.toBytes(col.getColName()),
                                                                fieldDataValue.toString().getBytes(Charset.forName("UTF-8"))); // convert2Bytes(col, fieldDataValue)
                                        Changing vm- change the hbase file
                                                resolve domain name in /etc/hosts file
                                        upserts happening in hbase- same timestamps for rows insertion- they are rejected/ updated- so #rows is different from expected
                                                Concept of versioning- after making date static- we ended up with only 1000 rows- although inserted 1M.
                                                This can also explain why rows were getting eaten up by 35%- case of upserts with same rowkeys.
                                        Call rate reduced to 0 calls/sec- region server had turned off.
                                        Many hacks to save space but with date as long, it wasn't working in comparisons
                                            cuz stored as string b4 writing and string comparison says 99 > 104, so epochtime was showing wrong results.
                                <Shell>
                                        can't do backspace with hbase shell- press shift/Ctrl and backspace
                                                                                                    drop 'TLG_Widj' ; create 'TLG_Widj', 'TagsWidj'
                                                                                                    it just should not end with the semi-colon in hbase client
                                        DDL
                                                C
                                                create table,colFamily 
                                                R
                                                list - all tablenames
                                                discribe table
                                                exists 'emp'
                                                U
                                                alter 'emp', NAME ? 'personal data', VERSIONS ? 5
                                                alter 'emp', READONLY
                                                alter  table name , delete ?  column family  
                                                D
                                                disable 'table'
                                                        disable_all 'raj.*'		//tables start with the letters raj
                                                drop 'table'
                                                        drop_all t.* 
                                                truncate 'table name'- drop and recreate empty.
                                        DML
                                                C
                                                put
                                                put 'emp','1','personal data:name','raju'	//put 'table', 'rowid', 'colFamily data:colName', 'value'
                                                put 'emp','1','personal data:city','hyderabad'
                                                put 'emp','1','professional data:designation','manager'
                                                put 'emp','1','professional data:salary','50000'
                                                R
                                                get
                                                        get 'emp', '1'- get 'table', 'rowid'
                                                        get 'emp', 'row1', {COLUMN ? 'personal:name'}
                                                scan 'employee'- all outputs
                                                count 'tableName'- no of rows

                                                U
                                                read table
                                                D
                                                delete 'emp', '1', 'personal data:city',1417521848375	//delete 'tableName', 'rowid', 'colFamily data:colName', 'TS'
                                                deleteall 'emp','1'
                                        DCL
                                                grant 'Tutorialspoint', 'RWXCA'
                                                revoke 'Tutorialspoint'
                                                user_permission 'emp'
                                        Uncat
                                                create 't1', { NAME => 'cf1', COMPRESSION => 'SNAPPY' }
                                                Errors
                                                        ERROR: Can't get master address from ZooKeeper; znode data == null -- HBase service hasn't been started- go to ambari.
                                                scan 't1', {COLUMNS => 'c1', TIMERANGE => [1303668804, 1303668904]}
                                                scan 't1', { TIMERANGE => [0, 1416083300000] }
                                                scan 't1', {VERSIONS => 100 }
                                                scan '.META.', {COLUMNS => 'info:regioninfo'}
                                                scan 't1', {COLUMNS => ['c1', 'c2'], LIMIT => 10, STARTROW => 'xyz'}
                                        Filters
                                                show_filters
                                                scan 'airline',{ FILTER => "KeyOnlyFilter()"}
                                                scan 'airline',{ FILTER => "FirstKeyOnlyFilter()"} (rowkeys only once)
                                                scan 'airline', {FILTER => "(PrefixFilter ('row2'))"} (only specified rowkey)
                                                scan 'airline', {FILTER => "(PrefixFilter ('row2')) AND ColumnPrefixFilter('destination')"} (specified columnName)
                                                scan 'airline',{FILTER => "MultipleColumnPrefixFilter('source','destination','date')"} (specified columns)
                                                scan 'airline',{FILTER => "ColumnCountGetFilter(2)"} (putting limit on #columns)
                                                scan 'airline',{FILTER => "PageFilter(1)"} (returns pagesize number of rows)
                                                scan 'airline',{FILTER => "InclusiveStopFilter('row1')"} (rowkey after which you stop looking for more)
                                                scan 'airline',{ FILTER => "QualifierFilter(=,'binary:flightno')"} (expression for every columnName to qualify for listing)
                                                scan 'airline', { COLUMNS => 'flightbetween:source', LIMIT => 4, FILTER => "ValueFilter( =, 'binaryprefix:hyd' )" } (valuefilter)
                                                scan 'airline' ,{ FILTER => "SingleColumnValueFilter('flightbetween','source',=, 'binary:hyd')" } (show all columns if a condition matches for a rowkey)
                                </Shell>
                                JDBC
                                        Scan- for shortlisting rows
                                        GET- for shortlisting column names
                                If Table Doesn't exist in HBase Java- org.apache.hadoop.hbase.TableNotFoundException: TLG_Widf
                                                                            Exceptions
                                                                                    hadoop dependency- 
                                                                                            connection started happening but 'table not found' exception
                                                                                                    or ConnectionFactory.getConnection() failing
                                                                                            java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation- hadoop.fs.FileSystem in stacktrace
                                                                                            <!-- <dependency>
                                                                                                    <groupId>org.apache.hadoop</groupId>
                                                                                                    <artifactId>hadoop-hdfs</artifactId>
                                                                                                    <version>2.7.1</version>
                                                                                            </dependency> -->
                                                                                            solution- removed it
                                                                                    Set /etc/hosts
                                                                                            Got HBase configuration properties from the Ambari HBase- one configuration was unmatching (not wrong)- 
                                                                                                    file:///var/lib/ambari-metrics-collector/hbase instead of hdfs://192.168.50.147:8020/apps/hbase/data- both work!!
                                                                                                    Root cause- /etc/hosts had to be updated- 192.168.50.143	sandbox.hortonworks.com
                                                                                                    without this updation, it just gets stuck when it comes to put method of table, although there was no reference to the domain name within the code (must have been with zookeeper or so)

                                                                                    Creating a preexisting table- exits without notification.
                                                                                    Ref- HBase Java API Guide: The definitive Guide is good reference but showcases deprecated APIs- so APIs guided to using newer classes.
                                                                                    Setting durability
                                                                                            this.durability = Durability.valueOf(HBaseConnection11.getHbaseProperties().getHBaseDurability().toUpperCase());
                                                                                            p.setDurability(durability);
                                                                            Darn slow hbase
                                                                                    cuz hbase service not turned on
                                                                                    or hbase ain't working or /etc/hosts need updation
                                                                            Ensure that the configuration properties for HBase are directly copied from ambari HBase
                                                                            HBase Java client worked well with HDP2.4 but not with HDP2.5
                                                                                    maybe cuz of the docker installed in 2.5
                                                                            Exception- for running it from centos os (not windows- as middleman) 
                                                                                    java.io.IOException: java.lang.reflect.InvocationTargetException and failed to create directory /var/lib/ambari/...
                                                                                    the resources like hbase-site.xml (being old) were creating mess. Removed them worked fine!
                                                                            to turn hbase thrift server on
                                                                                    /usr/hdp/2.4.3.0-227/hbase/bin/hbase-daemon.sh start thrift
                                                                                    c++ code for thrift connection to hbase
                                                                                    thrift 1 and thrift 2 use the same port so gotta stop b4 starting another thrift2 was used for experimentation.
                                                                                    start and again start should give error- read logs and they tell port is open
                                                                            Code working on windows but on centos client- 0 calls/sec why?
                                                                                    Property.conf was picked from windows- where \r\n is different from \n of linux.
                                                                                    so opening the file showed ^M everywhere- removed them and re-ran.
                                                                                    Still shows 0calls/sec but it was short lived after about 1 minute or so- cluster started inserted- maybe some setup.
                                                                            Hosts stated as nodes that have zookeeper for HBase and not the master node of cluster.
                                                                            Connection object doesn't get created without /etc/hosts being updated (?Confirm)
                        </HBase>
                        <InfluxDB>
                                Ubuntu VM- 8GB RAM, 4CPU, Bridged adapter
                                        location- 
                                        influx db- v1.2.0
                                                wget https://dl.influxdata.com/kapacitor/releases/kapacitor-1.1.0~rc2_linux_armhf.tar.gz (standalone linux binaries 64-bit)
                                                tar xvfz kapacitor-1.1.0~rc2_linux_armhf.tar.gz
                                                ./usr/bin/influxd; influx to get shell
                                how to create table- no, it's called measurement- what are vocabs for this data model
                                        running tpcds queries on influx- we won't query on time- that's ok.
                                InfluxDB manual for demo- terms and vocabs.
                                CPU used- 22.3, 2.9 memory (influxdb)
                                        8, 2.6
                                Batch process- works just fine for 
                                        10K- 11threads, 9 threads- fine
                                        155 threads (9616), 
                                        3 threads- fine
                                        3 threads- 20K- 17332
                                        20 threads 20K- 18000
                                if opening too many connections, we get an exception of address already open- use single connection 
                                Influx writer was failing to write too many records in a thread- reason- connection can't be kept indefinitely open.
                                        maybe the spirit is open the connection do the job and close it eftsoon.
                                JDBC- compiled to jar- the OSS influx client
                                        Let's try to understand the OSS code of influx sometime!! https://github.com/influxdata/influxdb-java/blob/master/src/main/java/org/influxdb/InfluxDBFactory.java
                                Disk space reqd by Influx
                                        show stats- for every database- check its shard's disk space occupied- for table not known.
                                org.influxdb was not working with pom version 2.1 but worked fine with 2.5
                                queries in influx group by
                                        select min(value) from pu_load_short group by host;
                                        not tried- SELECT MIN(column_name) FROM series_name group by time(10m)
                                        Influx query for tag trends- select * from TagsWideZipfian where TagUID='a50586aa-359a-41d2-9e86-29f1e0be7b99' and TagDate='2014-12-16' and time>=1490701528222;
                                influxDB.createDatabase(dbName) gives error that NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()
                                        so better create database beforehand or include dependency jars.
                                if IP is not valid- the error is java.net.SocketTimeoutException: connect timed out
                                Java client batch write
                                    enable batch has no significant impact on number of points that are getting written.
                                    points were not getting written because of racing condition- given thread.sleep between them- exact number of points were written.
                                            minimal sleep reqd between points addition is 1ms- it works fine.
                                            writing doesn't need any waiting time
                        </InfluxDB>
                        <Mongo>
                            download mongodb.com latest stable release for your os.
                            use installation wizard to get a command shell for mongo
                            run server- mongod.exe --dbpath "d:\set up\mongodb\data" 
                            run client- mongo.exe
                                    use mydb, db, show dbs, db.movie.insert(kv)
                                    db.dropDatabase()
                                    db.createCollection("mycollection")
                            db.createCollection("mycollection")
                            show collections
                            db.createCollection("mycol", { capped : true, autoIndexId : true, size : 
                               6142800, max : 10000 } )
                            >db.tutorialspoint.insert({"name" : "tutorialspoint"})
                            >show collections
                            db.COLLECTION_NAME.drop()
                            db.mycol.insert({
                               _id: ObjectId(7df78ad8902c),
                               title: 'MongoDB Overview', 
                               description: 'MongoDB is no sql database',
                               by: 'tutorials point',
                               url: 'http://www.tutorialspoint.com',
                               tags: ['mongodb', 'database', 'NoSQL'],
                               likes: 100
                            })
                            db.post.insert([
                               {
                                  title: 'MongoDB Overview', 
                                  description: 'MongoDB is no sql database',
                                  by: 'tutorials point',
                                  url: 'http://www.tutorialspoint.com',
                                  tags: ['mongodb', 'database', 'NoSQL'],
                                  likes: 100
                               },

                               {
                                  title: 'NoSQL Database', 
                                  description: 'NoSQL database doesn't have tables',
                                  by: 'tutorials point',
                                  url: 'http://www.tutorialspoint.com',
                                  tags: ['mongodb', 'database', 'NoSQL'],
                                  likes: 20, 
                                  comments: [	
                                     {
                                        user:'user1',
                                        message: 'My first comment',
                                        dateCreated: new Date(2013,11,10,2,35),
                                        like: 0 
                                     }
                                  ]
                               }
                            ])
                            db.post.save(document)
                            db.mycol.find().pretty()

                            db.mycol.find({"likes":{$lt:50}}).pretty()	
                            db.mycol.update({'title':'MongoDB Overview'},
                               {$set:{'title':'New MongoDB Tutorial'}},{multi:true})
                        </Mongo>
                    </SomeEg>
                    <GraphDB>
                        <Neo4J>
                            <GettingStarted>
                            Download and Install
                                    neo4j.com/download->community Edition->Download->Launch Wizard->Install->Run it->Start on server->localhost:7474
                            Environment setting
                                    neo4j.com/developer/example-data->drwho.zip download for data reference->extract into db location
                                            DB location in dialog box of neo4j launch.
                                            Start Server[->alert message- older version of neo4j->options->Database Tuning->Edit->uncomment allow_store_upgrade = true at the top]
                                            first time password is same and change it.
                            </GettingStarted>
                            <GUI>
                                    3 dots on left to see DB info
                                            Property keys- properties of nodes
                                                    click 1 key- all nodes with that property get selected.
                                            Relationship keys- relps
                                                    SELECT 1 relp types key and we get a subgraph.
                                            i tab for information- for learning
                                    Star for favorites- importing scripts and decrypt.
                                            Create node link-> creates query in editor section.
                                            Get some data-> eq to select query.
                                    Editor section- type query- add to favorites/ + to change/ arrow to exec.
                                            CREATE (n (name:"World")) RETURN "hello", n.name
                                    Result Section- Displays the graph/rows output.
                                            click a node- get it's data properties in status bar.
                                            set autocomplete (radiobutton)switch component off to ignore relps not asked for.
                            </GUI>
                            <DesignComponents>
                                    Basic Structure- Entities connected by a relationship.
                                    Relationship ~ Join in RDB
                                    Say- In social networking, Bob and Alice are friends with. member of Science group.
                                            message sent from bob to alice. Message is a node.
                                    Properties of entities/nodes- Data properties. can be different for same entity type => schemaless.
                                    Properties of relationships/edges- Member of science group since that data.
                            </DesignComponents>
                            <CypherQueryLanguage>
                                <Basics>
                                        Give pattern (instruction) to neo4j to match it from given DB and get desired results.
                                        write in editor console.
                                        ()- represents node within DB.
                                                MATCH () RETURN - means match all nodes and return relevant ones.
                                        variables in cypher- put it pattern- become columnNames in result by rows.
                                        MATCH (n) RETURN n limit 25 to get 25 nodes from huge DB.
                                                call all nodes with a variable n and return the set n so produced.
                                                returns set of nodes- of different types and their relps if autocomplete on.
                                        MATCH (o)-[r]->(d) RETURN o limit 25, MATCH (o)-[r]->(d) RETURN o,r,d limit 25
                                                - AND -> Give direction of relationship.
                                                [] stand for properties.
                                                returns all nodes which have some relationship moving outward with another
                                                r gives empty when it has no definition for the edge.
                                        Specific query- 
                                                specific relp that we are interested in.
                                                MATCH (o)-[r:appeared_in]->(d) RETURN o limit 25
                                                        subgraph returned which have relp appeared_in type.
                                        Shortlisting specific actor who took part in some episode
                                                in node component add curly braces and add components comma separated.
                                                MATCH(o{actor:'Tom Baker'})-[r:appeared_in]->(d) RETURN o,r,d limit 25
                                                        return graph of all relp where Tom Baker appeared.
                                                MATCH(o{actor:'Tom Baker'})-[r:appeared_in]->(d:{episode:'129'}) RETURN o,r,d limit 25
                                        Join on common object
                                                MATCH(o{actor:'Tom Baker'})-[r:appeared_in]->(d)<-[nr:NEXT]-(p) RETURN o,r,d,p limit 25
                                        Properties in return clause
                                                what if I am interested in only title data property of episode relp
                                                o{actor:'Tom Baker'}-[r:appeared_in]->(d)<-[nr:NEXT]-(p) RETURN o,r,d.title,p.title limit 25
                                                otherwise it return all data properties of a node.
                                        Using alias in queries
                                                AS keyword 		
                                                o{actor:'Tom Baker'}-[r:appeared_in]->(d)<-[nr:NEXT]-(p) RETURN o AS Actor,r,d.title AS Episode,p.title AS Previous limit 25
                                        Multiple patterns inside query
                                                comma separated, use the same variable referred before
                                                o{actor:'Tom Baker'}-[r:appeared_in]->(d)<-[nr:NEXT]-(p),(d)<-[pr:PREVIOUS]-(n) RETURN o AS Actor,r,d.title AS Episode,p.title AS Previous,n.title AS NEXT limit 25
                                        Eg- 
                                                CREATE (you:Person {name:"You"}) RETURN you
                                                MATCH (node:Label) WHERE node.property = "value" RETURN node
                                                Create Your Friends		
                                                        MATCH (you:Person {name:"You"})
                                                        FOREACH (name in ["Johan","Rajesh","Anna","Julia","Andrew"] |
                                                        CREATE (you)-[:FRIEND]->(:Person {name:name}))
                                </Basics>
                                "property-name-list" = "Property-name":"Property-Value"[,"Property-name":"Property-Value"]*
                                All queries need to be directional- otherwise, not supported (throws an error).
                                <Create>
                                    Neo4j Database Server uses this name to store this node details in Database.As a Neo4j DBA or Developer, we cannot use it to access node details
                                    Neo4j Database Server creates a label name as an alias to internal node name.As a Neo4j DBA or Developer, we should use this label name to access node details.
                                    An Id is associated with every node. Visible in Node property window (title)- 35 Billion is largest.
                                    <Nodes>
                                        CREATE ("node-name":"label-name"[:"label-name"]*[{"property-name-list"}])
                                        Eg- CREATE (dept:Dept)
                                        Eg- CREATE (emp:Employee{id:123,name:"Lokesh",sal:35000,deptno:10})
                                        CREATE (dept:Dept { deptno:10,dname:"Accounting",location:"Hyderabad" })
                                    </Nodes>
                                    <Relp>
                                        ["Select Query"]
                                        CREATE  ("node1-label-name")-["relationship-label-name":"relationship-name"{"define-properties-list"}]->("node2-label-name")
                                        [RETURN "relationship-label-name"]

                                        Eg- MATCH (e:Customer),(cc:CreditCard) CREATE (e)-[r:DO_SHOPPING_WITH ]->(cc) 
                                        Eg- MATCH (cust:Customer),(cc:CreditCard) CREATE (cust)-[r:DO_SHOPPING_WITH{shopdate:"12/12/2014",price:55000}]->(cc) RETURN r
                                        Eg- CREATE (fb1:FaceBookProfile1)-[like:LIKES]->(fb2:FaceBookProfile2) 
                                        Eg- CREATE (video1:YoutubeVideo1{title:"Action Movie1",updated_by:"Abc",uploaded_date:"10/10/2010"})-[movie:ACTION_MOVIES{rating:1}]->(video2:YoutubeVideo2{title:"Action Movie2",updated_by:"Xyz",uploaded_date:"12/12/2012"})
                                        Eg- MATCH (cust:Customer),(cc:CreditCard) WHERE cust.id = "1001" AND cc.id= "5001" CREATE (cust)-[r:DO_SHOPPING_WITH{shopdate:"12/12/2014",price:55000}]->(cc) RETURN r
                                    </Relp>    
                                </Create>
                                <Retrieve> Matching part to select and rendering part to present
                                    <Match>
                                            MATCH("node-name":"label-name")
                                        <Where>
                                            WHERE "condition" ["boolean-operator" "condition"]*
                                            condition = "property-name" "comparison-operator" "value"
                                            Eg- MATCH (emp:Employee) WHERE emp.name = 'Abc' RETURN emp
                                                using "Employee" or 'Employee' gives same impact
                                            IS NULL supported (all uninitialized are NULL by default)- all are shown by default
                                            IN["Collection-of-values"]
                                                Eg- MATCH (e:Employee) WHERE e.id IN [123,124] RETURN e.id,e.name,e.sal,e.deptno
                                        </Where>
                                    </Match>
                                    <Union>
                                        Must have same column names or use alias.
                                        <UnionAll>doesn't filter the duplicate rows.</UnionAll>
                                    </Union>
                                    <Render>
                                            RETURN "node-name"."property-name"[,"node-name"."property-name"]*
                                            <OrderBy>
                                                ORDER BY  "property-name-list"  [DESC]	
                                            </OrderBy>
                                            <Alias>
                                                RETURN cc.id as id
                                            </Alias>
                                            <LimitSkip>
                                                <Limit>LIMIT 2</Limit>
                                                <Skip>SKIP 5</Skip>
                                            </LimitSkip>
                                            <ChangeFontInUIView>
                                                Click on "style" tab (eye symbol) from properites tab in Node Property Window.
                                            </ChangeFontInUIView>
                                            <CaptionInUI>
                                                Generally Id is used inside UI nodes circles to identify the nodes.
                                                To change it click on id in Node Property Window's title. Use caption dropdown to choose the property name whose you wanna see.                                                                
                                                node is displayed by using "CAPTION by message" property.
                                            </CaptionInUI>
                                    </Render>
                                    <Node>
                                        MATCH ("node-name":"label-name")
                                    </Node>
                                    <Relp>
                                        MATCH ("node1-label-name")-["relationship-label-name":"relationship-name"]->("node2-label-name")
                                        RETURN "relationship-label-name"
                                    </Relp>

                                    MATCH (e)-[r:DO_SHOPPING_WITH ]->(cc) RETURN r
                                    MATCH (cust)-[r:DO_SHOPPING_WITH]->(cc) 
                                    RETURN cust,cc


                                    Eg-
                                            MATCH (dept: Dept)
                                            RETURN dept.deptno,dept.dname
                                            MATCH (DEPT: Dept)
                                            RETURN dept
                                </Retrieve>
                                <Update>
                                    <Set>
                                        To add/update a data property.
                                        "GetNode" SET "node-name"."property-name" = "value"[,"node-name"."property-name" = "value"]*
                                        Eg- MATCH (dc:DebitCard) SET dc.atm_pin = 3456 RETURN dc                                                        
                                    </Set>
                                    <Remove>To remove data properties/labels.
                                        "Select Nodes-name" REMOVE "property-name-list"
                                        Eg- MATCH(book:BOOK) MATCH (book { id:122 }) REMOVE book.price RETURN book
                                    </Remove>
                                </Update>
                                <Delete>
                                    "Select Node-names" Delete ["Node-name",]*
                                    Eg- MATCH (e: Employee) DELETE e
                                    Eg- MATCH (cc: CreditCard)-[rel]-(c:Customer) DELETE cc,c,rel
                                </Delete>
                                <Merge>
                                    MERGE = CREATE + MATCH
                                    Neo4j CQL MERGE command searches for given pattern in the graph, if it exists then it returns the results
                                    If it does NOT exist in the graph, then it creates new node/relationship and returns the results.
                                    MERGE ({"property-name-list"})  //duplicate entries avoided.
                                </Merge>
                                <CQLFunctions>
                                    <String></String>
                                    <Aggregation></Aggregation>
                                    <Relationship></Relationship>
                                </CQLFunctions>
                                <API>
                                    package com.tp.neo4j.java.cql.examples;
                                    import org.neo4j.cypher.javacompat.ExecutionEngine;
                                    import org.neo4j.cypher.javacompat.ExecutionResult;
                                    import org.neo4j.graphdb.GraphDatabaseService;
                                    import org.neo4j.graphdb.factory.GraphDatabaseFactory;
                                    public class JavaNeo4jCQLRetrivalTest {
                                       public static void main(String[] args) {
                                          GraphDatabaseFactory graphDbFactory = new GraphDatabaseFactory();
                                          GraphDatabaseService graphDb = graphDbFactory.newEmbeddedDatabase("C:/TPNeo4jDB");
                                          ExecutionEngine execEngine = new ExecutionEngine(graphDb);
                                          ExecutionResult execResult = execEngine.execute("MATCH (java:JAVA) RETURN java");
                                          String results = execResult.dumpToString();
                                          System.out.println(results);
                                       }
                                    }
                                </API>
                            </CypherQueryLanguage>
                        </Neo4J>
                    </GraphDB>
                    <Uncat>
                        <Influx>
                            http://www.thatsme-horoscopes.com/

                            written in go
                                    makes single binary with no dependencies
                            to handle high write and query loads.
                                    how? http api.
                                            built in web admin interface.
                                    Plugins support for other data ingestion protocols
                                    SQL like EE- tailored for aggregate data
                                            indexing in tags => fast and efficient queries
                                            frequent queries are more efficient.
                            Download and installation
                                    influxd
                                    influx
                                            on CLI exec, no news is good news.
                                            DDL
                                                    CREATE DATABASE (name)
                                                            _internal to start with.
                                                            for identifiers, if use any unicode with quotes, begin with digit, etc.
                                                            show databases
                                                            use (name)
                                            DML
                                                    Writing
                                                            points writing using line protocol- < measurement>[,< tag-key>=< tag-value>...] < field-key>=< field-value>[,< field2-key>=< field2-value>...] [unix-nano-timestamp]
                                                                    Eg- Insert cpu,host=serverA,region=us_west value=0.64
                                                                    Eg- INSERT temperature,machine=unit42,type=assembly external=25,internal=37
                                                            Using HTTP API
                                                                    POST request to query endpoint- curl -i -XPOST http://localhost:8086/query --data-urlencode "q=CREATE DATABASE mydb"
                                                                    curl -i -XPOST 'http://localhost:8086/write?db=mydb' --data-binary 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'
                                                                            default retention policy if it's not specified.
                                                                    using file- curl -i -XPOST 'http://localhost:8086/write?db=mydb' --data-binary @cpu_data.txt
                                                                    use batches if > 5k points, cuz 5 sec later, http request times out
                                                                    REST is the industry agreed style for organizing large numbers of endpoints. InfluxDB API makes no attempt to be RESTful.
                                                                    HTTP Response
                                                                            2xx: If your write request received HTTP 204 No Content, it was a success!
                                                                            4xx: InfluxDB could not understand the request.
                                                                            5xx: The system is overloaded or significantly impaired.
                                                                            Eg- curl -i -XPOST 'http://localhost:8086/write?db=hamlet' --data-binary 'tobeornottobe booleanonly=true'  
                                                                                    curl -i -XPOST 'http://localhost:8086/write?db=hamlet' --data-binary 'tobeornottobe booleanonly=5'
                                                                                    will return 400 error

                                                            Schemaless
                                                                    add new measurements, tags, and fields at any time
                                                                    but changing datatype by values throws error.
                                                            CLI
                                                            Client libraries
                                                            Data formats using graphite, etc
                                                    DataModel
                                                            organized by time series.
                                                            tables called measurements- like cpu
                                                            rows called points- cpu's point of measurement another point of measurement.
                                                            dimension against which measurement is taken- tag keys and their values- tag values.
                                                                    tags differentiate data sources and help fasten query response.
                                                                    both keys and values are string type.
                                                            measurement values at those points including time- field keys and field values.
                                                                    keys are strings and values are float (default)
                                                            Timestamp is UTC always- if unspecified, then local time's epochTime.
                                                    Queries
                                                            SELECT "host", "region", "value" FROM "cpu"
                                                            SELECT * FROM /.*/ LIMIT 1
                                                            SELECT * FROM "cpu_load_short"
                                                            HTTP API
                                                                    curl -G 'http://localhost:8086/query?pretty=true' --data-urlencode "db=mydb" --data-urlencode "q=SELECT \"value\" FROM \"cpu_load_short\" WHERE \"region\"='us-west'"
                                                                    InfluxDB returns JSON. Error also in JSON. Result array.
                                                                    Multiple queries- curl -G 'http://localhost:8086/query?pretty=true' --data-urlencode "db=mydb" --data-urlencode "q=SELECT \"value\" FROM \"cpu_load_short\" WHERE \"region\"='us-west';SELECT count(\"value\") FROM \"cpu_load_short\" WHERE \"region\"='us-west'"
                                                                    units of timestamp- curl -G 'http://localhost:8086/query' --data-urlencode "db=mydb" --data-urlencode "epoch=s" --data-urlencode "q=SELECT \"value\" FROM \"cpu_load_short\" WHERE \"region\"='us-west'"
                                                                    Authentication- disabled by default.
                                                                    truncates the number of rows returned to 10,000. max-row-limit configurable in http section of config file.
                                                                    chunked = true- answer returned in chunks.
                                            DCL
                                    exit
                            Queries
                                    CLI uses HTTP API port 8086

                            high availability thru relay
                            large amounts of timestamped data
                                    Eg- DevOps monitoring, application metrics, IoT sensor data, and real-time analytics.
                                    TSM engine allows for high ingest speed and data compression
                            Data maintenance
                                    Retention policies efficiently auto-expire stale data.
                        </Influx>
                    </Uncat>
                </NoSQL>
                <SemanticWeb>
                    Remaining- Notes from Semantic Web talks.
                    <VisionByTimBernersLee>
                    Time flies.
                    It's actually almost 20 years ago when I wanted to reframe the way we use information, the way we work together: I invented the World Wide Web.
                    Now, 20 years on, at TED, I want to ask your help in a new reframing.
                    So going back to 1989, I wrote a memo suggesting the global hypertext system. Nobody really did anything with it, pretty much.
                    But 18 months later -- this is how innovation happens -- 18 months later, my boss said I could do it on the side, as a sort of a play project, kick the tires of a new computer we'd got.
                    And so he gave me the time to code it up.
                    So I basically roughed out what HTML should look like: hypertext protocol, HTTP; the idea of URLs, these names for things which started with HTTP.
                    I wrote the code and put it out there. 
                    Why did I do it? with diversity of documents
                                    I was working as a software engineer in this huge, very exciting lab, lots of people coming from all over the world.
                                    They brought all sorts of different computers with them. They had all sorts of different data formats, all sorts, all kinds of documentation systems.
                                    So that, in all that diversity, if I wanted to figure out how to build something out of a bit of this and a bit of this, everything I looked into, I had to connect to some new machine,
                                    I had to learn to run some new program, I would find the information I wanted in some new data format. And these were all incompatible. It was just very frustrating.
                                    The frustration was all this unlocked potential. In fact, on all these discs there were documents. 
                            So if you just imagined them all being part of some big, virtual documentation system in the sky, say on the Internet, then life would be so much easier.
                    Well, once you've had an idea like that it kind of gets under your skin and even if people don't read your memo -- actually he did, it was found after he died, his copy.
                            He had written, "Vague, but exciting," in pencil, in the corner.
                            But in general it was difficult -- it was really difficult to explain what the web was like.
                            It's difficult to explain to people now that it was difficult then.
                                    But then -- OK, when TED started, there was no web so things like "click" didn't have the same meaning.
                                    I can show somebody a piece of hypertext, a page which has got links, and we click on the link and bing -- there'll be another hypertext page.
                                    Not impressive. you know, we've seen that -- we've got things on hypertext on CD-ROMs.
                                    What was difficult was to get them to imagine: so, imagine that that link could have gone to virtually any document you could imagine.
                                    Alright, that is the leap that was very difficult for people to make. Well, some people did.
                                    So yeah, it was difficult to explain, but there was a grassroots movement. And that is what has made it most fun.
                                    That has been the most exciting thing, not the technology, not the things people have done with it,
                                            but actually the community, the spirit of all these people getting together, sending the emails. That's what it was like then.
                    Do you know what? It's funny, but right now it's kind of like that again.
                            I asked everybody, more or less, to put their documents --
                            I said, "Could you put your documents on this web thing?" And you did. Thanks. It's been a blast, hasn't it?
                            I mean, it has been quite interesting because we've found out that the things that happen with the web really sort of blow us away. They're much more than we'd originally imagined
                            when we put together the little, initial website that we started off with. Now, I want you to put your data on the web.

                            Turns out that there is still huge unlocked potential. There is still a huge frustration that people have because we haven't got data on the web as data.
                    What do you mean, "data"? What's the difference -- documents, data?
                            Well, documents you read, OK? More or less, you read them, you can follow links from them, and that's it.
                            Data -- you can do all kinds of stuff with a computer. 
                            Who was here or has otherwise seen Hans Rosling's talk?
                                    One of the great -- yes a lot of people have seen it -- one of the great TED Talks.
                                    Hans put up this presentation in which he showed, for various different countries, in various different colors -- he showed income levels on one axis
                                    and he showed infant mortality, and he shot this thing animated through time. 
                                    So, he'd taken this data and made a presentation which just shattered a lot of myths that people had about the economics in the developing world.
                                    He put up a slide a little bit like this. It had underground all the data. 
                                    OK, data is brown and boxy and boring, and that's how we think of it, isn't it?
                                            Because data you can't naturally use by itself
                                            But in fact, data drives a huge amount of what happens in our lives
                                            and it happens because somebody takes that data and does something with it.
                            In this case, Hans had put the data together
                                    he had found from all kinds of United Nations websites and things.
                                    He had put it together, combined it into something more interesting than the original pieces and 
                                    then he'd put it into this software, which I think his son developed, originally, and produces this wonderful presentation.
                                            And Hans- really important to have a lot of data.
                    think about a world where everybody has put data on the web
                            virtually everything you can imagine is on the web
                            and then calling that linked data.
                    If you want to put something on the web there are three rules:
                            first thing is that those HTTP names --
                                    using them not just for documents now,
                                            things that the documents are about.
                                    using them for people, for places,
                                    for your products, events.
                                    All kinds of conceptual things, they have names now that start with HTTP.
                    Second rule, if I take one of these HTTP names and I look it up
                            do the web thing with it and I fetch the data using the HTTP protocol from the web,
                            I will get back some data in a standard format- useful data that somebody might like to know about that thing, about that event.
                            Who's at the event? Whatever it is about that person, where they were born, things like that.
                            So the second rule is I get important information back.
                    Third rule is that when I get back that information it's not just got somebody's height and weight and when they were born,
                            it's got relationships. Data is relationships.
                                    This person was born in Berlin; Berlin is in Germany.
                            And when it has relationships, whenever it expresses a relationship
                                    then the other thing that it's related to
                                    is given one of those names that starts HTTP.
                                    So, I can go ahead and look that thing up.
                                    Eg- So I look up a person -- I can look up then the city where they were born; then I can look up the region it's in, and the town it's in, and the population of it, and so on.
                                    So I can browse this stuff.
                    So that's it, really.
                    That is linked data.

                    wrote an article entitled "Linked Data" a couple of years ago
                            soon after that, things started to happen.
                            The idea of linked data is that we get lots and lots and lots of these boxes that Hans had,
                            and we get lots and lots and lots of things sprouting.
                                    It's not just a whole lot of other plants.
                                    It's not just a root supplying a plant,
                                    look at all the data, get it connected together, the more powerful it is.
                    The meme went out there.
                            pretty soon Chris Bizer at the Freie Universitat in Berlin
                                    who was one of the first people to put interesting things up,
                                    he noticed that Wikipedia, the online encyclopedia
                                            with lots and lots of interesting documents in it.
                                            Well, in those documents, there are little squares, little boxes- INFO BOXES there's data.
                                            He wrote a program to extract it from Wikipedia, and put it into a blob of linked data on the web, which he called dbpedia.
                            Dbpedia is represented by the blue blob in the middle of this slide
                                    and if you actually go and look up Berlin,
                                    you'll find that there are other blobs of data
                                    which also have stuff about Berlin, and they're linked together.
                                            So if you pull the data from dbpedia about Berlin, you'll end up pulling up these other things as well.
                    And the exciting thing is it's starting to grow.
                    This is just the grassroots stuff again, OK?
                    Data in different forms- thinnk of web diversity- that the web allows you to put all kinds of data up there.
                            all kinds of data- mention a few of them
                                    talk about government data, enterprise data is really important,
                                    there's scientific data, there's personal data,
                                    there's weather data, there's data about events,
                                    there's data about talks, and there's news and there's all kinds of stuff.			
                            so that you get the idea of the diversity of it,
                                    so that you also see how much unlocked potential.
                                    Let's start with government data.
                                            Barack Obama said in a speech, that he -- American government data would be available on the Internet in accessible formats.
                                            And I hope that they will put it up as linked data. That's important. Why is it important?
                                                    Not just for transparency, yeah transparency in government is important,
                                                    but that data -- this is the data from all the government departments
                                                            Think about how much of that data is about how life is lived in America. It's actual useful. It's got value.
                                                            I can use it in my company.I could use it as a kid to do my homework.
                                                    So we're talking about making the place, making the world run better by making this data available.
                    In fact if you're responsible -- if you know about some data in a government department, often you find that these people, they're very tempted to keep it --
                            Hans calls it database hugging. You hug your database, you don't want to let it go until you've made a beautiful website for it.
                            Well, I'd like to suggest that rather -- yes, make a beautiful website, who am I to say don't make a beautiful website?
                                    Make a beautiful website, but first give us the unadulterated data, we want the data. We want unadulterated data.
                            OK, we have to ask for raw data now.And I'm going to ask you to practice that, OK?
                            no idea the number of excuses people come up with to hang onto their data and not give it to you, even though you've paid for it as a taxpayer.
                    enterprises as well.
                    Science
                            we are very conscious of the huge challenges that human society has right now --
                                    curing cancer, understanding the brain for Alzheimer's,
                                    understanding the economy to make it a little bit more stable,
                                    understanding how the world works.
                                    The people who are going to solve those -- the scientists -- 
                                            they have half-formed ideas in their head, they try to communicate those over the web.
                                            a lot of the state of knowledge of the human race at the moment is on databases, 
                                                    often sitting in their computers,
                                                    and actually, currently not shared.
                                    if you're looking at Alzheimer's, for example, drug discovery -- there is a whole lot of linked data which is just coming out because scientists in that field realize
                            this is a great way of getting out of those silos,
                                    because they had their genomics data in one database
                                    in one building, and they had their protein data in another.
                                    Now, they are sticking it onto -- linked data --
                                    and now they can ask the sort of question, that you probably wouldn't ask, I wouldn't ask -- they would.
                                            What proteins are involved in signal transduction
                                            and also related to pyramidal neurons?
                                            Well, you take that mouthful and you put it into Google.
                                    Of course, there's no page on the web which has answered that question
                                            because nobody has asked that question before.
                                            You get 223,000 hits --
                                            no results you can use.
                                            You ask the linked data -- which they've now put together --
                                    vs 32 hits, each of which is a protein which has those properties
                                            and you can look at.
                                            The power of being able to ask those questions, as a scientist --
                                            questions which actually bridge across different disciplines --
                                            is really a complete sea change.
                                            It's very very important.
                            Scientists are totally stymied at the moment --
                                    the power of the data that other scientists have collected is locked up
                                            and we need to get it unlocked so we can tackle those huge problems.
                    In fact, data is about our lives.
                            Now if I go on like this, you'll think that all the data comes from huge institutions and has nothing to do with you. But, that's not true.
                            You just -- you log on to your social networking site, your favorite one, you say, "This is my friend." Bing! Relationship. Data.
                            You say, "This photograph, it's about -- it depicts this person. " Bing! That's data. Data, data, data.
                            Every time you do things on the social networking site, the social networking site is taking data and using it -- re-purposing it --
                            and using it to make other people's lives more interesting on the site. But, when you go to another linked data site --
                    and let's say this is one about travel,
                            and you say, "I want to send this photo to all the people in that group," you can't get over the walls.
                            The Economist wrote an article about it, and lots of people have blogged about it -- tremendous frustration.
                            The way to break down the silos is to get inter-operability between social networking sites.
                    We need to do that with linked data. One last type of data I'll talk about, maybe it's the most exciting.
                            Before I came down here, I looked it up on OpenStreetMap
                            The OpenStreetMap's a map, but it's also a Wiki.
                            Zoom in and that square thing is a theater -- which we're in right now --
                            The Terrace Theater. It didn't have a name on it.
                            So I could go into edit mode, I could select the theater,
                            I could add down at the bottom the name, and I could save it back.
                            And now if you go back to the OpenStreetMap. org,
                            and you find this place, you will find that The Terrace Theater has got a name.
                            I did that. Me! I did that to the map. I just did that! I put that up on there. Hey, you know what?
                            If I -- that street map is all about everybody doing their bit and it creates an incredible resource because everybody else does theirs.
                            And that is what linked data is all about. It's about people doing their bit to produce a little bit, and it all connecting.
                                    That's how linked data works. You do your bit. Everybody else does theirs.
                                    You may not have lots of data which you have yourself to put on there but you know to demand it.
                                    And we've practiced that. So, linked data -- it's huge. I've only told you a very small number of things. 
                                            There are data in every aspect of our lives, every aspect of work and pleasure,
                                    and it's not just about the number of places where data comes,
                                            it's about connecting it together.
                                    And when you connect data together, you get power
                    in a way that doesn't happen just with the web, with documents.
                    You get this really huge power out of it.
                            So, we're at the stage now where we have to do this -- the people who think it's a great idea.
                            And all the people -- and I think there's a lot of people at TED who do things because --
                                    even though there's not an immediate return on the investment
                                    because it will only really pay off when everybody else has done it --
                                    they'll do it because they're the sort of person who just does things which would be good if everybody else did them.
                    OK, so it's called linked data.
                            I want you to make it.
                            I want you to demand it.
                            And I think it's an idea worth spreading.
                    Thanks.
                    (Applause)
                    </VisionByTimBernersLee>
                </SemanticWeb>
            </DataModelTypes>
            <BigData>
                <Concept>
                    Data world- how to store and process- beyond storage and processing power.
                    sensors, cc-cams, FB, online shoppings, airlines, NCDC, hospitality data.
                    90% data in 2 years.
                    1990s- 1 GB to 20 GB and 28 MB RAM, 10Kbps reading speed
                            now, 1 TB, 16 GB RAM, 100 Mbps.
                            keep purchasing new HDD- better data center and get servers.
                            ultimately, keep at some other place- data center- @titanic movie not see now then kept there (st i may)
                            what is better sending 80Kb program to data center or 10 TB to my machine? program but should not do it still. why?
                                    b4 hadoop, computation is processor bound.
                    Farmer has small field- rice packets 10, 20, 30- room coverage getting bigger.
                            must store in some other place afterwards- godown- not throw.
                    IBM said defn of big data- cause for hadoop
                    1. Volume (GB, TB, PB)
                    2. Velocity- sending data to data center and fetching it
                    3. Variety- structured, unstructured, semi-structured. FB- vids, images, text, audio.
                            semi-structured- like log files- FB when we log in- log file. 4 accounts of mine 5 times- 20 log files.
                    600 GB is big data for 500 GB of HDD. OR 300 GB but huge time it will take
                            12 months by 1 guy to construct my house. split the job and get it faster.
                            parallelly systems work and give output.
                    processing speed- i have to sign files but by the time i do 50 files, i get 100 more- better get 4 people- but fast ppl.
                            we grew in data generation buty not in processing speed so hadoop.
                            data is growing and processing speed also- it should not seem lesser- equalize them.
                            best solution is hadoop.

                    B4 we start we need a proper storage capacity to process it better.
                            hadoop knows huge data and how to do it in less time.
                            we are aware of google search engine- top 10 best searches- huge data
                            in 1990- how to store huge data? it took them 13 years- 
                                    in 2003, GFS how to store big data and 
                                    2004- map reduce they gave how to process big data.
                                    both given on paper. suggestion and technique.
                            Later yahoo- search engine. also gives results.
                                    how where store dta and retrieve- HDFS and map reduce.
                                    Idea taken from google.
                                    who is inventor of hadoop- doug cutting. with elephant logo (big animal? his kid used to call elephant as hadoop)
                </Concept>
                <Hadoop>
                Hadoop is an open-source framework to store and process Big Data in a distributed environment
                Hadoop- core concept/ components= storage + Processing = HDFS + Mapreduce
                vs Cloud computing
                    Distributed computing involves dividing a large problem into smaller slices and having multiple networked computers process the slices. Cloud computing usually refers to providing a service via the internet.
                    Hadoop itself is based on the methodology of Distributed computing. It's like asking difference between programming and java.
                <Components>
                        <HDFS>
                        HDFS- Hadoop distributed file system
                        Defn- HDFS:Hadoop Distributed File System is a part of Hadoop framework, used to store and process the datasets. It provides a fault-tolerant file system to run on commodity hardware.
                            <Concept>
                                Hadoop is OSS overseen by apache software foundation- for storing and processing huge datasets with commodity softwares.
                                Framework like struts, hibernate.
                                Hadoop is OSS overseen by apache software foundation.
                                        Indu prefer OSS- free.
                                        used for huge data sets but not small data sets- 10 GB in 20 mins on my machine.
                                        Distribute to 10 systems and parallelly- distribution and combine- 40 mins also so go for it when big data.
                                It is specially designed file system for storing huge data sets with cluster of commodity with streaming access patterns.
                                        Commodity hardware- Cheap hardware. PC, laptops, 40K Rs
                                                but 1 TB of disk with no tolerance for failure it costs in Lakhs.
                                                Hadoop doesn't need high reliable hardware- 4-5 crore Rs for 500 TB- 1 crore if not reliable.
                                                Companies want to invest less and profit more.
                                                Failure of system- then loss of data- but can be overcome.
                                        Cluster- Set of machines in a single LAN.
                                        Streaming access pattern- @slogan in java- write once, run anywhere (platform, OS) 
                                                slogan- write once, read any times, but don't change content of file once you are keeping content on hdfs.
                                        File system- way of storing files and directories- where in HDD- can we say HDD is file system? when given OS it gets capabilities of file system support.
                                                Hard disks- 500 GB HDD- blocks (4KB of blocks) chunks- in which i store file of size only 2KB- it will be wasted.
                                                on top of HDD, we install hadoop or hdfs- it is given block size of 64 MB by default.
                                                        or 128 if huge file size- we are developers not administrators.
                                                        many 4KB of blocks are combined to make 64MB of block.
                                                        now storing file of size 35 MB- 29 MB space is free now- released for another file or not?
                                                        why going for big size- cuz huge data sets not small ones.
                                                        Here it releases block space for another file unlike normal. Imagine wasting for 1 KB file- many systems.
                                                        if huge data in small data sets- it takes more metadata- later.
                                we use yahoo's hdfs with modification.
                                5 services for master + slave- masters can talk to each other and so can slaves
                                        master daemons/nodes
                                                services 
                                                        name node- it corresponding slave node is data node. can talk to each other not others.
                                                        secondary name node
                                                        job tracker- it's correspnding slave node is task tracker. can talk to each other nt others.
                                                means- 
                                                        demons means background process
                                        Slave nodes
                                                services
                                                        data node
                                                        task tracker
                                Client has mission of store and process some data.
                                        proper storage capacity but more time- so hadoop he chooses- parallel work.
                                        say 200 MB data for ease of understanding.
                                        so 4 64 MB blocks to be used of hdfs.
                                        give 4 splits names for ease of understanding- a.txt, b.txt, c.txt, d.txt (3*64+8)
                                        client contacts name node as he doesn't know empty data nodes in cluster being external.
                                        @Huge rice growth- increase home area for them- but too much then godown is approached.
                                                uses a lorry to get rice packets there- he doesn't know space inside godown so contacts manager for space.
                                                white paper and fills a form
                                                Manager knows and allots- 50-30Rs quality packets, 50-40Rs packets, 100-50Rs packets.
                                                without agreement of room numbers- there can be fight. so allot room 1, 3 and 7- money for rooms.
                                        clients requests to name node- give me data nodes to keep my huge files
                                                name node takes care of metadata like manager had taken for rice.
                                                Eg- file.txt split into a.txt, b.txt, c.txt, d.txt (aka input splits)- please go and store your data nodes in 1, 3, 5 and 7
                                                client approaches nodes 1, 3, etc
                                                        every node in cluster has capacity of 250GB, 500GB.
                                                        if one node is down then same problem as in traditional distributed file systems
                                                        to overcome, 3 replications are given by default- now 200MB data has 600 data including back up.
                                                                where replications?
                                                                say a replica is sent from 1 to system 2 (to overcome data loss problem), and 2 to 4.
                                                        ACK reverse 4 to 2 to 1 to client to ensure that replications are same.
                                                name node knows where a.txt is stored (1,2,4)
                                                        all data nodes are slave nodes to name node.
                                                        data nodes send reports to name node- block (block is being allotted) and heartbeat (we are alive)
                                                        similarly, b.txt is given 3,5,8; c.txt- 5, 6, 7 and d.txt- 7,9,10.
                                                        if heart beat not given in time- data node may be dead- say data node 1 is dead
                                                                no heart beat- remove metadata from HDD- copy of a.txt is given to another system say 7.
                                                                and so on- when system 1 is active again- it will start with fresh data.
                                        If paper is lost, farmer and managers will fight- so with metadata
                                                even if hadoop is running- no use- not accessible.
                                                name node is single point of failure
                                                        name node should be maintained with high reliable hardware.
                                                1 lakh packets of rice but small size reqd for paper- so with name node.
                                                so metadata doesn't take lotta HDD space.
                                                why size of block was increased to reduce the size of metadata for name node.
                                                500GB data- 7500 blocks vs 1 crore blocks- lesser metadata reqd.
                                                500TB data- make 500MB of block size and store metadata
                            </Concept>
                            <HadoopDeployment>
                                <Experience>
                                    It wasn't working on /usr/local- so used home
                                    Install node and nps
                                            Install gcc, make, etc
                                                    yum install -y gcc-c++ make
                                    could not create directory- can't open file or directory-
                                            chown the hadoop directory to the user.
                                            chmod 777 for the directory.
                                    Changing name node to 8020 in core-site- allow -put of hadoop.
                                    1.x- all installed and working fine.
                                    2.x- All installed except Resource Manager
                                            so MR job complains while making a connection to it.
                                </Experience>
                                <ContextTerms>
                                        These days companies collect massive amounts of data, hundreds and thousands of terabytes
                                        Analyzing terabytes of data on one computer is practically impossible
                                        One simple analysis job could take hours
                                        A different approach is to spread the data over a cluster of computers and analyze it in parallel with Hadoop and Map Reduce
                                        In this first lesson, you'll deploy a simple hadoop cluster
                                        You wouldn't actually use this cluster in a real production setting, but it will help you understand the Hadoop architecture
                                        In later lessons, you'll use automated methods to deploy larger clusters, like you'd use in production
                                        To complete this course, you'll need a basic understanding of Hadoop and HDFS, and be able to write MapReduce programs
                                        If you don't know about these, take our excellent Intro to Hadoop course, then return here
                                        Also you'll need to use Shell commands such as SSH and no terminal editor such as vim or nano
                                        You can learn about these in our command line course
                                        Okay, let's get started
                                        I am assuming you already know about Hadoop a bit
                                        So, here's just a brief summary
                                Uncat
                                        Necessitates java
                                                cuz Hadoop runs on Java
                                                OpenJDK- open source version of Java
                                        Environment Variables
                                                These will help applications locate Java and Hadoop
                                        On 1 machine (Pseudo-Distributed Mode)
                                                You run all the daemons on one machine
                                Terms
                                Cluster- a set of communicating nodes
                                        nodes take on different roles for parallel execution- name node,A secondary name node, resource manager and one or more data nodes
                                        node type (role) is determined by the Hadoop daemon running on the machine
                                Daemons
                                        just programs that run in the background (not foreground)
                                        handle all sorts of fun stuff like, communication across a cluster
                                        track jobs and resources and run MapReduce code
                                Data Node
                                        When they load data under the cluster, the files are split by name node into blocks, then typically replicated across the data nodes
                                        So if a data node dies
                                                your data is still around somewhere
                                MapReduce and other tools, 
                                        run in parallel on the data nodes
                                        This increases the effective computational speed
                                The NameNode stores metadata about the blocks in a file called .fsimage
                                        Metadata is data about the data
                                        which block is stored on which data node
                                        @address book for the blocks
                                secondary name node 
                                        logs edits to the file system (not actual FS image)
                                                changes to the FS image
                                        used to update the file on the name node
                                        fairly, poorly named though cuz name node dies, metadata is all gone.
                                Resource Manager
                                        YARN for yet another resource negotiator
                                        allocates resources like CPU and memory to applications running on the cluster
                                </ContextTerms>
                                <Install>
                                        java -version
                                                jdk-7u71-linux-x64.tar.gz will be downloaded
                                                tar zxf jdk-7u71-linux-x64.gz
                                                mv jdk1.7.0_71 /usr/local/- to make it available to all users
                                                setting up PATH and JAVA_HOME- echo $JAVA_HOME
                                                add to ~/.bashrc file-
                                                        export JAVA_HOME=/usr/local/jdk1.7.0_71
                                                        export PATH=$PATH:$JAVA_HOME/bin (prefer the other way round)
                                                apply changes- source ~/.bashrc					
                                                configure java alternatives
                                                        # alternatives --install /usr/bin/java java /usr/local/java/bin/java 2
                                                                which update-alternatives
                                                        # alternatives --install /usr/bin/javac javac /usr/local/java/bin/javac 2
                                                        # alternatives --install /usr/bin/jar jar /usr/local/java/bin/jar 2
                                                        # alternatives --set java /usr/local/java/bin/java
                                                        # alternatives --set javac /usr/local/java/bin/javac
                                                        # alternatives --set jar /usr/local/java/bin/jar
                                        hadoop -version
                                                Download
                                                        # cd /usr/local
                                                        # wget http://apache.claz.org/hadoop/common/hadoop-2.4.1/ hadoop-2.4.1.tar.gz
                                                                //2.6.5 was a stable release
                                                        # tar xzf hadoop-2.4.1.tar.gz
                                                        # mv hadoop-2.4.1/* to hadoop/				(getting rid of the number)
                                                        # exit
                                                Installing Hadoop in Pseudo Distributed Mode
                                                        Setting up Hadoop- echo $HADOOP_HOME
                                                                append to ~/.bashrc
                                                                        export HADOOP_HOME=/usr/local/hadoop 
                                                                        export HADOOP_MAPRED_HOME=$HADOOP_HOME 
                                                                        export HADOOP_COMMON_HOME=$HADOOP_HOME 
                                                                        export HADOOP_HDFS_HOME=$HADOOP_HOME 
                                                                        export YARN_HOME=$HADOOP_HOME
                                                                        export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export
                                                                        PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
                                                                source ~/.bashrc
                                                        Hadoop Configuration
                                                                config files location- $HADOOP_HOME/etc/hadoop (conf for 1.x)
                                                                        hadoop-env.sh- 
                                                                                Resp- set java envt- export JAVA_HOME=/usr/local/jdk1.7.0_71
                                                                                        By default the JAVA_HOME setting line is commented
                                                                                hadoop-classpath is optional so let go
                                                                                without this configured- only NN,JT will start and error on start-all.sh that java_home is not set
                                                                        core-site.xml- 
                                                                                Resp- RPC port for NN.
                                                                                information such as the port number used for Hadoop instance, memory allocated for the file system, memory limit for storing the data, and the size of Read/Write buffers.
                                                                                add property
                                                                                <configuration>
                                                                                        ...
                                                                                   <property> 
                                                                                          <name>fs.default.name</name>			//OR fs.defaultFS
                                                                                          <value>hdfs://localhost:9000</value>	//RPC port for Name node; it can change also to like 8020
                                                                                   </property>
                                                                                   <property>
                                                                                        <name>hadoop.tmp.dir</name>
                                                                                        <value>/home/hadoop/work/hadoopdata/tmp</value>
                                                                                   </property>	//for storage of namenode
                                                                                   ...
                                                                                </configuration>
                                                                        hdfs-site.xml- 
                                                                                Resp- Replications and Tempdata
                                                                                information such as the value of replication data, the namenode path, and the datanode path of your local file systems. It means the place where you want to store the Hadoop infra
                                                                                for following- consider "hadoop" is the ***username***
                                                                                hadoopinfra/hdfs/datanode is the directory created by hdfs file system- (confirm- we don't need to create them)
                                                                                <configuration>
                                                                                        ...
                                                                                   <property> 
                                                                                          <name>dfs.replication</name> 
                                                                                          <value>1</value> 	//can go from 1 to 512- 3 is default
                                                                                   </property> 
                                                                                   <property> 
                                                                                          <name>dfs.name.dir</name> 
                                                                                          <value>/home/hadoop/work/hadoopdata/dfs/name</value> 
                                                                                   </property> 
                                                                                   <property> 
                                                                                          <name>dfs.data.dir</name>
                                                                                          <value>/home/hadoop/work/hadoopdata/dfs/data</value> 	//or file:///home/...
                                                                                   </property>
                                                                                   ...
                                                                                </configuration>
                                                                        mapred-site.xml
                                                                                Resp for 2.x- yarn related values.
                                                                                specify which MapReduce framework we are using
                                                                                cp mapred-site.xml.template mapred-site.xml
                                                                                <configuration>
                                                                                        ...
                                                                                   <property>
                                                                                          <name>mapreduce.framework.name</name>
                                                                                          <value>yarn</value>
                                                                                   </property>
                                                                                   vs
                                                                                   1.x
                                                                                   <property>
                                                                                          <name>mapred.job.tracker</name>
                                                                                          <value>localhost:9001</value>		//9001 is RPC port for JT- communication between JT,TT	//vs core-site.xml which told hdfs://localhost- it's syntax- no protocol is reqd
                                                                                   </property>
                                                                                   <property>
                                                                                          <name>mapred.local.dir</name>
                                                                                          <value>/home/hadoop/work/hadoopdata/mapred/local</value>			//where /home/hadoop is home directory- hadoop being username
                                                                                   </property>
                                                                                   <property>
                                                                                          <name>mapred.system.dir</name>
                                                                                          <value>/mapred/system</value>
                                                                                   </property>
                                                                                        ...
                                                                                </configuration>
                                                                        Masters- localhost- no change
                                                                        slaves- localhost
                                                                        for 2.x more files
                                                                                yarn-site.xml
                                                                                        resp- set resource manager and node manager.
                                                                                        used to configure yarn into Hadoop
                                                                                        <configuration>
                                                                                                ...
                                                                                           <property> 
                                                                                                  <name>yarn.nodemanager.aux-services</name> 
                                                                                                  <value>mapreduce_shuffle</value> 
                                                                                           </property>
                                                                                           also, try
                                                                                           <property>
                                                                                                <name>yarn.resourcemanager.hostname</name>
                                                                                                <value>localhost</value>
                                                                                                </property>
                                                                                                <property>
                                                                                                <name>yarn.nodemanager.aux.services.mapreduce_shuffle.class</name>
                                                                                                <value>org.apache.hadoop.mapred.ShuffleHandler</value>
                                                                                                </property>...
                                                                                           ...
                                                                                        </configuration>
                                                                                yarn-env.sh- setting JAVA_HOME here.
                                                                                mapred-env.sh
                                                                                        set the JAVA_HOME (>=7 (7 is very compatible))
                                                        Verifying Hadoop Installation
                                                                Name Node Setup
                                                                        $ cd ~
                                                                        $ hdfs namenode -format
                                                                                if you don't format you don't get namenode displayed for fresh installation.
                                                                                show hostname; starting and shutting.
                                                                Start Hadoop
                                                                        start-all.sh
                                                                                encompasses following
                                                                                $ start-dfs.sh
                                                                                        Verifying Hadoop dfs- starts NN,SNN,DN
                                                                                $ start-yarn.sh
                                                                                        start-mapred.sh for 1.x
                                                                                        Verifying Yarn Script- starts JT,TT
                                                                                starting namenode, logging to; etc
                                                                                you can see directories formed- check where .fsimage is, etc
                                                                        stop-all.sh
                                                                                stopping..
                                                                        jps
                                                                                you will see following
                                                                                        NN,TT,SNN,JPS,JT,DN (order not fixed)
                                                                                        for 2.x- JT, TT replaced by Resource Manager and node Manager
                                                                Verify
                                                                        85% linux commands are supported in hadoop
                                                                        WebUI
                                                                                Accessing Hadoop on Browser
                                                                                        http://localhost:50070/ on browser
                                                                                                /logs
                                                                                                /dfshealth.jsp
                                                                                                :50075 also for traversal
                                                                                                /user/(username) is home directory.
                                                                                Verify all applications for cluster
                                                                                        http://localhost:8088/ on browser
                                </Install>
                                <AWS>
                                <EC2>
                                on Amazon EC2 (Elastic Compute Cloud)
                                =========== 
                                What
                                deploy a Hadoop cluster on Amazon Web Service (AWS) EC2 virtual machines
                                        free offering
                                        costly but for organization needs (store and analyse reqt)
                                Why
                                EC2 for commodity hardware in a data center but few of have server racks/(bunch of hardware)
                                request and run from anywhere in the world at fixed cost (hourly, per machine, per specification (CPU,RAM,HDD))
                                How
                                Create Account
                                        If you dont have an AWS account, youll need to create one
                                                It might ask you for your credit card information
                                                This is because the instances are charged by the hour
                                                EC2 has a free offering but Ambari version and EMR will cost money
                                EC2
                                        choose a region that is closest to you from navigation bar.
                                        create instances- 
                                                what- Were going to install Hadoop on this instance and use it as a generic Hadoop node
                                                services/EC2/Launch Instance/Ubuntu 64-bit/t2.micro/
                                                "next:configure"- instance=1
                                                "Next: Add Storage"- 8 GB
                                                        Suppose you're using instances with 300 GB of storage
                                                        If each block is replicated 3 times, calculate the smallest number of data nodes you need to store a 1 TB dataset
                                                        Also add extra space reqd (not just arithmetic)
                                                "Next: Tag Instance"- name=hadoop
                                                        Tagname value can be anything- like hadoop
                                                "Next:Security Group"
                                                        The security group- who can connect to your instances
                                                        default is SSH and 0.0.0.0/0 => any PC can connect over ssh (easy to not change this)
                                                        Better, restricted list of IP addresses that can access your cluster
                                                change the type to All Traffic
                                                        All Traffic => access the instances over HTTP as well as SSH
                                                Review and Launch
                                                prompted to create and download a private key
                                                        private key is reqd for ssh connection over rsa
                                                        lost it => can't access so if lost shut down the instances and start up new ones
                                                Note- Public DNS (hostnames)
                                                        Dashboard View Instances- see booting up in EC2 dashboard
                                Access the Instance 
                                        ssh- logging
                                                tool
                                                        Windows- use moba, git bash, cygwin/mingw, etc
                                                        MAC/LINUX- available in terminal.
                                                $ sudo chmod 600 /path/to/key_file
                                                $ ssh -i /path/to/key_file.pem (user)@(instance_hostname)
                                                        pem is the private key
                                        scp
                                                $ scp -i /path/to/key_file
                                                gui- advanced settings
                                        open ports like 50070

                                Instance Installation and Making Image
                                        $ sudo apt-get update && sudo apt-get dist-upgrade
                                                good practice- optional
                                        java -version
                                                Removal
                                                        sudo apt-get autoremove openjdk-7-jre
                                                                cleans up all the additional dependency libraries that were installed with it.
                                                        sudo apt-get purge openjdk*
                                                                additional cleanup
                                                $ sudo apt-get install openjdk-7-jdk or 8.
                                                readlink -f $(which java)
                                        hadoop -version
                                                $ wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz -P ~/Downloads
                                                        for 1.x- 1.2.0 is good
                                                $ sudo tar zxvf ~/Downloads/hadoop-* -C /usr/local
                                                $ sudo mv /usr/local/hadoop-* /usr/local/hadoop 
                                        Environment Variables- append to ~/.bashrc
                                                get java path- using find / -name "jre"
                                                export JAVA_HOME=/path/to/java 
                                                export PATH=$PATH:$JAVA_HOME/bin 
                                                export HADOOP_HOME=/usr/local/hadoop 
                                                export PATH=$PATH:$HADOOP_HOME/bin 
                                                export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop 
                                        Load variables
                                                $ source ~/.bashrc
                                        Make Image and Launch
                                                with all this installed, your instance is ready to be used as a node in the hoodoo cluster
                                                You can easily save it and launch multiple copies by creating an amazon machine image usually just called image
                                                instances launched from image already have all the software installed
                                                Instance with java, hadoop installed to be used as image for more nodes
                                                Select the instance, then actions, image, create image and give a name- in minutes check in sidbar/AMI
                                                select image/launch/4 instances (1N+3D)/config as b4 (security="all traffic")/key- (createdKey)
                                                        Instances panel- shows all 4 nodes
                                                Note hostnames of each instance needed often
                                        Convenient Logging
                                                create an SSH config file
                                                        $ touch ~/.ssh/config
                                                        $ vi ssh/config- Add these lines to the file
                                                                Host namenode
                                                                  HostName namenode_public_hostname
                                                                  User ubuntu
                                                                  IdentityFile ~/.ssh/key_file.pem

                                                                Host datanode1
                                                                  HostName datanode1_public_hostname
                                                                  User ubuntu
                                                                  IdentityFile ~/.ssh/key_file.pem

                                                                Host datanode2
                                                                  HostName datanode2_public_hostname
                                                                  User ubuntu
                                                                  IdentityFile ~/.ssh/key_file.pem

                                                                Host datanode3
                                                                  HostName datanode3_public_hostname
                                                                  User ubuntu
                                                                  IdentityFile ~/.ssh/key_file.pem
                                                        This file lets SSH associate a shorthand name with a hostname, a user, and the private key, so you don't have to type those in each time
                                                        assuming your private key key_file.pem is in .ssh
                                                        log into the NameNode with just 
                                                $ ssh namenode
                                                copy the config file to the NameNode: 
                                                        $ scp ~/.ssh/config namenode:~/.ssh 
                                                        You need to make sure the NameNode can connect to each DataNode over ssh without needing a password
                                        Seamless nodes connection in cluster
                                                Youll do this by creating a public key for the NameNode and adding it to each DataNode
                                                Generate Key
                                                        $ ssh namenode create a public key 
                                                        $ ssh-keygen -f ~/.ssh/id_rsa -t rsa -P "" 
                                                        $ cat ~/.ssh/id_rsa
                                                copy it into authorized_keys of all slaves
                                                        to enable passwordless login
                                                        pub >> ~/.ssh/authorized_keys 
                                                        On the NameNode: 
                                                                $ ssh datanode1 'cat >> ~/.ssh/authorized_keys' < ~/.ssh/id_rsa.pub
                                                                $ ssh datanode2 'cat >> ~/.ssh/authorized_keys' < ~/.ssh/id_rsa.pub 
                                                                $ ssh datanode3 'cat >> ~/.ssh/authorized_keys' < ~/.ssh/id_rsa.pub 
                                                Test this by logging into a DataNode from the NameNode
                                                        $ ssh datanode1 
                                                Back to the NameNode 
                                                        $ exit 

                                ====== 
                                Table of Contents
                                        Configuring the cluster
                                        Hadoop environment variables
                                        Cluster-wide configuration
                                        NameNode Specific Configuration
                                        DataNode specific configuration
                                        Configuring the cluster
                                public hostname of each instance - keep them handy- view by selecting all the instances
                                Hadoop has an intimidating number of configuration options- check the Hadoop documentation and Hadoop: The Definitive Guide for details
                                hadoop-env.sh- it holds environment variables used by Hadoop
                                        change export JAVA_HOME=/path/to/java 
                                Cluster-wide configuration
                                        First, youll deal with the configuration on each node, then get into specific configurations for the NameNode and DataNodes
                                        On each node, 
                                                Configuration Folder- go to the Hadoop configuration folder, you should be able to get there with 
                                                        $ cd $HADOOP_CONF_DIR since we set that in .bashrc earlier
                                                        editing needs root access 
                                                        $ sudo
                                                In the configuration folder, edit core-site.xml: 
                                                        <configuration>
                                                          <property>
                                                                <name>fs.defaultFS</name>
                                                                <value>hdfs://(namenode_public_hostname):9000</value>
                                                          </property>
                                                        </configuration>
                                                        Notes
                                                                namenode_public_hostname is the public hostname of your NameNode
                                                                configuration fs.defaultFS tells the cluster nodes which machine the NameNode is on and that it will communicate on port 9000
                                                in yarn-site.xml, set options related to YARN, the resource manager: 
                                                        <configuration>
                                                        <! Site specific YARN configuration properties -->
                                                          <property>
                                                                <name>yarn.nodemanager.aux-services</name>
                                                                <value>mapreduce_shuffle</value>
                                                          </property>
                                                          <property>
                                                                <name>yarn.resourcemanager.hostname</name>
                                                                <value>(namenode_public_hostname)</value>
                                                          </property>
                                                        </configuration>
                                                        Notes
                                                                Similarly with fs.defaultFS, yarn.resourcemanager.hostname sets the machine that the resource manager runs on
                                                copy mapred-site.xml from mapred-site.xml.template 
                                                        $ sudo cp mapred-site.xml.template mapred-site.xml 
                                                        <configuration>
                                                          <property>
                                                                <name>mapreduce.jobtracker.address</name>
                                                                <value>(namenode_public_hostname):54311</value>
                                                          </property>
                                                          <property>
                                                                <name>mapreduce.framework.name</name>
                                                                <value>yarn</value>
                                                          </property>
                                                        </configuration>
                                                        Notes
                                                                Again, mapreduce.jobtracker.address sets the machine the job tracker runs on, and the port it communicates with
                                                                The other option here mapreduce.framework.name sets MapReduce to run on YARN
                                        NameNode specific configuration
                                                Add the DataNode hostnames to /etc/hosts
                                                        get the hostname for each DataNode by entering $ hostname, or $ echo $(hostname) on each DataNode
                                                        Now edit /etc/hosts and include these lines:
                                                                127.0.0.1 localhost
                                                                (namenode_public_hostname) namenode_hostname
                                                                (datanode1_public_hostname) datanode1_hostname
                                                                (datanode2_public_hostname) datanode2_hostname
                                                                (datanode3_public_hostname) datanode3_hostname
                                                edit hdfs-site.xml: 
                                                        <configuration>
                                                          <property>
                                                                <name>dfs.replication</name>
                                                                <value>3</value>
                                                          </property>
                                                          <property>
                                                                <name>dfs.namenode.name.dir</name>
                                                                <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
                                                          </property>
                                                        </configuration>
                                                Notes
                                                        dfs.replication sets how many times each data block is replicated across the cluster
                                                        dfs.namenode.name.dir sets the directory for storing NameNode data (.fsimage)
                                                create the specified directory to store the data: 
                                                        $ sudo mkdir -p $HADOOP_HOME/data/hdfs/namenode 
                                                create the masters file in HADOOP_CONF_DIR
                                                        The masters file sets which machine the secondary namenode runs on
                                                        in our case we'll have the secondary NameNode run on the same machine as the NameNode, 
                                                                so edit masters, add the hostname of NameNode (Note: Not the public hostname, but the hostname you get from $ hostname)
                                                        Typically though, you would have the secondary NameNode run on a different machine than the primary NameNode
                                                Next, edit the slaves file in HADOOP_CONF_DIR, this file sets the machines that are DataNodes
                                                In slaves, add the hostnames of each datanode (Note: Again, not the public hostname, but $ hostname hostnames)
                                                        The slaves file might already contain a line localhost, you should remove it, otherwise the NameNode would run as a DataNode too
                                                        It should look like this:
                                                                datanode1_hostname
                                                                datanode2_hostname
                                                                datanode3_hostname
                                                change the owner of HADOOP_HOME to ubuntu: 
                                                        $ sudo chown -R ubuntu $HADOOP_HOME
                                        DataNode specific configuration
                                                edit HADOOP_CONF_DIR/hdfs-site.xml: 
                                                <configuration>
                                                  <property>
                                                        <name>dfs.replication</name>
                                                        <value>3</value>
                                                  </property>
                                                  <property>
                                                        <name>dfs.datanode.data.dir</name>
                                                        <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
                                                  </property>
                                                </configuration>
                                                Again, this sets the directory where the data is stored on the DataNodes
                                                create the directory on each DataNode: 
                                                        $ sudo mkdir -p $HADOOP_HOME/data/hdfs/datanode 
                                                change the owner of the Hadoop directory 
                                                        $ sudo chown -R ubuntu $HADOOP_HOME 

                                Launch Hadoop Cluster 
                                        On the NameNode, 
                                                format the file system
                                                        $ hdfs namenode -format 
                                                start HDFS
                                                        $ $HADOOP_HOME/sbin/start-dfs.sh
                                                Start YARN:
                                                        $ $HADOOP_HOME/sbin/start-yarn.sh
                                                Start the job history server: 
                                                        $ $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver
                                                To see the Java processes (Hadoop daemons for instance), enter 
                                                        $ jps 
                                                                run it on both master and slaves to see daemons running on machines.	
                                Test And Benchmark
                                        Create a home directory on HDFS
                                                $ hdfs dfs -mkdir /user
                                                $ hdfs dfs -mkdir /user/ubuntu
                                        Create random data for the Terasort example
                                                Teragen writes rows of random data to a file
                                                Each row is 100 bytes long and we have 500,000 rows. So 50 megabytes in total * 3 replication = 150 MB across 3 DN.
                                                run terasort on the data and sort the data and write it to a file sorted data.
                                                $ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar teragen 500000 random-data
                                                        we may want for TB but our storage capacity is ltd
                                        Sort random data,
                                                $ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar terasort random-data sorted-data
                                        Write files to disk with TestDFSIO:
                                                $ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 5MB
                                        Read files from disk with TestDFSIO:
                                                $ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 5MB
                                Using
                                        check that your nodes are running namenodeHostName:50070 web interface in a browser
                                                see we have three live nodes, data nodes
                                        Start yarn, job history servers and fire jobs- see them in there.
                                        on the data node, you see a NodeManager and the DataNode daemons

                                Before you move on, I provided a dataset for you to analyze with your new cluster
                                =========
                                Deploying Hadoop On Ambari
                                =========
                                        suitable for creating a larger cluster now- use an automation tool to deploy a larger cluster
                                                Apache Ambari makes it much easier to provision, monitor, and maintain Hadoop clusters vs tedious individual play
                                                also install common applications built on HDFS, such as Hive and Spark
                                                to create a larger cluster, like the one you'd have at a real company
                                        => NameNode needs more memory to keep track of the metadata than what you used in the previous lesson.
                                        How
                                                Select Hardware
                                                        EC2 dashboard/ launch an instance/ Again, choose Ubuntu server/ 
                                                        m3.large/1 instance/choose 30 GB
                                                                t2.micro instance won't work anymore, instead, you should use m3.large since it has 7.5 GB of memory
                                                                Just one instance is fine for now.
                                                                Call this guy Ambari server.
                                                        configure the security group
                                                                add a rule to allow access to the Ambari web client. 
                                                                        Add a Custom TCP Rule, set the port to 8080, and leave the source as 0.0.0.0 (open to anyone)
                                                                        Typically you'd restrict the source address so that only your organization could access the client.
                                                                open the port 50070 manually in rules- it won't open by default.
                                                        Finally, launch your instance! Copy the domain hostname- Those names also show the IP addresses
                                                                Use them to edit the inbound rules- Port to a range from 0 to 65535
                                                                        And the Source to x.0.0.0/8, where x is your network number
                                                                        Be sure this rule is added to the Ambari server as well
                                                        Private key- use same as created in EC2 chapter- select same old key from drop down menu else create and download.
                                                Prepare instance for use as Ambari nodes
                                                        Ref- Transparent huge pages- https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/s-memory-transhuge.html
                                                        use the instance you just created as a base node of cluster.
                                                        Log into your instance like you did previously
                                                                $ ssh -i /path/to/key_file.pem ubuntu@instance_hostname
                                                        Turn off Transparent Huge Pages, a memory management system. 
                                                                This will avoid potential problems with Hadoop. Edit the file /etc/rc.local,
                                                                $ sudo vim /etc/rc.local
                                                                        Add these lines:
                                                                        if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
                                                                                echo never > /sys/kernel/mm/transparent_hugepage/enabled
                                                                        fi
                                                                        if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
                                                                                echo never > /sys/kernel/mm/transparent_hugepage/defrag
                                                                        fi
                                                        NTP
                                                                why- this package helps synchronize timing across the cluster.
                                                                Install
                                                                        $ sudo apt-get update && sudo apt-get dist-upgrade
                                                                        $ sudo apt-get install ntp
                                                                Run
                                                                        $ sudo service ntp start 
                                                                        $ sudo service ntp status
                                                        Save the instance to an image, call it something like Ambari node. 
                                                                Make sure to check "No reboot"
                                                                you'll be using this instance as the Ambari server.
                                                Install and start Ambari server
                                                        Install (for Ambari version 2.2.0)
                                                                First, copy the private key from your computer to the server instance:
                                                                        $ scp -i key_file.pem key_file.pem ubuntu@server_public_hostname:~/.ssh/id_rsa
                                                                SSH into the Ambari server
                                                                        On the server, add the Ambari repository to the repository sources.
                                                                                $ cd /etc/apt/sources.list.d
                                                                                $ sudo wget http://public-repo-1.hortonworks.com/ambari/ubuntu14/2.x/updates/2.2.0.0/ambari.list
                                                                        Add the key to authenticate Ambari package
                                                                                $ sudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com B9733A7A07513CAD
                                                                        update the package list and upgrade
                                                                                $ sudo apt-get update && sudo apt-get dist-upgrade
                                                                        install Ambari server and run the setup guide
                                                                                $ sudo apt-get install ambari-server
                                                                                $ sudo ambari-server setup
                                                                                You can just do all defaults here
                                                                        Now start the server
                                                                                $ sudo ambari-server start
                                                        Connect to the server's web client and continue with the deployment
                                                                open namenodeHostName:8080 (ensure 8080 is open)
                                                                Log on with username admin/Launch install wizard/name your cluster/select stack- software version installed on cluster (choose latest) HDP means hortonworks data platform
                                                                list the host names cluster nodes or the internal host names
                                                                Upload your SSH private key and change the user account to Ubuntu
                                                                Okay, hit register and confirm, everything should install register just fine
                                                                choose from list of installable services on cluster- for now, just do HDFS, Yarn, Zookeeper and Ambari Metrics
                                                                choose the hosts that the various services run on
                                                                        Ambari has reasonable suggestions
                                                                        but you can see here that there is one service per computer so let's move metrics collector with zookeeper
                                                                        hit next.
                                                                now need assign data nodes and clients- which node take what role.
                                                                        Node manager on systems works with Yarn to keep track of the data node resources
                                                                        Client, installs all these clients on machines
                                                                        Since these hosts are running master components, make these three data nodes with node managers
                                                                next, you shouldn't need to customize any services right now
                                                                        look around to see which options you can change okay, and ignore this
                                                                finally you can deploy it
                                                                        might take a few minutes, so take a break
                                                        add a new custom TCP rule opening port 50070 cuz it doesn't open by default.
                                                        Updating cluster
                                                                more data => more storage, more power, more nodes
                                                                super easy to do it on Ambari
                                                                Make new node
                                                                        Launch a new Node instance
                                                                        Be sure to set the security rules correctly with all the ports open on the subnet
                                                                        Copy the internal host name when it is up and running because you'll need that
                                                                Add host thru ambari
                                                                        Go to the host page from the Ambari dashboard, select Actions, then Add New Hosts
                                                                        So, add the internal host name of the new instance here
                                                                        And configure the rest the same as before
                                                                        Here install the instance as a new Data Node with the Node manager
                                                                        And all this the same and deploy it
                                                        Standby Name Node- Cluster has single point failure of NameNode.
                                                                unreachable the metadata about the blocks
                                                                synchronize with the active NameNode and in case of failure, the standby can take over
                                                                Services > HDFS > Summary > Service Actions menu > Enable NameNode HA (High Availability) > Give any name >
                                                                choose which host you want to be running, the JournalNode, NameNode and what not
                                                                wizard will lead you through the process
                                                                prompt you to log into the NameNodes with their internal host names
                                                                On dashboard, Now we see standby NameNode and an active NameNode

                                                Test and Benchmark Hadoop
                                                        Why? You should do this just to make sure everything is working as you expect.
                                                        How?
                                                                ssh into the NameNode
                                                                        remember, you can find the NameNode through the Ambari dashboard.
                                                                On the NameNode, change to user hdfs
                                                                        $ sudo su - hdfs
                                                                Test with TeraSort
                                                                        $ hadoop jar /usr/hdp/2.3.4.0-3485/hadoop-mapreduce/hadoop-mapreduce-examples-*.jar teragen 500000 random-data
                                                                        $ hadoop jar /usr/hdp/2.3.4.0-3485/hadoop-mapreduce/hadoop-mapreduce-examples-*.jar terasort random-data sorted-data
                                                                        Note- The version numbers (2.3.4.0-3485) might be different for your installation, but just change them to what you have installed
                                                                benchmark the I/O speed with TestDFSIO:
                                                                        $ hadoop jar /usr/hdp/2.3.4.0-3485/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 50MB
                                                                        $ hadoop jar /usr/hdp/2.3.4.0-3485/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 50MB
                                    <Services>
                                        connect to them using ssh connection- refer moba tag under ssh.
                                        Use services
                                            file system
                                                    service- s3- create bucket- make folder- upload files
                                                    aws s3 ls s3://
                                                    aws s3 ls s3://prestodata/2011/
                                                    Web access of hdfs using 50070 port of hostname
                                                            /user/hive/warehouse/trydb.db
                                            hive
                                                    show databases;
                                            presto-cli --catalog hive
                                                presto > show schemas;
                                    </Services>
                                </EC2>
                                <AWSCLI>
                                https://aws.amazon.com/cli/
                                http://docs.aws.amazon.com/cli/latest/reference/emr/create-cluster.html
                                </AWSCLI>
                                <EMR>
                                On Demand Hadoop Cluster (EMR- Elastic Map Reduce)
                                you won't need a cluster running all the time, and idle cluster is a waste of money and computation time
                                terminate the cluster after use- rent + usage cost.
                                We can clone the cluster which is being terminated
                                aws.amazon.com; login cred
                                EMR internally uses EC2
                                        utilities
                                        EC2- Network and security- key pairs- create keypair- Give a name- create
                                                auto-download pem- extract ppk- using moba- tools- moba keygen- load pem file- save ppk file
                                        EC2- Network and security- key pairs- security group
                                Services - EMR- create cluster- advanced
                                        Release latest- hadoop, hue, hive, tez, presto, hcatalog.
                                        uncheck- termination protection
                                        EC2 key pair- choose created key's name
                                        EC2 Security Groups- should allow port 22- Master & slave use- vsc_pig_pspg_poc_grp- already created for team
                                        Let cluster turn on- get the ssh- sth like 'hadoop@ip-10-130-120-249'- hostname and username in there.
                                EMR (not organized)
                                === 
                                Welcome back! In the previous lessons, you deploy your own Hadoop clusters
                                A long running cluster is great when you have many jobs and it's in constant use
                                However, for many cases, you won't need a full cluster all the time
                                It will just be sitting there idle
                                Amazon EC2 instances are charged hourly, so if your cluster is idle, you're wasting money
                                A better option is to deploy a cluster only when you have jobs to run
                                When the the tasks are done, you can shut down the cluster until the next time you have a job
                                Maintaining the cluster also takes time away from other projects
                                For a small organization there might not be enough people to spend time worrying about a cluster
                                To solve these problems, you can use an on-demand Hadoop cluster
                                Let's look at some options
                                There are many services out there that let you create clusters on demand in minutes
                                This allows you to pay for a cluster only when you need the resources
                                Amazon's offering is called Elastic MapReduce, or EMR for short
                                It works well with other AWS services such as S3 and automatically installed applications such as Hive, Presto, and Spark
                                Google has a service too
                                Cloud Data Proc allows you to analyze data with MapReduce, Hive, Spark, and others
                                Microsoft also has a Hadoop cloud service, HDInsight
                                Which one you choose depends on your budget and how well it works with other cloud services you're using
                                For this lesson you'll be using Amazon's EMR
                                Log in to your AWS account and go to the EMR service
                                This is what I see all my past clusters
                                Can create a new one here
                                Here you see a quick set up for clusters
                                There are two different modes you can use
                                Either you can launch a long running cluster or do step execution
                                Which runs all the jobs you add then terminates the cluster
                                Here you can see various applications you can install in the cluster
                                And this is where you select the number of nodes and what type of instances you'll use
                                Before you get started launching a cluster though let's consider how we're going to get data into and out of our cluster
                                Since we won't be storing data on this thing over time
                                Typically transferring data into and out of EC2 instances costs money
                                But Amazon has a service called S3 for storing data
                                Transfers between S3 and EC2 are free
                                So it makes a lot of sense to keep your data on S3 then transfer it to an EMR cluster for analysis
                                Okay let's get started with creating an S3 bucket
                                Go to S3 manager AWS, this is what you should see
                                Click on Create Bucket, there
                                And give it a unique name
                                This can only contain lowercase letters, hyphens, and periods separating words
                                Make sure to choose a region close to you
                                Okay, go in here and create a folder to hold input data which is call input
                                Similarly, create an output folder called output
                                You also need somewhere to store mapreduce and other analysis files
                                Create a folder for that stuff too
                                Okay, now that you have somewhere to store your data, it's time to launch a cluster
                                Thank you editor, that's great
                                Now on to launching a cluster
                                It's better to use the advanced options so you have more control over the nodes in the cluster
                                You can choose which applications you want to install in the cluster
                                Let's just stick with the Hadoop and Hive for now
                                You won't be doing any additional configuration so skip this part
                                But do read up on configuration in the future as the default settings such as the block size might not fit your needs
                                But do read up on the configuration in the future as the default settings such as the block size might not fit your needs
                                Here you can add the steps that the cluster will run when it's created
                                You can have the cluster terminate itself after all the steps are completed
                                This is a convenient way to run a bunch of analysis jobs without having a permanent cluster
                                For now, we'll leave this unchecked since we want to keep the cluster around for a bit
                                Next, you can add nodes to the cluster
                                The master node doesn't need much computation power
                                So this could be a smaller, less expensive instance
                                Amazon suggests you should use M1.large instance
                                The core nodes are used to store and process data
                                You need to create enough core nodes to store your input data, taking into account the replication factor
                                By default, replication factors three for a cluster of ten or more nodes, two for a cluster of four to nine nodes, and one for a cluster with three or fewer nodes
                                For now, Ill just choose two m3.xlarge instances just the default setting
                                Task nodes are used solely for processing data
                                You wont use any now, so leave this at 0 and continue with the setup
                                You should point the login directory to a folder on your s3 bucket then continue on
                                Finally, continue without a private key or choose one you've used previously and is still around on your computer
                                You can use this key to SSH into the cluster instances later
                                Now create the cluster
                                After a few minutes, it'll be ready
                                You can add more core and task nodes here by clicking Resize
                                You run jobs on the cluster by adding steps through here
                                This is what you'll be doing next
                                All right, let's add a step now
                                I provided some fixed server log data
                                What we are doing here is counting the number of hits to the main web page for each day in the logs
                                You can use the mapper or reducer files I provided or write your own
                                Upload the extracted data, the mapper, and reducer files to your S3 bucket
                                The mapper and reducer are written in Python for Hadoop streaming
                                So, for the step 2 streaming program
                                So here is where you link to your mapper and reducer programs
                                Click the folder icons to select the files on your S3 bucket
                                And same thing for the input, and find the output
                                And give this a name, something like Hits
                                And run it, just hit Add
                                Down here you can see the step running
                                You can also view the logs to see what's happening on the cluster and you can also see the jobs running to make sure they're not failing
                                If they do fail you can look at the job logs to help to bug your MapReduce code
                                All right, now that is done running, you should see the output data on your S3 bucket
                                The process is similar for high events spark jobs
                                You put your analysis code input data on this three, then link to them when adding the step
                                And don't forget to terminate the cluster
                                If you leave it running, it can cost you quite a bit
                                Congratulations on completing this course
                                At this point, you've deployed Hadoop clusters manually, automatically with and on demand with Amazon's Elastic MapReduce
                                >From here, I suggest looking into the vast configuration options
                                Things like adjusting the block size, the number of mappers and reducers, and compressing the mapper output can make your jobs run much more efficiently
                                I'll give you some links to information on these configurations
                                Now you're ready to tackle big data projects
                                </EMR>
                                                                            <Uncat>
                                                                            It may be an interesting idea to decouple master node and slate nodes and clients
                                                                                    we prefer accessing all services from edge nodes (which has all clients like hive, hbase shells, ambari)
                                                                                    ambari is not installed on master node so that if master node collapses we have ambari to reinstate it.
                                                                            </Uncat>
                                </AWS>
                                <NaveenNotes>
                                Uncat Naveen's notes on installations
                                User and Group
                                        User Accounts App allows create user (after unlocking) with standard/Admin privileges
                                        Advanced App has come User and Groups- helps manage user and groups better- properties to see currenter users and privileges; add to add groups. User and Group has m:n cardinality.
                                        Log off as a user and Log in as another- click on user also takes us- 2 ticks- double click removes 1.
                                Commands
                                        sudo nano /etc/hostname
                                        sudo adduser hadoop3
                                        sudo passwd hadoop3
                                        sudo su hadoop3
                                        java -version
                                        sudo apt-get install openjdk-6-jdk
                                                /usr/lib/jvm (not java) is where you will find installation
                                                this should be path for java_home in hadoop-env.sh
                                        sudo apt-get install ssh
                                        sudo apt-get install eclipse
                                        sudo apt-get install mysql-server mysql-client
                                        history- all commands recently fired
                                        google- hadoop releases- 1.1.2 tar.gz
                                                right click- extract- open- conf- 
                                                        6 files- open with- gedit
                                                        core-site.xml, hadoop-env.sh
                                                                java_home, hadoop-classpath (envt)
                                                        hdfs-site.xml
                                                        mapred-site.xml, masters- has info abt secondary name node (not master)
                                                        slaves
                                        ssh localhost
                                                yes, passwd
                                                ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
                                                cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
                                Commissioning node, decomissioning- adding/removing nodes from cluster.
                                hadoop fs -lsr /usr/uname
                                -touchz to create a files
                                hadoop fs -get /use/nav1 /home/hadoop/
                                        replace get by copyToLocal for same experience
                                hadoop fs -cp /user/xyz.txt /naveen/
                                -rmr to remove a directory with files recursively.
                                hadoop fs -stat /user/
                                hadoop fs -du /user

                                HDFS- Distributed Storage
                                MR- Distributed Processing
                                YARN- Distributed Scheduling
                                Hadoop Essentials- like oozie, avro, flume, mahout
                                Oozie- workflow
                                DataAccess
                                        Hive- DWH system, PIG- script to analysis not DWH.
                                Data Storage
                                        HBase- NOSQL DBs on Hadoop, Cassandra.
                                Interaction
                                        HCatalog, Crunch
                                Serialization
                                        AVRO
                                ML
                                        Mahout
                                Data Integration
                                        Sqoop, Flume- to get unstructured data (say from FB, twitter), Chukwa.
                                Management- Ambari
                                Zookeeper- monitoring

                                Error
                                        Namenode for null remains unresolved for id null.
                                        go to core-site.xml first- it was wrong ip.
                                        on stopping reveals that no proxy-server to stop- format namenode and restart.
                                        namenode and datanode folder path can't be different directories at >1 level. So, data node didn't start- so hdfs-site.xml
                                JT,TT of MR are replace by Resource Manager and Node Manager of YARN.
                                Node Configuration
                                Eg- for SNN- NN(advanced config)- StNN (replica => same config)
                                        RAM- 32GB-64GB-64
                                        HDD- 1TB-1TB-1
                                        Processor- Xenon 4 cores-8 cores- 8
                                        #Ethernet port- 3 X 1 GB speed-3-3
                                        OS- 64-bit any LINUX version-same-same
                                        Power Supply- Uninterrupted/Redundant-same-same
                                        NN would communicate with SNN after 1 hr- for replica.
                                How do you make a cluster

                                2.x introduces 2 significant things
                                        Hadoop Federation- >= 1 NN (Passive or Stand-by NN)
                                                Separating Namespace layer from blockstorage layer => >1 NN
                                                        cuz- bottleneck on NN would be- RAM (metadata being its part)
                                                Metadata of NN is 2 layers (besides fsImage and editlogs)
                                                Namespace- 
                                                        Where does a block or file exist- directories and files mgmt
                                                        creation/modification/deletion of files.
                                                        Listing files and directories
                                                Blockstorage layer-
                                                        2 parts/layers-
                                                        block management- any information about blocks on cluster.
                                                                number of replication and its info; its placement.
                                                        Physical storage
                                                                stores the blocks
                                                                provides read and write access.
                                        High Availability- 
                                </NaveenNotes>
                            </HadoopDeployment>
                        </HDFS>
                        <Yarn>
                            yarn application --kill application_1486055689667_0003
                        </Yarn>
                        <MapReduce>
                        Defn- It is a parallel programming model for processing large amounts of structured, semi-structured, and unstructured data on large clusters of commodity hardware
                        Mapreduce- technique for processing your data stored in hdfs
                                Now send 10KB program to cluster; if 200MB was to be brought to client- why kept in first place.
                                        job tracker takes request and how it processes we will see now
                                        he wants to read file.txt and get output in folder testOutput
                                        job trackers sends request to name node (can't talk to data node)
                                                client wants to process file.txt- tell me where blocks are
                                                name node checks it's there and metadata is returned to job tracker
                                                Now 10 KB data- gives task to task tracker.
                                                a.txt in 1, 2 and 4- 1 system is chosen (near by or so) for giving task.
                                                        oh system 1, you have a.txt, take 10 KB program to tasktracker and start working on it. This task is called Map.
                                                b.txt in ..., oh system ?, take 10KB program and start... another Map
                                                #mappers = input splits
                                        if task tracker is disturbed, can't finish the task- inform job tracker- it will use some other replication.
                                                task trackers- is it dead? they send heart beat every 3 sec.
                                                no heart beat => dead or very busy- being near, all requests are sent to this node; in either case give task to someone else.
                                                equal distribution given better service, then just contacting nearby node.
                                                task trackers can request, i have empty slots give me jobs.
                                        @train ticket- 100 people in queue- be 101th guy or walk 5 mins to another counter with 5 ppl- train is in 5 mins.
                                        If job tracker is disturbed, all tasks are disturbed, problem! so reliable hardware- so single point of failure here also.
                                        Let's say system 1 gave 4 KB output, 7 gave 1 KB of output and so on.
                                                say population of AP- getting population stats of all districts and 7 has to combine all the outputs
                                                counting process is a process- map.
                                                Reducer is what will combine all your outputs.
                                                #Reducers = #outputFiles
                                                There is 1 reducer for a cluster by default- don't know where it resides.
                                                say it is in system 8- get outputs of mapper and combine.
                                        output is given in data node 8. when giving heartbeat to name node- some client has given it data with output of name testOutput- it's with me.
                                WordCount Job- file.txt has size of 200MB
                                        make 4 input splits (64+64+64+8)
                                                find no of occurence in file- how many time hi is available
                                                        hi- 1; your- 4; how- 5
                                                run program as J$hadoop jar test.jar DriverCode file.txt testOutput
                                                        file.txt is internally split to input splits
                                                        input splits are converted for mapper.
                                                                mapper and reducer can work on key,value pairs.
                                        Record reader for every input split to make it suitable for mapper.
                                                there are 2 records in the first input split of file.txt
                                                reads only 1 record at a time and generates key, value as per file formats
                                                        file formats
                                                                TextInputFormat- by default if file format is not specified.
                                                                        long, text format
                                                                KeyValueTestInputFormat
                                                                        specify in driver code
                                                                        100 Ram 10000
                                                                        101 Krishna 20000
                                                                        b4 first space is taken as key and after as value.
                                                                SequenceFileInputFormat
                                                                        specify in driver code
                                                                SequenceFileAsTextInputFormat
                                                        in form of (byteOffset, EntireLine) for every line/ record is generated.
                                                                like Java map works with wrapper classes not primitive types, Hadoop has given box classes.
                                                                Autoboxing unboxing
                                                                        Primitive types in java and their wrapper class
                                                                        Similarly, hadoop has introduced box classes.
                                                                                int- Integer- IntWritable 
                                                                                long- Long- LongWritable
                                                                                float- Float- FloatWritable
                                                                                double- Double- DoubleWritable
                                                                                String- String- Text
                                                                                char- Character- Text
                                                                        how to convert int to Integer- autoboxing to autounboxing.
                                                                                new IntWritable(int); get();
                                                                                text to string- new Text(str); toString() method.
                                                                                java does conversion automatically- earlier used parseInt Methods.
                                                                Eg- (0, Hi how are you) needs to be stored as (LongWritable, Text) type
                                                                        record reader is written in such a way that from file format it interprets the key,value pair accordingly.
                                                                        if keyValueTestInputFile or SequenceFile InputFormat then write explicitly specify the format in driver code for it to be identified as that.
                                                                                in that case also read first line and convert into key, value pair type.
                                                                                key is separated on basis of tab character- 101\tRam\t 1000\n...(101,(Ram,1000))
                                                                                duplicate keys or values are never checked by record reader in all.
                                        Mapper- as per logic of mapper- those records again spit key-value pairs
                                                for each key, value pair- mapper runs once.
                                                all these work in parallel so looks like parallel computing
                                                based on logic written in mapper.java- (wordcount) spits out (hi,1),(how,1),(are,1),(you,1)
                                                for 2nd line again, it spits- (how,1), (is,1),(your,1),(job,1)
                                                so for all input splits, for every record, result is spitted out.
                                                same mapper code is shared for all mappers for all input splits.
                                                result here is of type (Text, IntWritable)
                                                here, no duplicate keys but duplicate values.
                                        Reducer- now combine all the map output key,value pairs (intermediate data)
                                                duplicate keys here. so use shuffling and sorting to remove them.
                                                Shuffling
                                                        combine all the values corresponding to a key in KV pairs.
                                                        eg- how repeated 5 times- combine to say (how,(1,1,1,1,1)), (is,(1,1,1,1,1,1))
                                                                no more duplicate keys.
                                                Sorting
                                                        sort order is determined using compareTo method in wrapper class.
                                                        WritableComparable Interface- its implementations has compareTO.
                                                reducer also executes 10 times for 10 KV pairs.
                                                        @Servlet's doGet method was getting executed for all request.
                                                        Hadoop has already given identity reducer- write your own for shuffle and sorting. If not given, then identity reducer takes over.
                                                        no shuffling, only sorting in identity reducer.
                                                        reducer has logic to iterate over all values and add them up to produce (how,6)
                                        RecordWriter- just writes 1 KV pair to output file (not folder)- 'part-00000'
                                                _SUCCESS (0KB that successfully done), _logs (directory- history of job, XML files affected)
                                                part-00000 is output file. first KV would be (are, 1),(brother,1),... (Text,IntWritable) type.
                                        Mapper and reducer just know how to run their logic on KV pairs and nth else.
                            <Uncat>
                                Find anagrams using mapreduce
                            </Uncat>
                        </MapReduce>
                        <Tools>
                            <Sqoop>
                                Defn- used to import and export data to and from between HDFS and RDBMS
                                SQOOP
                                        SQL HADOOP
                                        data exchange tool to import and export from/to RDBMS to/from HDFS (not LFS)
                                        helps move into hadoop- transition or for OLAP sake.
                                Notes
                                        Only RDBMS which are java supported- jdbc
                                        default delimiter must be ','
                                        import into hdfs is easy but for export to rdbms- 
                                                must give corresponding schema in advance- without matching it rejects.
                                sqoop version
                                        input- using mysql
                                                mysql -u root -p
                                                show databases;
                                                create database SqoopDB;
                                                use SqoopDB;
                                                show tables;
                                                create table emp(eid int(5), ename varchar(15), salary int(10));
                                                desc emp;
                                                insert into emp values(101,'abc',10000);
                                                insert into emp values(102,'abc2',20000),(103,'abc3',30000),(104,'abc4',40000) ;
                                                select * from emp;
                                                grant all privileges on SqoopDB.* to '%'@'localhost';	//% means all users. nth inside means non-db users also.
                                                CREATE USER 'newuser'@'localhost' IDENTIFIED BY 'password';
                                                GRANT ALL PRIVILEGES ON * . * TO 'newuser'@'localhost';
                                                FLUSH PRIVILEGES;
                                                quit;
                                        sqoop import --connect jdbc:mysql://localhost/SqoopDB --table emp -m 1 --target-dir /SqoopImport
                                                sometimes- gotta add --driver com.mysql.jdbc.Driver to remove error of streaming result sets be closed.
                                                gotta add --username user --password user (-P for safety)
                                                hadoop fs /directory is not same as directory.
                                                access denied for user localhost for database SqoopDB (permissions!)
                                                Error during import-- not primary key could be found.
                                                        use -m 1 (1 mapper) sqoop uses only mapper and not reducer.
                                                hadoop fs -ls /SqoopImport
                                                hadoop fs -cat /SqoopImport/part-m-00000
                                                sqoop import --connect jdbc:mysql://localhost/SqoopDB --table emp -m 1 --target-dir /SqoopImport2 --fields-terminated-by '\t'
                                                query- sqoop import --connect jdbc:mysql://localhost/SqoopDB --table emp -m 1 --target-dir /SqoopImport2 --fields-terminated-by '\t' --columns 'eid,ename' --where 'salary > 25000';
                                                binary object form output- query- sqoop import --connect jdbc:mysql://localhost/SqoopDB --table emp -m 1 --target-dir /SqoopImport2 --fields-terminated-by '\t' --columns 'eid,ename' --where 'salary > 25000' --as-sequencefile	//avrodatafile (for serialization, deserialization- json format)
                                                --query "select * from SqoopDB.emp where eid=101 and \$CONDITIONS" --split-by eid
                                        sqoop export
                                                make a file emp2.txt as follows in LFS
                                                        110,asdf,15000
                                                        111,qwer,18000
                                                        112,zxcv,17000
                                                this style makes insertion into mysql easy.
                                                        hadoop fs -mkdir SqoopImport2
                                                        hadoop fs -put emp2.txt /SqoopImport2
                                                sqoop export --connect jdbc:mysql://localhost/SqoopDB --table emp --export-dir /SqoopImport2/emp2.txt
                                                        the lines were appended to original table.
                                                        not csv- --fields-terminated-by '|'
                                                        -m 1 is very important
                                                        make sure absolte matching of schema to the file- not newline accepted.
                                        How to copy the new data in same directory, same file
                                                use sqoop jobs- time scheduled executions
                                                sqoop job --create First --import --connect jdbc:mysql://localhost/SqoopDB --table emp -m 1 --target-dir /SqoopImport --incremental append --check-column eid --last-value 112
                                                        sqoop job --list
                                                        sqoop job --show First
                                                        sqoop job --exec First
                                                for scheduling execution of jobs we can use oozie tool.
                                        sqoop job --delete First
                                        sqoop job --list
                                        sqoop import --connect jdbc:mysql://localhost/databaseY --username 
                                root --password PASSWORD --query "select * from databaseY.tableX where number = 1474
                                 AND \$CONDITIONS" --target-dir /tmp/ok --as-textfile --direct --split-by number
                            </Sqoop>
                            <Pig>
                                Defn- It is a procedural language platform used to develop a script for MapReduce operations
                                import in hadoop ecosystem- processes huge data without using MR programs.
                                Pig also works with processing on huge data- gives better performance than MR
                                MR uses java- Pig doesn't (externally)- pig script is list of commands written in Pig Latin (made by pig)
                                introduced by yahoo in 2k6 to serve a client request. Apache Pig is OSS.
                                Pig never stores data- processing thru scripts.
                                [Data Flow] Transformations- Built-in commands/operators
                                        Pig always takes I/O from parallel processing
                                        It's abstraction of MR- internally uses MR. But not MR code.
                                        all DFTs are converted into MR programs and from there, execution of mapper/reducer.
                                Pig is not a hadoop core component so need to install it.
                                Modes of execution
                                        MapReduce Mode- data taken from HDFS and using MR.
                                                using command- pig
                                        Local Mode- fetching data from LFS (without using MR) using pig scripts.
                                                using pig -x local
                                        pig -version
                                Modes wrt user interaction
                                        interactive- command and immediate output.
                                        script mode- in a .pig file containing a set of commands to be executed at once.
                                        embedded- our own UDF (User Defined Functions) in code.
                                start
                                        create file- hello.txt- write sth
                                        pig -x local
                                                grunt > (shell starts)
                                                UDF,etc are case sensitive not others.
                                        quit;
                                        vi pigip.txt- name:marks\nabc:65...
                                        grunt> A= LOAD 'pigip.txt' using PigStorage(':'); 	//: being separator
                                                DUMP A;- print command
                                                B= FOREACH A GENERATE $0			//$0 for 1st column
                                                        for each row (records) of table spit out its 1st column
                                                DUMP B;
                                                FOREACH A GENERATE $0,$2;
                                                STORE B INTO 'pigop'		//makes pigop folder containing part-000 file.
                                        hadoop fs -mkdir /pigip
                                        hadoop fs -ls pigip
                                        hadoop fs -put 'pigip.txt' /pigop
                                        hadoop fs -cat /pigip/pigip.txt
                                        pig
                                        grunt> 
                                                A= LOAD '/pigip/pigip.txt' using PigStorage(':');	//spacing is imports A=(space) else error!
                                                same set of commands as above- this takes help of MR program and returns.
                                                in production ppl mostly work in hdfs envt only using MR.
                                                A=LOAD '/pigip/pigip.txt' using PigStorage(':') as (name:chararray,marks:int,gender:chararray);
                                                B=FILTER A by gender=='M';
                                                DUMP B;
                                                B=ORDER A by name;
                                                Males and Females separated by files
                                                        SPLIT A INTO X IF gender=='M',Y if gender=='F'
                                                        STORE X INTO 'pigopMales'
                                                        STORE Y INTO 'pigopFemales'
                                                        split necessitates at least 2 conditions.
                                                        B= A LIMIT 4;
                                                        B= DISTINCT A;
                                                Sum on group by
                                                        C= GROUP B by gender;		//COGROUP gives better performance with large data (same otherwise)
                                                        DUMP C;
                                                        get rid of metadata from tables
                                                        D= FOREACH C GENERATE group, SUM(B.marks)
                                        Script
                                                vi pig1.pig- write down as above
                                                to be run in local mode- don't enter grunt shell
                                                        pig -x local pig1.pig
                                                        from hdfs- 
                                                                copy pig1.pig in apt hdfs
                                                                replace LOAD '/pigip/pigip.txt' using PigStorage(':')
                                                                pig pig1.pig- pig determines it is a hdfs mode- so uses that location in load.
                                                                        pig dir/file.pig gives error- file does not exist
                                        if data is in 2 files
                                                doing join for transformation
                                                cat pigip2.txt- abc		India\nmnp		USA....
                                                load A and load B using '\t' delimiter
                                                SJ=JOIN A by name, B by name
                                                DUMP SJ
                                                LJ = JOIN A by name LEFT, B by name;
                                                RJ- RIGHT, FOJ- FULL
                                        TOKENIZE
                                                FOREACH A GENERATE TOKENIZE(name);
                                                if client demands data to be read as a row making an entry.
                                        FLATTEN
                                                group by returns nested ouputs- change format by FLATTEN
                                                C= FOREACH B GENERATE FLATTEN(A);
                                                many rows streak for males then streak for females.
                                        Diagnostic building operators
                                                DESCRIBE A
                                                B, C
                                                Illustrate
                                                explain- gives MR plan- logical query exec plan.
                                        cat wordcount.pig
                                                A=load 'wc.log'
                                                B=foreach A generate flatten(TOKENIZE((chararray)$0)) as word
                                                C= group B by word;
                                                D=foreach C generate COUNT(B), group;
                                                store D into './wordcount';
                                        UDF- eg
                                                myfile.txt- sample ==> SAMPLE (no facility)
                                                custom data types
                                                        atom- smallest info unit?
                                                        field- each attrib info?
                                                        tuple- equivalent of row?
                                                        bag/ relation- kinda table
                                                        inner bag- only subset of all tuples.
                                                how
                                                        make a java code- UPPER.java
                                                                import java.io.IOException;
                                                                import org.apache.pig.EvalFunction;
                                                                import org.apache.pig.data.Tuple;
                                                                import org.apache.pig.impl.util.WrappedIOException;

                                                                public class UPPER extends EvalFunc< String > {
                                                                        public String exec(Tuple input) throws IOException {	//whenever call UPPER- exec will be executed- only tuple at a time.
                                                                                if(input == null || input.size() == 0)
                                                                                        return null;
                                                                                try {
                                                                                        String str = (String) input.get(0);
                                                                                        return str.toUpperCase();
                                                                                } catch(Exception e) {
                                                                                        throw WrappedIOException.wrap("Caught exception processing input row ", e);
                                                                                }
                                                                        }
                                                                }
                                                                add jars- pig-0.8.0.jar, pig-0.3.0-core
                                                                add created by- in manifest.mf of UP.jar- make sure it is not older than the one that pig uses.
                                                        Export project as java archive- UP.jar- copy to VM, to HDFS.
                                                                b4 trying with HDFS, try the jar with LFS.
                                                                Write in pig file- UP.pig
                                                                        REGISTER UP.jar;
                                                                        A= LOAD 'emp.log' using PigStorage(':') as (eid:int, name:chararray,salary:int,dept:chararray,loc:chararray);
                                                                        B= FOREACH A GENERATE eid,UPPER(name),salary, dept, loc;
                                                                        DUMP B;
                                                                pig -x local UP.pig
                                                        Already built in are aggregators and accumulators.
                                                Pig
                                                        also from semi-structured and unstructured also- like xml, text file for wordcount.
                                                        fast cuz uses parallel processing of MR.

                            </Pig>
                            <HBase>
                                put 'emp','1','professional data:designation','manager'
                                column-oriented database from hadoop stack on top on hdfs cluster to store data
                                        can do random access- nosql database from hadoop stack for huge data store.
                                        why invented? nosql means not only sql- besides sql, other stuff also.
                                        Huge data e1 stores hadoop- processing possible but nosql only for storing.
                                        nosql famous- dynamo (amazon)- key value pair, bigtable- google, cassandra, terastore, mongodb, flockDB (Twitter's graphdb)
                                        all these are for storing- for processing on top of huge data.
                                        HBase is bigtable version of hadoop.
                                HBase vs RDBMS
                                        row structure manner- data is stored in row forms. vs Column oriented- high performance in OLAP.
                                                insertion is by rows
                                                Eg- SID-SName-Marks stored as 101-a-98/102-b-97
                                                ColumnOriented- SID-101-102/SName-a-b...
                                        Distribution is possible with hbase, highly scalability horizontal.
                                        Sparse data performs good with hbase not so with rdb.
                                        semi-structured also do good, unstructured also but not rdb.
                                        sqoop can help import data from rdb.
                                Hadoop maintains only sequential order- doesn't permit random order.
                                        hdfs for high latency or batch processing tools.
                                        hbase does low latency and random access.
                                HBase is 
                                        distributed
                                        nosql from hadoop
                                        for sparse, semi-structured
                                        maintains huge data
                                        multi-dimensional operation.
                                Chapter2
                                Hbase has tight coupling to MR
                                        version db- 101- 98 and updated to 97- then 96- db remembers 3 at least.
                                        good for millions of columns and billions for records
                                        100 records? doesn't suite.
                                        4-5 node cluster and you want 100-1000 records- hbase is not the one
                                        HBase Software is built on top of hdfs- it has following 2 users
                                                HMaster
                                                HRegionServers1, HRegionServers2,..- where actual data will be stored in hbase mechanisms.
                                        Client will contact zookeeper which will help connect to HMaster.
                                                zookeeper service management tool cuz hbase itself uses hmaster and hregion servers as master slave framework.
                                                HRegionServers actually store data in manner of MemStores and HFiles.
                                                St HMaster is tightly coupled with HDFS MR.
                                                HBase allows data loading thru PIG, MR, Hive. Integration is possible.
                                                It supports- building apps- thrift server, avro- to perform operations. web/external application.
                                        HBase maintains batch or iterative shell environment.
                                                like hive shell.
                                                        Tables are key storage
                                                        list- to see tables.
                                                        create 'tablename', 'columnfamily'
                                                                use desc 'tablename' to see details.
                                                        eg- create 'Student', 'Sid', 'Sname', 'Smarks'
                                                                or create 'Student', 'Address'= doorno, street, city, country.
                                                        Insert data/ update data- new colname, rowkey, value are seemless but not columnfamily
                                                                put 'Student', 'rowkey', 'Address:doorno', '13'
                                                                ..
                                                                alter 'emp', 'e2'
                                                                alter 'emp', {NAME=>'e1', VERSIONS=>'3'}- scan still returns latest version- use get with versions to get all.
                                                                confirm by desc 'tablename'
                                                        delete 'Student', 'rowkey', 'Address:doorno', '13'
                                                        Retrieve- get 'tablename','rowkey'; get 'tablename', 'rowkey', 'columnfamily:column'; scan 'tablename'
                                                        file insert operations for bulk inserts.
                                                also hue like- WEB UI Interfaces.
                                                Java programs- a class exists for every command.
                                        start-all.sh- to start all storage and processing components
                                                jps- all up services.
                                                        Namenode, datanode, secondary namenode, etc.
                                                start-hbase.sh to start hbase.
                                                        JPS- starts HMaster, HRegionServers, HQuorumPeer.
                                                hbase shell
                                                        client cli shell- hbase(main):(command_seq_no):(0)

                            </HBase>
                            <Hive>
                                <What>
                                    Hive- Datawarehousing (OLAP) on top of hadoop.
                                    who- Big Data Analytics using Hadoop Framework. ETL developers and professionals
                                    Analytics (manage and process) for structured data.
                                        stores schema in a database and processed data into HDFS but not a RDB.
                                        familiar, fast, scalable, and extensible but not for real-time queries and row-level updates
                                    defn- data warehouse infrastructure tool to process structured data in Hadoop.
                                        OR platform used to develop SQL type scripts to do MapReduce operations.
                                    Uses HQL/HiveQL ~ SQL
                                    REL FB needed fast processing on hadoop- so hive.
                                    vs MR would take lengthy code- Hive is short and smart.
                                    stores in hdfs in file formats
                                            as avro, parquet, etc files but getting data takes more time than rdb.
                                            so it takes same hdfs data but maintains in the form of tables.
                                            hive's main duty is to add schema to the hdfs
                                    Eg- Student.txt has id-name-marks/1-a-98/...
                                            making table 'Student' is beneficiary here. data retrieval as row/cols easy here- making indexes.
                                </What>
                                <InstallingHive>
                                    Assuming that hadoop is already installed
                                    DownloadAndPlacement
                                        download from http://apache.petsads.us/hive/hive-0.14.0/
                                        tar zxvf apache-hive-0.14.0-bin.tar.gz
                                        copy files to /usr/local
                                                $ su -
                                                passwd:
                                                # cd /home/user/Download
                                                # mv apache-hive-0.14.0-bin /usr/local/hive
                                                # exit
                                    HiveSettings
                                        Setting up environment for Hive
                                                append to ~/.bashrc
                                                    export HIVE_HOME=/usr/local/hive
                                                    export PATH=$PATH:$HIVE_HOME/bin
                                                    export CLASSPATH=$CLASSPATH:/usr/local/Hadoop/lib/*:.
                                                    export CLASSPATH=$CLASSPATH:/usr/local/hive/lib/*:.
                                                $ source ~/.bashrc
                                        Configuring hive
                                                $HIVE_HOME/conf
                                                        $ cd $HIVE_HOME/conf
                                                        $ cp hive-env.sh.template hive-env.sh
                                                        edit hive-env.sh
                                                                append- export HADOOP_HOME=/usr/local/hadoop (where hadoop is installed)
                                                Hive installation is completed successfully
                                    MetastoreSettings
                                        Downloading and Installing Apache Derby
                                                external database server to configure Metastore
                                                Download and extract
                                                        $ cd ~
                                                        $ wget http://archive.apache.org/dist/db/derby/db-derby-10.4.2.0/db-derby-10.4.2.0-bin.tar.gz
                                                        $ tar zxvf db-derby-10.4.2.0-bin.tar.gz
                                                        $ ls
                                                Copy to /usr/local
                                                        $ su -
                                                        passwd:
                                                        # cd /home/user
                                                        # mv db-derby-10.4.2.0-bin /usr/local/derby
                                                        # exit
                                                environment for Derby
                                                        append to ~/.bashrc
                                                        export DERBY_HOME=/usr/local/derby
                                                        export PATH=$PATH:$DERBY_HOME/bin
                                                        Apache Hive
                                                        18
                                                        export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar
                                                $ source ~/.bashrc- to execute the file
                                                Create a directory to store Metastore
                                                        $ mkdir $DERBY_HOME/data
                                                        Derby installation and environmental setup is now complete
                                        Configuring Metastore of Hive
                                            meaning- specifying to Hive where the database is stored
                                            editing the hive-site.xml in $HIVE_HOME/conf directory
                                                    $ cd $HIVE_HOME/conf
                                                    $ cp hive-default.xml.template hive-site.xml
                                                    <configuration>
                                                            ....
                                                            <property>
                                                               <name>javax.jdo.option.ConnectionURL</name>
                                                               <value>jdbc:derby://localhost:1527/metastore_db;create=true </value>
                                                               <description>JDBC connect string for a JDBC metastore </description>
                                                            </property>
                                                            ...
                                                    </configuration>
                                            Create a file named jpox.properties and add the following lines into it:
                                                    javax.jdo.PersistenceManagerFactoryClass =
                                                    org.jpox.PersistenceManagerFactoryImpl
                                                    org.jpox.autoCreateSchema = false
                                                    org.jpox.validateTables = false
                                                    org.jpox.validateColumns = false
                                                    org.jpox.validateConstraints = false
                                                    org.jpox.storeManagerType = rdbms
                                                    org.jpox.autoCreateSchema = true
                                                    org.jpox.autoStartMechanismMode = checked
                                                    org.jpox.transactionIsolation = read_committed
                                                    javax.jdo.option.DetachAllOnCommit = true
                                                    javax.jdo.option.NontransactionalRead = true
                                                    javax.jdo.option.ConnectionDriverName = org.apache.derby.jdbc.ClientDriver
                                                    javax.jdo.option.ConnectionURL = jdbc:derby://hadoop1:1527/metastore_db;create = true
                                                    javax.jdo.option.ConnectionUserName = APP
                                                    javax.jdo.option.ConnectionPassword = mine
                                    Verifying Hive Installation
                                            $ $HADOOP_HOME/bin/hadoop fs -mkdir /tmp 
                                            $ $HADOOP_HOME/bin/hadoop fs -mkdir /user/hive/warehouse
                                            $ $HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp 
                                            $ $HADOOP_HOME/bin/hadoop fs -chmod g+w /user/hive/warehouse
                                            $ cd $HIVE_HOME
                                            $ bin/hive
                                                    opens hive shell
                                </InstallingHive>
                                <Architecture>
                                Internal Architecture Components on top of MR and HDFS (can't take from RDB, other os)
                                        <Clients>
                                                User Interface- software that can create interaction between user and HDFS
                                                <HiveShell>
                                                    Hive SHELL- CLI to fire HQL.
                                                        hive- to start hive shell.
                                                                                                                                    sudo hive for root permission but both are not allowed to access hive.
                                                                                                                                    we need hive user- sudo su hive; and then hive.
                                                        hive -f "file.hql"- creates in default directory if not specified.
                                                    <Commands>
                                                    Data types
                                                            Column Types
                                                                    int
                                                                            variants- tiny, small, big
                                                                    string- char (255), varchar (1 to 65355)
                                                                    Timestamp- unix timestamp with nano facility
                                                                            supports yyyy-mm-dd hh:mm:ss.ffffffffff
                                                                            Dates
                                                                    decimal(10,0)
                                                                    Union Types- collection of heterogenous data types
                                                                            {3:{"a":8,"b":"eight"}} 
                                                            Literals
                                                                    Floating
                                                                    Decimal
                                                            Null Values
                                                                    Missing values are represented by the special value NULL.
                                                            Complex Types
                                                                    ARRAY< data_type >
                                                                    MAP< primitive_type, data_type >
                                                                    STRUCT< col_name : data_type [COMMENT col_comment], ... >
                                                    DDL
                                                            theme for structured data analysis is to store the data in a tabular manner, and pass queries to analyze it
                                                            default database exists already
                                                            database is a namespace or a collection of tables
                                                            CREATE DATABASE|SCHEMA [IF NOT EXISTS] < database name>
                                                                    eg- CREATE DATABASE IF NOT EXISTS userdb;
                                                                    Eg- create database HiveDB; use hivedb; (cases lower)
                                                            commands- show tables; show databases, use database;
                                                            DROP DATABASE StatementDROP (DATABASE|SCHEMA) [IF EXISTS] database_name 
                                                                    [RESTRICT|CASCADE];
                                                                    eg- DROP DATABASE IF EXISTS userdb;
                                                                    cascade means drop tables b4 database- which it anyway does.
                                                            create table
                                                                    CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name
                                                                            [(col_name data_type [COMMENT col_comment], ...)]
                                                                            [COMMENT table_comment]
                                                                            [ROW FORMAT row_format]
                                                                            [STORED AS file_format]
                                                                            [location "s3://prestodata/2011/next"];
                                                                    Eg- CREATE TABLE IF NOT EXISTS employee ( eid int, name String,
                                                                            salary String, destination String)
                                                                            COMMENT 'Employee details'
                                                                            ROW FORMAT DELIMITED
                                                                            FIELDS TERMINATED BY '\t'
                                                                            LINES TERMINATED BY '\n'
                                                                            STORED AS TEXTFILE;
                                                                    Notes- terminated by ',', stored as sequencefile, etc
                                                                    Verify - show tables; desc tablename;
                                                            Alter table
                                                                    ALTER TABLE name RENAME TO new_name
                                                                    ALTER TABLE name ADD COLUMNS (col_spec[, col_spec ...])
                                                                    ALTER TABLE name DROP [COLUMN] column_name
                                                                    ALTER TABLE name CHANGE column_name new_name new_type
                                                                    ALTER TABLE name REPLACE COLUMNS (col_spec[, col_spec ...])
                                                                    Eg- hive> ALTER TABLE employee RENAME TO emp;
                                                                            hive> ALTER TABLE employee CHANGE name ename String;
                                                                            hive> ALTER TABLE employee CHANGE salary salary Double;
                                                                            ALTER TABLE employee ADD COLUMNS (dept STRING COMMENT 'Department name');
                                                                            ALTER TABLE employee REPLACE COLUMNS (eid INT empid Int, ename STRING name String);
                                                            DROP TABLE [IF EXISTS] table_name;
                                                                    hive> DROP TABLE IF EXISTS employee;
                                                                    emp will be deleted but emp2 being external doesn't delete the file.
                                                                    but select * from emp2 still won't return a thing cuz emp2 not in metadata but there in hdfs.
                                                                    to delete from file- use hadoop fs -rm /user/hive/....emp2/emp.txt
                                                            <Partitions>
                                                                Why- 
                                                                    Say 1000 employees and wanna tell- 
                                                                    emp with salaray > 1M in Hyd (100 records), blr (200), chn(300)
                                                                    better search only hyd bucket
                                                                How
                                                                    create table partemp(eid int, ename string, esal double)
                                                                        partitioned by (loc string)
                                                                        row format delimited
                                                                        fields terminated by ','
                                                                        lines terminated by '\n'
                                                                        stored as textfile
                                                                    Load Data
                                                                        load data local inpath 'emp.txt' into table partemp partition(loc='HYD');	//loc not in emp.txt
                                                                        load data local inpath 'emp2.txt' into table partemp partition(loc='BLR');	
                                                                        set hive.exec.dynamic.partition.mode=nonstrict;
                                                                            insert overwrite table partition_table partition (aeic) select unit_key,snsr_rdng_utc_ts
                                                                    State
                                                                        select * from partemp; 	//also shows loc='HYD' as a column.
                                                                        another directory of loc=BLR in partemp.
                                                                            hadoop fs -cat /user/hive/warehouse/hivedb.db/partemp/loc=HYD/emp.txt
                                                                            select * from partemp where loc='BLR' and esal > 1000000; (only blr directory queried)
                                                                    Related Info
                                                                        desc partemp;- tells partition by which column
                                                                        show partitions partemp;
                                                            </Partitions>
                                                            <Bucketing>
                                                                Why- Fixed number of divisions using hash algo, useful for efficient sampling
                                                                How
                                                                    set hive.enforce.bucketing=true;	//in hive shell
                                                                    create table buckemp2 (eid int , ename string, esal double, loc string)
                                                                        clustered by (eid) into 3 buckets;
                                                                    insert overwrite table buckemp2
                                                                        select * from employee;
                                                                        Verify-
                                                                            hadoop fs -ls /user/hive/warehouse/hivedb.db/buckemp2/
                                                                                buckemp2 has 3 buckets- 000000_0, 000001_0, etc.
                                                                            desc buckemp;
                                                                            show tables;
                                                                            select * from buckemp2;

                                                                        How
                                                                            uses MR and creates buckets while making buckemp2 from existing table employee
                                                                            distribution id done based on hash algorithms.
                                                                    how to do sampling with bucketing
                                                                        select * from buckemp2 TABLESAMPLE(BUCKET 2 OUT OF 3 ON eid);	//OR BUCKET 1
                                                            </Bucketing>
                                                            PARTITION
                                                                    What
                                                                        dividing a table into related parts based on the values of partitioned columns
                                                                        easy to query a portion of the data
                                                                        bucketing for more efficient querying
                                                                                based on the value of hash function of some column of a table
                                                                        if you partition the employee data with the year and store it in a separate file, it reduces the query time for queries on year
                                                                    Add PARTITION
                                                                            ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec
                                                                                    [LOCATION 'location1'] partition_spec [LOCATION 'location2'] ...;
                                                                                    partition_spec:: (p_column = p_col_value, p_column = p_col_value, ...)
                                                                            Eg- hive> ALTER TABLE employee 
                                                                                    ADD PARTITION (year='2013')
                                                                                    location '/2012/part2012';
                                                                    Rename PARTITION
                                                                            hive> ALTER TABLE employee PARTITION (year='1203')
                                                                                    RENAME TO PARTITION (Yoj='1203');
                                                                    Delete PARTITION
                                                                            hive> ALTER TABLE employee DROP [IF EXISTS]
                                                                                    PARTITION (year='1203');
                                                            Views
                                                                    CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ]
                                                                            [COMMENT table_comment]
                                                                            AS SELECT ...
                                                                    Eg- hive> CREATE VIEW emp_30000 AS
                                                                            SELECT * FROM employee
                                                                            WHERE salary>30000;
                                                                    hive> DROP VIEW emp_30000;
                                                            Index
                                                                    creating a pointer on a particular column of a table
                                                                    CREATE INDEX inedx_salary ON TABLE employee(salary)
                                                                            AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';
                                                                    hive> DROP INDEX index_salary ON employee;
                                                    DML
                                                            Insertion
                                                                    LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename 
                                                                            [PARTITION (partcol1=val1, partcol2=val2 ...)]
                                                                    Eg- hive> LOAD DATA LOCAL INPATH '/home/user/sample.txt'
                                                                            OVERWRITE INTO TABLE employee;
                                                                    How- if from LFS- copied to HDFS but if already hdfs, n copies are not maintained (it is moved)
                                                                        hive doesn't allow insert row by row- meant to do batch insert- very purpose. discourages.
                                                                    Verify
                                                                        open the file- as hadoop fs -cat /user/hive/.../emp.txt
                                                                            check hadoop fs -ls /user/hive- there is one warehouse folder in there.
                                                                            inside it all databases exist- table inside and also partitions.
                                                    DQL- Select
                                                            SELECT [ALL | DISTINCT] select_expr, select_expr, ... 
                                                                    FROM table_reference 
                                                                    [WHERE where_condition] 
                                                                    [GROUP BY col_list] 
                                                                    [HAVING having_condition] 
                                                                    [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]] 
                                                                    [LIMIT number];
                                                            Operators
                                                                    Relational Operators
                                                                            =,!=, <,<=,>,>=, IS NULL, IS NOT NULL, A LIKE B, A RLIKE B (any substring of A maches B)
                                                                            NULL if either is null.
                                                                    Arithmetic
                                                                            BODMAS, MOD, BITWISE.
                                                                            hive> SELECT 20+30 ADD FROM temp; => ADD column has value 50
                                                                    Logical
                                                                            && and AND are same (AND, OR, NOT)
                                                                    Complex
                                                                            A[n], M[key], S.x (Array, map, struct)
                                                            hive> SELECT * FROM employee WHERE Salary>40000 && Dept=TP;
                                                            Built-in functions
                                                                    String
                                                                            concat(string A, string B,...), substr(string, 3), substr(string,3,5)
                                                                            ucase/upper, trim/ltrim, regexp_replace(string A, string B, string C)
                                                                    Int
                                                                            cast('1' as BIGINT) 
                                                                    decimal
                                                                            round(double a), floor, ceil, rand(), 
                                                                    Date
                                                                            from_unixtime(int unixtime)	
                                                                            to_date(string timestamp)
                                                                            year(string date)
                                                                    Complex
                                                                            size(Map< K.V >), size(array)
                                                                            get_json_object(string json_string, string path)
                                                                    Eg- SELECT round(2.6) from temp;
                                                                    Aggregate functions
                                                                            count, sum, avg, min, max
                                                            Resultset res = stmt.executeQuery("SELECT * FROM employee WHERE salary>30000;");
                                                                    while (res.next())
                                                                            System.out.println(res.getInt(1);
                                                            Eg- SELECT Id, Name, Dept FROM employee ORDER BY DEPT;
                                                            Eg- SELECT Dept,count(*) FROM employee GROUP BY DEPT;
                                                            //Nesting
                                                            with t1 as (select aeic,gpn_nm,avg(snsr_rdng_val) as sensor_value, snsr_rdng_utc_ts
                                                            from sevenyeardata1
                                                            where aeic='236073' AND snsr_rdng_utc_ts >='2010-11-01'
                                                            group by aeic,snsr_rdng_utc_ts,gpn_nm limit 1000),
                                                            t2 as (select aeic,gpn_nm,avg(snsr_rdng_val) as sensor_value, substr(snsr_rdng_utc_ts,1,10) as dayOfYear
                                                            from sevenyeardata1
                                                            where aeic='236073' AND snsr_rdng_utc_ts >='2010-11-01'
                                                            group by aeic,substr(snsr_rdng_utc_ts,1,10),gpn_nm limit 100)
                                                            select * from t2 limit 5;

                                                            //weekbins quantum
                                                            select aeic, gpn_nm, (snsr_rdng_epc_ts-0)/604800000 as week_bin, avg(snsr_rdng_val) as avgval
                                                            from sevenyeardata1 
                                                            where aeic='236073' AND snsr_rdng_utc_ts >='2010-11-01' 
                                                            group by aeic,gpn_nm,(snsr_rdng_epc_ts-0)/604800000
                                                            order by avgval desc limit 10;
                                                            date > 'string' return dictionary comparison's results- so works well on date when it's in yyyy-mm-dd... format
                                                                    date to week- thru epoch, thru subtract from a date and period dealings, 
                                                            'group by' is a combination- the sequence doesn't matter.
                                                                    to verify I had to order by along most random column (numeric)- so values matched for permutations.
                                                                    don't you dare propagate average along a chain in sql- cuz average of average is not average.
                                                            JOIN
                                                                INNER JOIN
                                                                    hive> SELECT c.ID, c.NAME, c.AGE, o.AMOUNT 
                                                                            FROM CUSTOMERS c JOIN ORDERS o 
                                                                            ON (c.ID = o.CUSTOMER_ID);
                                                                LEFT OUTER JOIN
                                                                RIGHT OUTER JOIN
                                                                FULL OUTER JOIN
                                                            <AddingUDF>
                                                                client request can be served better
                                                                GenderUDF.java
                                                                        import org.apache.hadoop.hive.q1.exec.UDF;
                                                                        import org.apache.hadoop.io.Text;
                                                                        public class GenderUDF extends UDF {
                                                                                private Text result = new Text();
                                                                                public Text evaluate(Text str) {
                                                                                        if(str == null) {
                                                                                                return null;
                                                                                        }
                                                                                        String s1 = str.toString(), s2;
                                                                                        if(s1.matches("M")) {
                                                                                                s2 = "MALE";
                                                                                        } else {
                                                                                                s2 = "FEMALE";
                                                                                        }
                                                                                        result.set(s2);
                                                                                        return result;
                                                                                }
                                                                        }
                                                                        referenced jars- hive-exec-0.13.1.jar, hadoop-ant-0.20.2-cdh.3u5.jar, hadoop-core-0.20.2-cdh3u5.jar, hadoop-examples-0.20.2-cdh3u5.jar
                                                                        export as jar- genderUDF.jar- added to hive
                                                                show functions;
                                                                        //shows built-in functions
                                                                        if still not satisfied then use user defined functions.
                                                                add jar genderUDF.jar; 	// adds to hive classpath- we need to open hive shell while the jar is in same folder as the jar.
                                                                create temporary function genderUDF as 'GenderUDF'	//confirm- GenderUDF being class name.
                                                                show functions;
                                                                select max(eid) from udfemp2;
                                                                select eid, ename, genderudf(gender) from udfemp2;
                                                                //uses jar logic added to hive directory.
                                                            </AddingUDF>
                                                    </Commands>
                                                </HiveShell>
                                                <WebInterface>
                                                    you can choose tez or mr mode from settings
                                                </WebInterface>
                                                <API>
                                                    APIs- Thrift ; JDBC, ODBC, Python
                                                                                                                    works same as sql java driver, username will be hdfs to access the resource.
                                                </API>
                                        </Clients>
                                        <Driver>
                                            all given by apache
                                            takes all input from the user and Passes apt request to Compiler
                                        </Driver>
                                        <Compiler>
                                            translates the CLI, GUI commands and forwards to Execution Engine.
                                        </Compiler>
                                        <ExecutionEngine>
                                                defn- processes the query and generates results as same as MapReduce results using MR flavor.
                                                maintains Architecture. 
                                                    does conversion to MR jobs.
                                                helps storage
                                                    May take help of Metastore for storing as table data and metadata.
                                                    <Metastore>
                                                        defn- respective database servers to store the schema or Metadata of tables, databases, columns in a table, their data types, and HDFS mapping
                                                        What- stores metadata and Tables for hive. 
                                                        Eg- The student.txt is stored as student table here
                                                        to store all this information- internal store- mechanism to store all. all schema, views, etc.
                                                        DB Used- by default metastore is taken care by derby operating server or db
                                                            we can also configure mysql, oracle, etc.
                                                        Details- 
                                                                Databases- data is divided into databases.
                                                                Tables- rdbms or structure information which we get from hdfs.
                                                                    Types
                                                                        managed/Internal- default table creation
                                                                            How- xyz.txt (LFS) -> Manxyz created holding reference to xyz.txt of hdfs Metastore (Hive) -> HDFS stores xyz.txt
                                                                                deleting xyz database- deletes manxyz along with xyz.txt of hdfs.
                                                                        external- create external table tablename (colname datatype,..)
                                                                            How- ab.txt uploading makes extxyz and data is actually inserted as relational form.
                                                                                deleting ab database- deletes extxyz information but doesn't delete actual data in hdfs as metastore.
                                                                                metainfo still remains- we can delete using commands.
                                                                                external helps us get higher performance
                                                                        ?without any file specification hive's default warehousing directory in hdfs.
                                                                Views- help manage huge data.
                                                                        eg- keep join-table as view for further manipulations.
                                                                Maintaining indexes- gives better speed.
                                                                built-in functions
                                                                        aggregates
                                                                        User defined functions (UDF)- upper (changing cases)		
                                                                Partitions- bucketing principles for programming.
                                                                        why- alter performance- data from last 5 years- entire data in slices/partitions.
                                                                                make groups of data and search in the groups only. Eg- partition data by years and then querying for a year means only that group.
                                                                                Eg- Joining dates for students- partitioned by years- I will need to search only 2015 year's paritition if I do.
                                                                        partitioned by- can change dynamically.
                                                                                use when slicing not known in advance.
                                                                                condition will be used for choosing paritition
                                                                        cluster by- declared at the creation time.
                                                                                Eg- 3 buckets to be created for 2015, 2016, 2017. for 2018? nay!
                                                                                hash algorithm is used for choosing bucket.
                                                                Views/Indexes- for optimizing for efficeint processing.
                                                    </Metastore>
                                        </ExecutionEngine>
                                        HiveQL Process Engine- querying on schema info on the Metastore; query for MapReduce job and process
                                </Architecture>
                                <Working>
                                    Execute Query- Interface sends query to Driver
                                        Get Plan- Driver uses compiler that parses the query to check the syntax and query plan
                                            Get Metadata- compiler request metadata info from Metastore.
                                        Execute Plan- driver sends the execute plan to execution engine
                                            Execute Job- execution engine sends the job to JobTracker (in namenode)
                                                JobTracker assigns this job to TaskTracker (Data node)
                                                ||execution engine can execute metadata operations with Metastore
                                </Working>
                                MR queries HDFS- does processing and returns upwards.
                                <ObservationsUncat>
                                    Version- use `hive --version`- Hive 1.2.1000.2.4.0.0-169
                                    Set some important configs
                                            GUI- Ambari > Hive > Configs > Advanced > Custom hiveserver2-site > Key = hive.security.authorization.sqlstd.confwhitelist.append, Value = mapred.job.name > Restart All Affected
                                    Fire queries
                                            CLI- hive
                                                    not starting- relogin ambari, or restart ;check all services running
                                            GUI- Hive query view
                                                    st we get only after VM restart, or using ssh, use hive shell to fire queries like show databases
                                                    change whether you want tez mode or mr in settings
                                            Stop execution- Ctrc+C for command only cancels our view. Also fire yarn kill
                                    View query status
                                            http://192.168.237.131:19888/ws/v1/history/mapreduce/jobs- doesn't work cuz tez by default- make it mr AND only MR generating jobs are recorded
                                            change from gui settings
                                    Making GZip files
                                            http://stackoverflow.com/questions/14614126/hive-gzip-file-decompression
                                            https://cwiki.apache.org/confluence/display/Hive/CompressedStorage
                                            https://sites.google.com/site/hadoopandhive/home/how-to-create-output-in-gzip-files-in-hive
                                            https://community.hortonworks.com/questions/28856/hive-table-format-and-compression.html
                                            ORC+Zlib after the columnar improvements no longer has the historic weaknesses of Zlib, so it is faster than SNAPPY to read, smaller than SNAPPY on disk and only ~10% slower than SNAPPY to write it out.
                                            when it comes to HBase, Snappy is usually better :) (for query throughput?)
                                            http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.3/bk_performance_tuning/content/hive_perf_best_pract_use_orc_file_format.html
                                    exit
                                    location vs load
                                        location "s3://prestodata/2011/next"; inline of query vs load data inpath 's3://prestodata/2011/next' into table trytable;
                                        load data- s3 is hadoop envt part- so gotta write inpath (not local)
                                            the file vanishes after load data command.
                                        location "s3://...."- in both external and internal tables- the file doesn't move into hive metastore but remains in specified location
                                            even load data will write in that location only- data is loaded automatically from the location.
                                    hive.force-local-scheduling=false
                                </ObservationsUncat>
                            </Hive>
                            <Spark>
                                    Version- 
                                    Command Line Client
                                        start the thrift server 
                                            GUI- using ambari
                                            CLI- if not accessible thru GUI
                                                    cd /usr/hdp/current/spark-client	
                                                            //and not cd /usr/hdp/2.4.0.0-169/spark/sbin/start-thriftserver.sh
                                                    chown spark:hadoop logs
                                                    ./sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10015 --master yarn-client
                                                            should be in same tab cuz environment change is known to it- localhost and ip both are honored
                                                            failed to launch org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 (not a problem)
                                                            connected message shows up-'started'
                                            Verify
                                                [root@sandbox ~]# beeline
                                                beeline > !connect jdbc:hive2://192.168.237.131:10015/ (\n*)
                                        To open spark shell- $ spark-shell
                                            many bindExceptions may show up but can be ignored.
                                        fire queries
                                            sqlContext.sql("show databases").show()
                                            sqlContext.sql("SELECT truckid from geo.geolocation group by truckid limit 100").show();
                                            +-------+
                                            |truckid|
                                            +-------+
                                            |     A7|
                                            |    A18|
                                            |    A24|
                                            +-------+
                                            only showing top 20 rows
                                        exit
                                    Notes
                                            fire a query- show databases (responds), jdbc sends 53 sized packets cuz 
                                                even spark shell ain't responding here- in turn, cuz hive wasn't responding.
                                                Root cause was the location of start-thriftserver.sh wasn't right- only show tables; was responding with previous executable.
                                                //not sure- check the metastore in hive-conf.xml in spark/conf
                                            Cannot modify mapred.job.name at runtime
                                            code not working- look into concept of apache thrift server- sql layer for spark.
                                <Suresh>
                                    Apache Spark
                                        faster and efficient big data processing and data analytics
                                        execution engine to process big data.
                                        Spark Evaluation
                                        Features
                                        Ecosystem tools of spark
                                        Big data generated by web and mobile (that which can't be stored/processed with traditional architecture)
                                                Eg- 8 GB PD has 8.1GB data- it is big data wrt PD.
                                                Eg- FB, Twitter, etc are generating data at huge rate- can't tell them to stop- analyse faster.
                                                Eg- eCommerce,healthcare, e-goverance.
                                                Eg- every document is converting into e-documents- faster searches we want thru papers.
                                                Eg- Search Engines- google, yahoo- generate in petabytes.
                                        Char
                                                Size- store and process
                                                Kinds- Structured (tables),semi-structured (XML, excel), Unstructured (media)
                                                        90% unstructured. ETL converts unstructured to structured
                                        Hadoop
                                                OSS, Fault Tolerant, High Scalability, Cost Effective.
                                        Analytics use in real life- company can study 10 yrs and decide what to continue and what to drop.
                                                Weather forecast uses analysis of past years, etc.
                                                Single machine tools
                                                        R tool, Pandas which on top of python for analytic tool.
                                                        unix also gives some analytics- grep, awk, etc.
                                                Proposal of big data world is do all analytics over cluster
                                                        for better performance.
                                        Hadoop vs spark
                                                spark has its own cluster management- not dependent on hdfs but can work best on hdfs.
                                                        can compute and process fast but for storage we are using hdfs as storage component.
                                                        for processing, people have moved from MapReduce to Spark cuz MapReduce is slower for complex, iterative and long running jobs.
                                                Spark moves disk computations to in-memory computations. 10-100 times faster. But can store in disk also.
                                                Along with Mappers and Reducers we have Transformations and Actions
                                                Along with Batch has Iterative and streaming jobs in spark.
                                                        Batch means you send queries or commands- Q1,Q2,Q3 together and get answer A1,A2,A3
                                                        Interactive means Q1,A1,Q2,A2,...
                                                        Iterative means Q1,Q2,Q3,Q4,Q1,Q2,...
                                                                clarify- means Q1's output is input for Q2 or it is sequence of queries.
                                                        Streaming- Q1,Q2, etc all are taking data from engine.
                                                Spark programmers can use API in java, python, scala, etc.
                                                        involves lessers lines of code for same function
                                                        supports advanced analytics
                                        Benchmarking speed
                                                hadoop vs spark results in 2013 iterative jobs benchmarking
                                                        K-means- 121 vs 4.5 sec
                                                        Logistic Regression- 80 sec vs 0.91 sec.
                                                2014 benchmarking
                                                        100TB- 72 mins- 23 mins
                                                        1PB- -234 mins
                                                        1.42 TB/min vs 6.27 TB/min
                                        Usecases
                                                Capital banking- to recommend products
                                                Conviva uses it for processing media
                                                yahoo- for news generation
                                                IOT- wearables, etc
                                                genomics (study of combining DNA)
                                                Performance optimization
                                                        Used for catalyst optimization technique- reduces 70% time.
                                                        Trunsten off heap optimization- reduces the memory- less garbage collection.
                                        History
                                                2009 UCBerkley's proposed as hDP's subproject.
                                                2010- OSS license it got
                                                2013- moved to Apache Spark Foundation.
                                                2014- popularity and Top level project for ASF.
                                                github community support.
                                        Spark Features
                                                Lower overhead for starting jobs
                                                less expensive to maintain
                                                lazy evaluation- pattern evaluation.
                                        Ecosystem
                                                Spark Core engine- basic functionality
                                                        facilitates utilities and architecture to all components as follows
                                                Components
                                                        Spark SQL- competes HQL
                                                        Spark Streaming- does live data streaming- competes Storm.
                                                        Spark Machine Learning Library- competes Mahout
                                                                supports most of ML algorithms.
                                                        Spark GraphX- competes Google Graphs.
                                                                network data- social network, mobile communication providers, etc.
                                                        Spark R- competes R.
                                        RDD- Resilient Distributed Datasets
                                                is a fault tolerant collection of elements that can be operated on in parallel.
                                                Restricted form of distributed shared memory- read-only
                                                Creation
                                                        from any data source- 
                                                                load from scala collection (1 of easiest), local file system, hdfs, amazon s3, HBase table.
                                                                supports text files, sequence files and other hadoop input format and also take a directory or a glob
                                                                        In local mode (single node operation), you just need to specify the file location
                                                                        In distributed mode, the pre-requisite is the availability of the file in all nodes of the cluster
                                                                                Sample
                                                                                        import spark.SparkFiles;
                                                                                        ...
                                                                                        sc.addFile("Sample.txt");			//used to copy the file to designated location to all machines in the cluster
                                                                                        val inFile = sc.textFile(SparkFiles.get("Sample.txt"));		//inFile is an RDD created.
                                                        Transformations from other RDDs
                                                                Express computation by defining RDD
                                                                computation to create the data in a RDD is only done when the data is referenced
                                                                        example: caching results or writing out the RDD.
                                                        Even though the RDDs are defined, they don't contain any data.
                                                Data Loading in RDD
                                                        RDD is recomputed every time when it is materialized
                                                        So, it is a good idea to improve performance by caching a RDD if it is accessed frequently
                                                        SparkContext provides parallelize function, which converts the scala collection into the RDD of the same type.		
                                                Manipulating RDDs
                                                        similar to scala collections manipulation
                                                                simple if familiar with scala's collection library.
                                                        Many of the standard functions are available directly on spark's RDDs with the primary catch being that they are immutable.
                                                                The Hallmark functions of map and reduce are available by default
                                                                The map and other spark fucntions do not transform the existing elements, they always create a new RDD with transformed elements (immutability in action!)
                                                        Programmer need not worry about the RDDs being executed on the same machine or multiple machines.
                                                        f(a,b) == f(b,a) and f(a,f(b,c)) == f(f(a,b),c)
                                                        eg- sum all elements
                                                                use rdd.reduce(x,y => x+y)
                                                                        ( or )
                                                                rdd.reduce(new Function2< Integer, Integer, Integer>() 
                                                                {
                                                                public Integer call(Integer x, Integer y) {
                                                                return x+y; } })
                                                RDD functions
                                                        foldByKey
                                                        reduceByKey
                                                        groupByKey- more details existing
                                                        lookup
                                                        mapValues
                                                        collectAsMap
                                                        countByKey
                                                        partitionBy
                                                        flatMapValues
                                                Double RDD Functions
                                                        mean
                                                        sum
                                                        variance
                                                        stats
                                                        cache
                                                        collect
                                                        count
                                                        countByValue
                                                        distinct
                                                        filter
                                                        foreach
                                                        persist
                                                        sample
                                                        toDebugString
                                                        count
                                                        unpersist
                                                        union
                                                Fault Recovery
                                                        Efficient fault recovery using lineage- log one operation to apply to many elements (lineage)
                                                                recomputed lost partitions on failure
                                                        Eg- messages = textFile(..).filter(_.contains("error"))
                                                                                                                .map(_.split('\t')(2))
                                        Spark SQL- Similar HQL.
                                                Backbone of these operations is DataFrames and SchemaRDD
                                                Interactive thru SQL, DataFrames API and Datasets API.
                                                DataFrame is a distributed collection of data organized into named columns.
                                                Conceptually equivalent to a table in a relational database.
                                                SchemaRDDs are mode of row objects along with the metadata information.
                                                Spark SQL needs SQLContext object, which is created from existing SparkContext
                                                Steps
                                                        Start Spark Shell- ./bin/spark-shell
                                                        import following packages:
                                                                val sqlContext=new org.apache.spark.sql.SQLContext(sc)
                                                                //sc being spark context.
                                                                convert implicitly RDD to DataFrames
                                                                        import sqlContext.implicits._
                                                                Import Spark SQL data types and Row:
                                                                        import org.apache.spark.sql._
                                                        Load data into a new RDD
                                                                val data=sc.textFile("Your file name")
                                                                //sc is sql context here.
                                                        Define the schema according to your dataset using a case class.
                                                                Eg- case class Product(productid: Integer, product_name: String, product_rating: Integer)
                                                                        as per the dataset
                                                                check your schema- print to console in tree form
                                                                        printSchema()
                                                        Load RDD into case class
                                                                val productData=data.map(_.split(",")).map(p=>Product(p(0).toInt,p(1),p(2).toInt))
                                                        Change productData RDD of Product objects to a DataFrame
                                                                val product=productData.toDF()
                                                        Explore and query the product data with Spark DataFrames
                                                                DataFrames provide a domain-specific language for structured data manipulation in Scala, Java and Python.
                                                        Register the DataFrame as a temp table
                                                                eg- ....product.registerTempTable("product")
                                                        Now execute the SQL statement
                                                                Eg- val results=sqlContext.sql("Select * from Product");
                                                                result.show()
                                                Eg- Name,age csv file.
                                                        sqlContext, import implicits...
                                                        case class Person(name:String,age:Int)
                                                        val people= sc.textFile("/home/user/people.txt").map(_.split(",")).map(p=>Person(p(0),p(1).toInt)).toDF()
                                                                                                                                    this path is that of hdfs not lfs
                                                        people.registerTempTable("people")
                                                        val c1=sqlContext.sql("select * from people where age > 18")
                                                        c1.collect	//give in form of array
                                                        c1.map(x=>"Name: "+x(0)+" Age: "+x(1)).collect().foreach(println)
                                                        c1.map(_.getValuesMap[Any](List("name","age"))).collect.foreach(println)
                                                                //load into map only
                                                Specifying the schema
                                                        val people=sc.textFile("/user/new.txt")
                                                                                                                                    //defaults to /user/root/people.txt in hdfs
                                                        val schemaString="name,age"
                                                        import org.apache.spark.sql.Row;
                                                        import org.apache.spark.sql.types.{StructType,StructField,StringType}
                                                        val schema=StructType(schemaString.split(",").map(fieldName=>StructField(fieldName,StringType,true)))
                                                        val rowRDD = people.map(_.split(",")).map(p=>Row(p(0),p(1).trim))
                                                        val peopleDataFrame=sqlContext.createDataFrame(rowRDD,schema)
                                                        peopleDataFrame.registerTempTable("people")
                                                        val results=sqlContext.sql("Select name from people")
                                                        results.map(t=>"Name: " + t(0)).collect().foreach(println)
                                                Reading CSV files with header (ain't working)
                                                        ./bin/spark-shell --packages com.databricks:spark-csv_2.11:1.1.0
                                                        val df = sqlContext.read.format("com.databricks.spark.csv").option("header","true").load("/Movies.csv")
                                                        df.printSchema()
                                                        :q to exit scala shell.
                                                Reading Parquet File processing with sparkSQL
                                                        JSON files
                                                        columnar storage values- data arranged so
                                                        high speed- fast performance, fast results
                                                        Make parquet file
                                                                emp.txt to parquet
                                                                        id,name,role,dept
                                                                        import org.apache.spark.rdd.RDD
                                                                        case class emp(id:Int,name:String,role:String,Dept:String)
                                                                        val r1:RDD[emp]=sc.textFile("emp.txt").map(_.split(",")).map(x=>emp(x(0).toInt,x(1),x(2),x(3)))
                                                                        val r2= r1.toDF();
                                                                        r2.saveAsParquetFile("empData.parquet")
                                                                query parquet file
                                                                        val acc=sqlContext.parquetFile("empData.parquet")
                                                                        acc.registerTempTable("parTable")
                                                                        val acc1=sqlContext.sql("select name from parTable where dept='dept1' ")
                                                                        acc1.collect()
                                                                        acc1.map(x=>"Name: "+x(0)).collect().foreach(println)


                                </Suresh>
                            </Spark>
                            <Drill> 
                                    v1.9.0
                                    Installation
                                            Embedded for windows
                                                    windows download- http://apache.mirrors.hoobly.com/drill/drill-1.10.0/apache-drill-1.10.0.tar.gz
                                                    unzip the file
                                                    cd bin, sqlline.bat -u "jdbc:drill:zk=local"
                                                    !quit
                                            single node distributed (VM)
                                                    wget http://apache.mirrors.hoobly.com/drill/drill-1.10.0/apache-drill-1.10.0.tar.gz
                                                    tar -xzvf apache-drill-1.10.0.tar.gz
                                                    drill-override.conf, use the Drill cluster ID (same across cluster), and provide ZooKeeper host names and port numbers
                                            <DrillClusterSetUp>
                                                    In master node- drill-override.conf
                                                            drill.exec: {
                                                              cluster-id: "HDP23",
                                                              zk.connect: "172.17.26.72:2181,172.17.26.73:2181,172.17.26.74:2181"
                                                            }
                                                            drill-env.sh- modify 8G to 2G memory to get it- confirm using log.
                                                                    #export DRILL_HEAP=${DRILL_HEAP:-"4G"}
                                                                    export DRILL_HEAP=1G
                                                                    # Maximum amount of direct memory to allocate to the Drillbit in the format
                                                                    # supported by -XX:MaxDirectMemorySize. Default is 8G.

                                                                    #export DRILL_MAX_DIRECT_MEMORY=${DRILL_MAX_DIRECT_MEMORY:-"8G"}
                                                                    export DRILL_MAX_DIRECT_MEMORY=2G
                                                                    # Value for the JVM -XX:MaxPermSize option for the Drillbit. Default is 512M.
                                                            ./drillbit.sh start, status
                                                                log is generated- read for error like memory could not be allotted
                                                            ./drill-conf
                                                            confirm- http://(ip):8047 works fine
                                                                storage tab- to configure hive plugins
                                                                    give all details from ambari hive. And hive databases will also show up in show databases;
                                                    In slave nodes- zookeeper servers should be installed. (zookeeper is just a delegater)
                                                        have your drillbit running in all nodes of the cluster.
                                                            checking 8047 ports of slave nodes thru web- not accessible if drill not started
                                                    Uploaded in master- tpcds_text_2- it was visible throughout the cluster in their drill machines.
                                            </DrillClusterSetUp>
                                            <working>
                                                    I fire a query to node 73
                                                        73 as a client forwards the request to designated zookeeper ip
                                                            more are given in drill-override.conf as backup- 1 is sufficient
                                                            if ip given in drill-override.conf doesn't have zookeeper then cluster can't be searched for drillbits.
                                                        which can delegate a request to all the nodes of the cluster which have drillbit
                                                    Zookeepers lost their heartbeats cuz slave nodes which had it installed were powered off- so couldn't connect to DB.
                                            </working>
                                    Start drill-
                                            bin/drillbit.sh restart
                                                    error log- /opt/apache-drill-1.8.0/log/drillbit.out- if space, change it in conf/drill-env.sh (may need to reduce for system usage is more)
                                            drill-conf/ drill-localhost, sqlline
                                            bin/drillbit.sh stop
                                    Fire Queries
                                            GUI-
                                                    web ui was not working earlier- solved by installing zookeeper separately.
                                            CLI- 
                                                    dfs- SELECT columns[0] FROM dfs.`/root/FromWin/a.csv`; 
                                                        OR (USE dfs.tesla; SELECT * FROM `a.csv`;) where tesla is configured for path
                                                        SELECT * FROM dfs.`C:\\Users\\ic071166\\Desktop\\donuts.json'- no change for config files
                                                    cp- SELECT * FROM cp.`employee.json` LIMIT 20;
                                                    hive plugin-
                                                        select * from hive.geo.`geolocation` g, hive.geo.`trucks` h where g.driverid = h.driverid limit 10;
                                                        select * from hive.geo.`geolocation` g, hive.geo.`trucks` h where g.truckid = h.truckid limit 10;
                                                        select * from geo.geolocation g, trucks.truck_event_text_partition t where CAST(SUBSTRING(g.driverid,2,3) AS INT) % 5 = t.driverid % 5;
                                                        select g.truckid, g.driverid, count(g.city), t.model from hive.geo.`geolocation` g, hive.geo.`trucks` t where g.truckid = t.truckid group by g.truckid, g.driverid, t.model having count(*) > 3;

                                            Notes
                                                    query from ida_vsc_test.tablename doesn't show result if already using ida_vsc_test
                                                    preparation for hive queries- 
                                                            whenever from- use regex to prepend the syntax. why? drill syntax to run hive queries- allowed keywords.
                                                            show databases results
                                                            search for (DB).(a-z) ck ---> convert to hive.(DB).`(a-z)` ck
                                                            all alias with integer beginning but use keyword AS.
                                    Cancel Query
                                            Cancel a drill query- from profiles in webUI- edit query
                                    !quit or !q
                                    JDBC- maven dependency doesn't work says classes clash- so used .jar
                                    Errors
                                            Drillbit down error
                                                    query execution still going on while we start to collect metric
                                                    Drillbit session is over- restart Drill sesssion, Ambari, VM.
                                                    Drillbit down error comes when heavy query and you invoke close connection.
                                            Unable to connect to drillbit
                                                    Queries run in parallel on shell but on code they want gap and we want gap.
                                                    Done- Make sleep configurable in properties file- Thread.sleep cuz eclipse makes asynchronous sql call.
                                    Queries Status- REST API/ WebUI > Profiles
                                            Long queries not visible- getting truncated in RESTAPI output- different map created

                                    <DrillExploration>
                                            Confirming speed- set up another single-node distributed on 172.17.26.71 using settings as above but don't know how to use hive storage plugin thru cmd line.
                                                    cp.employee- 0.615 sec vs 1.052 vs 0.495 (cluster vs VM)
                                                    query3 (tpcds)- 21.618 vs 74.84  vs 26 sec(cluster vs VM vs single-node)
                                                    query7- 31.047 vs 58.176 vs 28.174 (single)
                                                    query12- 7.358 vs 17.52 vs 6.092
                                                    query15- 14.806 vs 8.77 vs 8.72
                                                    query17- 27.171 vs 21.801 vs 24.385
                                                    Priyansh query- 53.683/42 vs  vs 43.051/34
                                                    query19- 22.7 vs 27 vs 33
                                                    query20- 5.328 vs 9.52 vs 9.526
                                                    query21- 12.404 vs 16.16 vs 21.231
                                                    Hence demonstrated.//

                                                    added "hive.metastore.cache-ttl-seconds": "0",
                                                            "hive.metastore.cache-expire-after": "access" for no caching- standard deviation in results = , average
                                    </DrillExploration>
                            </Drill>
                            <Presto>
                            Presto- version 0.170
                                    configured as per documentation
                                    Unrecognized VM option 'ExitOnOutOfMemoryError'- remove this key from jvm.properties
                                    Exception in thread "main" java.lang.UnsupportedClassVersionError: com/facebook/- it works with java 8 since version 0.86- check your java version, update
                                    launcher start was claiming to have started but run (foreground) was throwing exception
                                            java.net.BindException: Address already in use
                                            in linux- netstat -plten to check listening ports and windows- netstat -ano|findstr 80 (confirm!)
                                            because it was using port 8080 which is also used by ambari- changed to 8081 in config.properties
                                            OR use kill -9 9488 to force stop the process using the port.
                                            It could also be possible that previous run's daemon hasn't stopped.
                                    jdbc:presto://example.net:8080/hive/sales
                                    no viable alternative- cuz show databases, etc are not acceptable commands on presto
                                            try- create table test (a varchar); and show schemas;
                                    Updation of java from 7 to 8 messed up all other services- God!!
                                            after updating java_home in /etc/profile- we started to see echo $JAVA_HOME in different command shell (not old ones)
                                            after updating hadoop-env.sh we start to get ambari-server but hbase and hive services won't start from gui, lui. complain that java not found on /usr/lib/java path.
                                            after copying the same java to that specified path- services can be turned on from ambari but their shell don't work. 
                                                    hive won't open at all 
                                                    hbase gives exception
                                                            ERROR [main] zookeeper.ZooKeeperWatcher: hconnection-0x3b5c665c0x0, quorum=sandbox.hortonworks.com:2181, baseZNode=/hbase-unsecure Received unexpected KeeperException, re-throwing exception
                                                    waited for a long time
                                            used ambari to restart many stopped services. Some would simply deny to turn on.
                                            starting zookeeper started hbase in gui but not its client cli shell
                                            restarting atlas started hbase well in cli shell also and also hive shell, also yarn web ui.
                                    There is a gui for presto at http://192.168.237.137:port- for view details of cluster- useless otherwise.
                                            click finished button to see finished queries.
                                            webui for presto- https://github.com/airbnb/airpal
                                    In presto-cli, it is important to specify the --server option, else it doesn't work
                                            Presto from local file
                                                    https://prestodb.io/docs/current/connector/localfile.html
                                                    Google: can presto connect to s3
                                    Installing Airpal
                                            gradlew- on windows threw error cuz it was meant for linux system- it was gradlew.bat for windows
                                    Installing Superset
                                            virtualenv -p /path/to/python venv for versions >= 2.6 for superset
                                                    gives cryptography install error but continues
                                                    or using python3.4's pyvenv- could not find location error doesn't come.
                                                            this gives not error at all.
                                                            Was unable to import superset Error: No module named 'pysqlite2'
                                                            Installing pysqlite 2.5.5 thru setup.py- error that print x instead of print(x)- version!
                                                            for v > python 2.5, working version of pysqlite 2, bundled as sqlite3
                                                    Create your virtualenv with --no-site-packages if you don't want it to be able to use external libraries
                                    Speeding up Presto
                                            specify requisite columns only (being columnar storage) instead of *
                                            specify time in where clause (hourly bucketing=> avoid unnecessary buckets)
                                                    fails with float, BETWEEN, expressions etc
                                            Select in order of high cardinality (group by uid, gender)
                                            use limit with order by- cuz order by is very costly in memory
                                            using approximate aggregator function when possible- standard error of 2.3% for optimization- approx_distinct(user_id)
                                            replace many LIKE in where clause by regexp_like(colName,'(expression)')- compare in 1 go.
                                            Specify large table first in a join- broadcast join partitions the lhs of join and rhs of join is broadcasted to all worker nodes having partitions. So save network and Exceeded max memory error.
                                                    vs distributed hash join- if memory error- partitions both tables- using hash values of join keys- compromises speed and more network load.
                                                    -- set session distributed_join = 'true'	embedded as sql comment.
                                            Prefer rank() over row_number() in ordered
                                            use ORC file format with SNAPPY compression- gives a big difference
                                                    DROP table if exists nation_orc;
                                                    CREATE table nation_orc like nation;
                                                    ALTER table nation_orc set fileformat orc;
                                                    ALTER table nation_orc set tblproperties ("orc.compress"="SNAPPY");
                                                    SET hive.exec.dynamic.partition = true;
                                                    SET hive.exec.dynamic.partition.mode = nonstrict;
                                                    INSERT INTO table nation_orc partition(p) SELECT * FROM nation;
                                                    select col from nation_orc limit 2;
                                            Keeping data sorted along column frequently appearing in where clause
                                                    INSERT INTO table nation_orc partition(p) SELECT * FROM nation SORT BY n_name;
                                                    SELECT count(*) FROM nation_orc WHERE n_name=AUSTRALIA;
                                            Presto- config.properties
                                                    task.max-worker-threads=greater number at cost of space
                                            Interesting links
                                            4 more
                                                    http://docs.qubole.com/en/latest/user-guide/presto/best-practices.html
                                                    https://www.youtube.com/watch?v=RggCAXBe6BA
                                    had to restart hdfs individually to get hive queries running after switched to jdk8 (atlas restart helps)
                                            Query is gone (server restarted?) why? cuz could not give 2GB memory requirement to query- updated in config.properties for 3 from 1 GB.
                                    Python connector for Presto
                                            Source code and docs
                                                    https://github.com/dropbox/PyHive
                                            ORC file format, etc
                                            cp -r /usr/local/java/jdk1.8.0_131 java to update hive in /usr/lib/jvm.
                                            Presto
                                                    easy_install.exe requests
                                                    python setup.py install
                                                            python inside classes
                                                    pip is at a location in scripts inside python installation directory available over there.
                                                    Installed alchemy
                                                            C:\Python27\python.exe .\setup.py install after downloading it from github- https://github.com/zzzeek/sqlalchemy/
                                                                    if you have installed the package but still working on old ide instance- it may still not detect, restart is required.
                                                            from sqlalchemy import *
                                                            from sqlalchemy.engine import create_engine
                                                            from sqlalchemy.schema import *
                                                            engine = create_engine('presto://192.168.237.140:8081/hive/tpcds_text_2')
                                                            logs = Table('customer', MetaData(bind=engine), autoload=True)
                                                            print select([func.count('*')], from_obj=logs).scalar()
                                                    import pyhive happening but not from pyhive import presto- opened presto.py and ran all imports 1 by 1- found out the culprit.
                                                    Installing packages
                                                            installing sasl
                                                                    download sasl-0.1.3-cp27-none-win_amd64.whl and run pip install sasl-0.1.3-cp27-none-win_amd64.whl
                                                            Install pip on windows (untested)
                                                                    pip comes installed after 2.7 and for b4
                                                                    Install setuptools:
                                                                            curl https://bootstrap.pypa.io/ez_setup.py | python
                                                                            Install pip:
                                                                            curl https://bootstrap.pypa.io/get-pip.py | python
                                                                            Optionally, you can add the path to your environment so that you can use pip anywhere. It's somewhere like C:\Python33\Scripts
                                                                    OR 	python get-pip.py
                                                            Create Virtual Envt (diff envt (versions) for diff project- but not write projects inside)
                                                                    pip list- to get all installed packages for global site.
                                                                    virtualenv project1_env
                                                                            for different python version to be used= say -p "path to python"
                                                                    source project1_env/bin/activate
                                                                            takes us inside new python envt (evident thru prompt)
                                                                            which python- it's inside of envt
                                                                            pip list- nth inside.
                                                                            pip install numpy; pip list
                                                                            pip freeze --local > requirement.txt (only local packages to be used)
                                                                                    later for bulk install- pip install -r requirements.txt
                                                                            deactivate; which python
                                                                            rm -rf project1_env

                                                    Installation
                                                            pip install presto-python-client
                                                                    didn't like pyhive cuz that was disgusting.
                                                            from prestodb.dbapi import Connection
                                                                    or replace from import by just import prestodb and use full prestodb.dbapi.Connection(...)
                                                            conn = Connection(host="192.168.237.140",port=8081)
                                                                    vs conn = prestodb.dbapi.Connection(host=192.168.237.140,port=8081)
                                                            conn = Connection(host="192.168.237.140",port=8081,user="root",catalog="hive",schema="tpcds_text_2")
                                                                    without user it complained that we should have a user. catalog was being complained- schema was not but query can have dbname.
                                                                    had to figure out on our own.
                                                            cur = conn.cursor()
                                                            cur.execute('select * from customer limit 5')
                                                                    until schema was found I preferred 'show schemas' as master statement or system.
                                                            rows = cur.fetchall()
                                                                    fetchone()
                                                            print rows
                                                            for x in rows:
                                                                    print x
                                                                    //(\n) for answer
                                Presto by REST API
                                    GET
                                            Ref- https://github.com/prestodb/presto/blob/master/presto-docs/src/main/sphinx/rest/query.rst (obtained thru issues section of presto)- people give prompt reply
                                            GET /v1/query
                                            GET /v1/query/{queryId}
                                    /health- ok
                                    Post
                                    /v1/statement
                                            headers- 
                                                    Content-Type	text/plain
                                                            no error if you remove it
                                                    x-presto-catalog	hive
                                                            schema is set catalog is not
                                                    x-presto-schema	default
                                                            works when schema is factually default; otherwise 200- with error stated inside; if full query is specified then QUEUED result is returned.
                                                    x-presto-user	presto
                                                            user must be set
                                            Payload- write the query to be fired.
                                            Get for this returns 405- method not allowed (cuz it is not defined)
                                            gotta do web scraping.
                                            https://github.com/prestodb/presto/wiki/HTTP-Protocol
                                            https://groups.google.com/forum/#!topic/presto-users/A0iJGeeOGKk
                                Presto's hive connector
                                    hive can read thru hierarchy but not presto
                                    In partitioning case, presto can read thru hierarchy cuz refers to hive metastore.
                                https://docs.treasuredata.com/articles/presto-performance-tuning
                                Superset documentation
                                    Ubuntu installation worked
                                        installed openssh server- ubuntu doesn't have by default
                                        nmap and checked- 22 is open, it was.
                                        sudo apt install virtualenv
                                            pip install virtualenv didn't work but apt install did, way till full installation
                            </Presto>
                            <Kafka>
                                <KafkaAndZookeeperWindowsInstall>
                                Kafka and zookeeper on windows
                                        JDK setup
                                        Download latest zookeeper from http://zookeeper.apache.org/releases.html and unzip to C: using 7-zip
                                                rename zoo_sample.cfg to zoo.cfg
                                                .\conf\zoo.cfg
                                                        edit dataDir=/tmp/zookeeper to :\zookeeper-3.4.7\data
                                                        port=2181
                                                System Variables
                                                        ZOOKEEPER_HOME = C:\zookeeper-3.4.7
                                                        Path += ;%ZOOKEEPER_HOME%\bin;
                                                cmd- zkserver
                                        Download latest kafka from http://kafka.apache.org/downloads.html and unzip to C: using 7-zip
                                                .\config\server.properties
                                                        edit "log.dirs=/tmp/kafka-logs to log.dir= C:\kafka_2.11-0.9.0.0\kafka-logs"
                                                        zookeeper.connect=localhost:2181
                                                        port=9092
                                                ensure the zookeeper running
                                                C:\kafka_2.12-0.10.2.0 cmd- .\bin\windows\kafka-server-start.bat .\config\server.properties
                                        Consumer producer
                                                C:\kafka_2.12-0.10.2.0\bin\windows cmd-
                                                        kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
                                                        kafka-console-producer.bat --broker-list localhost:9092 --topic test
                                                        kafka-console-consumer.bat --zookeeper 132.186.226.177:2181 --topic test
                                </KafkaAndZookeeperWindowsInstall>
                                Plans- Introduction of data from kafka to cassandra
                                        https://github.com/datamountaineer/docs/blob/master/source/cassandra-sink.rst
                                        http://docs.datamountaineer.com/en/latest/cassandra-sink.html
                                        https://github.com/datamountaineer/docs/blob/master/source/cassandra-source.rst
                            </Kafka>
                            <Ambari>
                                Ambari- Hive, REST API (Time duration- fork bomb demonstrated)
                                admin admin for ambari on sandbox
                                set rest API domain names in C:\Windows\System32\drivers\etc- hosts
                                Ambari is a manager. HIVE Execute the query- REST API answered by YARN
                                Need to start ambari completely to access the resources- metrics collector should be on to collect metrics.
                                https://cwiki.apache.org/confluence/display/AMBARI/Metrics+Collector+API+Specification
                                service ambari stop- from command prompt to start, stop, status viewing.
                                Cluster
                                        open ambari on master ip-
                                                heartbeat doesn't exist cuz ambari-agent are not active in slave nodes
                                        sudo ambari-agent start
                                        cd /usr/sbin/
                                        ./ambari-agent start
                                        start slave nodes as well following same way
                                                ssh ip
                                                ctrl+D to end the session
                                        in master
                                                etc/hbase/conf- get hbase-site.xml
                            </Ambari>
                        </Tools>
                </Components>
                        <SureshNotes>
                            To be merged with the main and practical remains
                            BIG DATA
                            Huge amount of data- beyond storage capacity of any memory and processor
                            Characteristics
                                    Volume- Size of data
                                            Eg- 
                                                    Google- 200 PB/day
                                                    FB- 15-20 PB/day
                                                    Twitter-
                                                    eBay- 10 PB/day
                                                    Air booking- 20 TB/hr
                                                    e-Goverance- increasing data. Every country talks about it.
                                            now and will grow in future.
                                                    exponentially increasing rate.
                                                    90% of all data in last 5 years as in 2016.
                                                    what can you say about 2020 (it should increase)
                                            can't control generation but storage is concern.
                                                    traditional storage we have GB to TB- but not beyond.
                                    Velocity- Speed of analyse/processing
                                            100 records/ 30 sec capability of processing of machine
                                            if I give 1M records? it will take hours.
                                            Google wants their search results in least possible time.
                                    Variety- types of data
                                            Data is generated from a variety of sources.
                                                    searching, social media, surfing, e-Goverance, healthcare, IOT.
                                            Structured Data- tables using rdbms- only 10% of total data.
                                            Unstructured Data- images, vids, docs, etc. Mostly this data prevails.
                                            Semi-structured data- xml, excel, etc
                                    Veracity- consistency of data
                            Details
                                    Distributed framework
                                    for storage and processing
                                    of huge amount of data
                                    running on commodity h/w
                                    written in java
                                    supports all 4V's
                                    Prerequisites
                                            Companies prefer linux implementations.
                                            OOPS- java; sql.
                                    Maintainable
                                            it provides a scalable & fault-tolerant (+/- easy handling)
                                            nodes can be added as per needs.
                                            achieved by replication of data blocks across nodes.
                                    History
                                            2005 Doug Cuttings team used it at yahoo for nutch. Later made it OSS.
                                            3 versions have come out- But now apache is main contributor to Hadoop ecosystem.
                            Components
                                    HDFS- for Storage
                                            maintains scalability, availability and its architecture.
                                            Characteristics
                                                    Fault Tolerant
                                                    High Scalability
                                                    High Availability
                                                    Reliability- Streaming not random.
                                            Scenario
                                                    say 1 Master and 4 slaves in DFS (any distributed file system)
                                                    If S2 goes down- go to cluster and use node recovery management system to rectify.
                                                    DFS required high configuration nodes => costly.
                                                    Not so with HDFS.
                                                    Here also Master slave concept
                                                            Master is called Name Node
                                                                    does authentication and managing all DNs.
                                                                    All file system operations like creation, deletion, updation, access- does issues tracking/file tracking
                                                                    If I want to store file1.txt in my hadoop cluster- I will ask NN to do it for me.
                                                                    NN divides file into blocks of size 64MB (128 MB for production, configurable) and distributed across DNs (say b1 in DN1 and b2 in DN2) and reported to NN where they are stored (metadata)
                                                                    Eg- if 200 MB to be stored and block size is 64 MB- we need 4 blocks.
                                                                    If DN2 goes down- file1.txt may not be accessible wholly (b2 of file1.txt not there)- so replications are maintained- at least 3 are maintained (b1, b1-copy1, b1-copy2; b2, b2-copy1,b2-copy2)
                                                                    so now that DN2 is down, use other DN which has b1-copy1 <= Metadata will also take care of replications
                                                                    DNs are organized in form of racks (1 rack contains 40 nodes generally)- say DN1,DN2 in rack1 and DN3,DN4 in rack2. At least 1 copy of a file block has to be in different rack (rack awareness)
                                                                    Eg- consider following scenario when DN2 goes down
                                                                            DN1- b1; DN2- b2, b1-copy1; DN3- b1-copy2
                                                                            DN4- b2-copy2, DN5- b2-copy1
                                                                            The request will immediately go to b2-copy1, i.e DN5 to fetch for file request.
                                                                            DN6 is given charge of b2, b1-copy1 as DN2 is lost!!
                                                                    NN retains metadata in files called .fsImage and Editlog
                                                                    Secondary Name Node (SNN) also there- came in Hadoop 1.x (SNN can start acting as NN but lost is lost- edits retained) vs standby.
                                                            Slave is called Data Node (Data files distribute across DN)
                                            1.x vs 2.x
                                                    Storage in NN, SNN, DN
                                                    2.x has Standby NN- shared metadata with NN. So if NN is down- StNN takes over without loss using checkpointing mechanism- StNN can act as Checkpointing NN- holds information of metadata copying on periodic basis.
                                                    Processing in JT, TT.
                                                    2.x has JT divided into Resource Manager in NN and Node Manager in DN.
                                                    2.x has YARN added to it for faster performance and throughputs.
                                                    1.x had 1 NN and Namespace but 2.x can have more
                                                            1 NN gives a lot of load on JT- let's distribute that.
                                                            And if that NN gets down then we have issues with accessing metadata.
                                                            3 independent requests types can be maintained in 3 different namespaces- this is called Hadoop Federation.
                                                    2.x has racks concept.
                                                    2.x has high availability- when all NN have shared NN metadata then easy access the DN- shared metadata.
                                                    Horizontal scalability supported in 2.x but 1.x had only vertical one.
                                                    2.x also has application master and capacity scheduler.
                                                            for single application maintaining checkpoints- 1 manager for that resource.
                                                            capacity scheduler- as per user requirements, give resources to applications.
                                                    Fencing- Giving editting power to the metadata- only 1 NN can write at a time- others will read.
                                            Being famous, many companies are giving support (developing components and giving features) of hadoop- CDH, HDP, MapR. 	Earlier only 1 contributor.
                                            Hadoop HDFS- NN b4 giving tasks- sees to health of the TT node. Updating metadata information in .fsimage is called checkpointing mechanism.
                                            Heartbeat- for NN to know which all DN are available for fulfilling a request- checked by getting ACK from DN.
                                            Hadoop Cluster only works with LINUX (preferable), Mac, Windows, SunSolaris.
                                            Hadoop modes of environment
                                                    standalone- eth in in 1 JVM only- NN, JT, etc.
                                                    pseudodistributed- every DN in separate JVM's.
                                                    fully distributed- Real production system- different nodes having installations with different JVMs.
                                            Commands
                                                    Hadoop versions
                                                    hadoop fs -help
                                                    hadoop fs -ls
                                                            unable to load hadoop library- cuz daemons are not started.
                                                            jps- to check daemons working
                                                            start-all.sh- to start all the daemons in hdfs.
                                                                    has 2 components- start-dfs.sh and start-yarn.sh
                                                                    stop-all.sh
                                                                    jps to confirm
                                                    hadoop fs -ls /
                                                    hadoop fs -mkdir dir1
                                                    hadoop fs moveFromLocal
                                                            LFS to HDFS- put/copyFromLocal
                                                            HDFS to LFS- get/copyToLocal
                                                            moveFromLocal- deletes from LFS and adds to hdfs.
                                                            moveToLocal doesn't exist.
                                                            copy file into hadoop envt
                                                            -cp/-mv is within hdfs.
                                                            hadoop fs -put first.txt /hadoopFolder1
                                                            hadoop fs -ls hadoopFolder1
                                                            hadoop fs -cat hadoopFolder1/first.txt
                                                            hadoop fs -get /hadoopFolder1/first.txt ~/home
                                                            to change content in lfs- use vi insert mode.
                                                            can't do changing hdfs- hdfs is not meant for that- it's not interactive but batch- no insert but load (WORA)- so get and then put.
                                                            hadoop fs -rm /hadoopFolder1/first.txt
                                                            hadoop fs -rmdir /hadoopFolder1
                                                            hadoop fs -copyFromLocal second.txt /hadoopFolder1
                                                            safe mode commands
                                                                    hadoop dfsadmin -safemode get
                                                                    like sleep mode- hdfs storage accessed
                                                                    hadoop fsadmin -safemode enter
                                                                    cannot create file/hadoopFolder1/first.txt cuz NN is in safemode.
                                                                    hadoop dfsadmin -safemode leave
                                                                    hadoop/etc/hadoop/hdfs-site.xml
                                                                            tells replication- 1 as 1 node.
                                                                            check contents of start-all.sh in bin directory- has start-dfs.sh and yarn.
                                                            1.x start-all.sh starts
                                                                    NN, DN, SNN, JT, TT
                                                            2.x
                                                                    NN, DN, SNN (DFS), Yarn, RM, NM (YARN)
                                    YARN- helps manage cluster resources (from 2.x)
                                    Mapreduce- for processing
                                            uses java (learning can better serve clients)
                                            JT finds from NN the locations and invokes TT accordingly.
                                            If TT is down, reassignment of task to another node is called Speculative execution.
                                            There is periodic response from TT to JT that they are alive/active/available (say 10 mins, hourly, etc)- aka Heartbeat mechanism.
                                            Does Processing of huge data.
                                            write logic as per client's request in java
                                            2 user stages/steps involved for parallel processing
                                            Mapper- map business logic to where data is residing.
                                            Sort and Shuffle (SNS)- no logic required from user- it's inbuilt.
                                            Reducer- combine the results of individual nodes.
                                            It's only the Key Value (KV) pairs that go along all the stages/steps
                                                    Except SNS which generates in Key and value list pair.
                                                    There is internal mechanism to convert data into KV pairs for Mapper- byte offset value for each line.
                                                    We give data to NN- never to DNs. Request may look like- we are asking NN to perform wordcount on my.txt
                                                    NN looks up for which data nodes have my.txt for block locations. If DN is inactive, then speculative execution.
                                                    DN invokes TT which finds the block and says yes found the block.
                                                    NN transforms all the logic to suit DN1's data block's processing.
                                                    Mapper gives output of the text given as (byteOffset, line) to KV of (word and found flag integer) form with repetitions as per data.
                                                    #Mappers = #inputSplits of the file. Mappers produce their outputs and while data in their RAM- tells JT that it's done. NN starts SNT. Split size configured in hadoop file.
                                                    Sort and Shuffle- sorts all the keys and pairs them up at Reducer location.
                                                    After SNS- NN starts Reducer on node.
                                                    Reducer finally adds them up to write number of times every word repeats.
                                                    #Reducer is configurable- MR to R1, Hadoop to R2, etc also.
                                            Practical
                                                    Make file MapperInput.txt- write some interesting text in there.
                                                    Data types in MR
                                                            int- IntWritable
                                                            float- FlatWritable
                                                            String- Text
                                                    Copy Mapper.java and add hadoop jars- and errors are gone
                                                            hadoop-ant-0.20.2-cdh3u5
                                                            hadoop-core-0.20.2-cdh3u5
                                                            hadoop-examples-0.20.2-cdh3u5
                                                    Mapper code- 
                                                            import java.io.IOException;
                                                            import java.util.StringTokenizer;
                                                            import org.apache.hadoop.conf.Configuration;
                                                            import org.apache.hadoop.fs.Path;
                                                            import org.apache.hadoop.io.IntWritable;
                                                            import org.apache.hadoop.io.Text;
                                                            import org.apache.hadoop.mapreduce.Job;
                                                            import org.apache.hadoop.mapreduce.Mapper;
                                                            import org.apache.hadoop.mapreduce.Reducer;
                                                            import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
                                                            import org.apache.hadoop.mapreduce.lib.output.
                                                            Class WordCount {
                                                                    class MapperEx extends Mapper< ipkey, ipvalue,opkey, opvalue > {
                                                                            1 way of breaking strings is stringTokenizer.
                                                                            map(object key, Text value, Context context) {
                                                                                    //overridden mapper logic
                                                                            }
                                                                    }
                                                                    class ReducerEx extends Reducer< Text, List< Value>, Text, Value> {
                                                                            reduce(key, value, context) {
                                                                                    //overridden reducer logic
                                                                            }
                                                                    }
                                                                    public static void main(String[] args) {
                                                                            //driver logic
                                                                            //configuration in new api- previous is deprecated
                                                                            //you can write more jobs in this.
                                                                            //outputkeyclass is output of reducer format.
                                                                            //combiner class- done after mapper and b4 SNS- to all mapper outputs concatenated.
                                                                                    it makes this,1 twice in mapper to this,2 (saving bandwidth of network but combiner doesn't combine across mapper)
                                                                    }
                                                            }
                                            Execution
                                                    Make a jar file- copy to VM
                                                    hadoop fs -mkdir MR
                                                    vi ip.txt
                                                    copy to hdfs
                                                    hadoop jar WordCount.jar WordCount /pathToInputFile/ip.txt /pathToOutputFiles
                                                    outputs of mapper files and reducer files as part-m-00000 and part-r-00000 respectivly- max 100000 files can be generated.
                                                    _SUCCESS is flag for output and may have logs.
                                                    the jar file file for hadoop command is in LFS.
                                                    For line number and #words- use LongWritable,Text, Text,IntWritable as mapper generics.
                                                            ignore the key passed and use line + lineno++; for outputkey added to context object.
                                            Errors
                                                    Unsupported major.minor version 52.0
                                                            Seems like you are using JDK8 for compiling and Lower version on Where you are using it
                                                    Class not found
                                                            ensure that full path name to class is being specified- including package.
                                            Partitioner
                                                    go to a reducer as per criteria
                                                            age (0,18),(18,50),(50-INF)
                                                            Here 3 reducer- and in code job.setNumReducerTasks(3);
                                                            can't setNumMapper()- no, cuz based on input splits.
                                                            if you want no reducer output- set it to 0.
                                                            Partitioner executes b4 Combiner.
                                            Compression-
                                                    Compression in MR- 
                                                            reduces number of bytes written/read from/to HDFS.
                                                            Compression effectively improved the efficiency of network Bandwidth.
                                                            saves the amount of data being transfered between map nodes to reduce nodes.
                                                            By default, compression is not enabled in MR- value is false.
                                                            mapred-site.xml
                                                                    <property>
                                                                            <name>mapred.output.compress</name>
                                                                            <value>false</value>
                                                                    </property>
                                                                    <property>
                                                                            <name>mapred.output.compression.codec</name>
                                                                            <value>org.apache.hadoop.io.compression.DefaultCodec</value>
                                                                    </property>
                                                    Types
                                                            LZO- CompressionCodec- com.hadoop.compression.lzo.lzopCodec
                                                            com.hadoop.compression.lzoCodec;
                                                            com.hadoop.compression.DefaultCodec;
                                                            com.hadoop.compression.SnappyCodec;
                                                            quite fast. Codec means compression-decompression.
                                                            Fast decompression.
                                                            Requires additional buffer during the compression- as per compression level (8/64 KB)
                                                            no additional buffer during decompression other than source destination => fast.
                                                            Allows the user to adjust the 
                                                            GZIP- reasonable compression and decompression.
                                                            Snappy- faster compression and decompression levels.
                                                            bGIP2- compression in splittable program.
                                                                            compression-decompression
                                                            LZO-reasonable-faster
                                                            GZIP-reasonable-reasonable
                                                            Snappy-faster-faster
                                                            public class SnappyDriver {
                                                                    public static void main(String[] args) {
                                                                            if(args.length != 2) {
                                                                                    System.err.println("Usage: FileDemo: < input path > < output path >");
                                                                                    System.exit(1);
                                                                            }
                                                                            JobConf conf = new JobConf(SnappyDriver.class);
                                                                            conf.setJobName("CompressionSnappy");
                                                                            FileInputFormat.addInputPath(conf, new Path(args[0]));
                                                                            FileOutputFormat.setOutputPath(conf, new Path(args[1]));
                                                                            conf.setMapperClass(SnappyMapper.class);
                                                                            conf.setReducerClass(SnappyReducer.class);
                                                                            conf.setBoolean("mapred.output.compress", true);
                                                                            conf.setClass("mapred.output.compression.codec", SnappyCodec.class, CompressionCodec.class);
                                                                            conf.setOutputKeyClass(Text.class);
                                                                            conf.setOutputValueClass(IntWritable.class);
                                                                            JobClient.runJob(conf);
                                                                    }
                                                            }
                                                            Need CompressCodec jar file- made in wasys similar to b4.
                                                            and a file- batsman score- Sachin 20 against Australia on 22/02/2010
                                                            ....
                                                            in mapper we just pass as output- playername, score.
                                                            after MR is run- check it out- the output file has extension .gz- print it to check if it's readable
                                                            compress to normal by hadoop fs -text /MRComp/part-00000.gz
                                                            bGIP2-Splittable faster- faster
                                                    By default makes 4 attemps to a mapper node.
                                                    status of hdfs- hdfs dfsadmin -report
                                                    To check number of blocks assigned to file- hadoop fsch -block /hdfsFolder1/BatsmanScores.txt
                                                    -block not needed from 2.x- gives error if you specify
                                                    1 block- 1 replication factore cuz 1 node only.
                                                    hadoop fsch /hdfsFolder1/BatsmanScores.txt

                                    Tools- 
                                            Pig- process data using commands
                                            Hive- maintains DWH on hadoop- uses SQL type.
                                            Flume- want data in live stream manner. does real time analytics on it.
                                            OOZIE- maintains work flows. helps scheduling.
                                            HBase- NoSQL DB for Hadoop uses random data access; columnal DB.
                                                    any data block randomly.
                                            Spark- higher performance than MR- in-memory access to data.
                                                    every company looks at it these days- uses scala, java, etc.
                                            Kafka
                                            Storm
                                            SQOOP- data exchange betw hadoop and RDBMS (Export/Import)
                                            Ambari- to view hadoop envt (abstraction)

                        </SureshNotes>
                        <PreinstalledVM>
                            <HDP2x>
                            HDP2.5
                            tries searching for files of name hbase and got /var/....gibberish- cuz it's in docker
                            docker- command responds thus we are using docker for all services.
                                    docker images, docker service
                                    docker run -it sandbox- takes me inside the docker (sandbox)
                                    after this, hbase shell and hive, hadoop- all start responding but with exception
                                    Solution- enter HDP2.5 with port 2222 instead of 22 and it works.
                            VMware player (12.5.1), Oracle virtualBox (5.1.14)
                                Virtualization software
                                        host os
                                        guestvirtual machines- name and its os (guest os)
                                        what's the name- host os has ________ the guest os.
                                        virtual manager should work.
                                        import ova format 
                                                in oracle virtualbox using menubar- file- import appliance- file name and settings.
                                                        VMware player was not importing ova file- repaired the software again- it started working
                                                in vmware player, open the virtual machine
                                        open exiting files for vm- 
                                                virtualbox- new vm- existing vm- location.
                                                vm ware- ...
                                        create new vm using iso
                                                virtualbox- new vm- create all settings- launch vm- choose location.
                                                vmware- new vm
                                        Oracle virtual box- ova files- how to export and import
                                        import- file - import - choose ova file and settings.

                                        Installing virtualbox and vmware inside of linux os. nesting is allowed.
                                        OS wise details- like java installation.
                                Centos for HDP 2.4 on NAT
                                        don't change network ports while running centos on vm- it doesn't detect the change
                                sharing folder in vm- player/mangage/settings/options/shared folders/always enabled/add/wizard-next/hostPath of sharedFolderName (create in desktop)/next/enable
                                        gets shared in /mnt/hgfs/sharedFolderName

                            </HDP2x>
                        </PreinstalledVM>
                </Hadoop>
            </BigData>
        </Types>
        <Mapping>

        </Mapping>
        <Usecases>
            <EnterpriseArchitecture>
                The Big Picture
                        Enterprise- an org which has a lot of diffn fns running.
                                Eg- Bank- Multiple aspects of bank- Loans, manages emp,  cust, financial aspects.
                                Eg- Govt- Adhar, BangaluruOne, e-Governance, IT.
                        1How to describe this big elephant blindfolded. Some structured way to understand, in IT conext.
                        Architecture- Set of principles to make designs.
                        Enterprise architect knows in and out of technical line and Biz.

                        EAF- vocab to describe an enterprise.
                        Eg- Zachman, ToGAF
                        Zachman- a 6X6 cells.
                                6 cols- What, How, Where, Who, When, Why.
                                6 rows- 
                                        Contextual- CEO level. I want to bring this new product- why, how, where, when.
                                        Conceptual- Scratch heads and discuss with Contextual- check feasibility,etc.
                                                what you want milk or PCs- we may need more this OS. Such questions have impact on PS.
                                                How? You want Mobile apps or web-sites, sms.
                                                Where- state level or country.
                                                        Readiness across India- to do the project. Infrastructure readiness.
                                                Who- Who is target audience. Inside who, have expertise in area.
                                                When- What time it should come.
                                                Why? why on earth- nth comes for free. Purpose? Objective- measure against it.
                                                        all below layers will shape according to decisions here.
                                        Logical- Designing business process models. How aspect of that.
                                                Start giving shape and preparing eco-system for development.
                                        Physical- Actual devp of infrastructure, ecosystem.
                                                Eg- Delivery boys of flipkart need gadgets to locate a place.
                                        As built- Cumulative current system- our data center, offices. Domains, technologies,etc.
                                        Functioning- BPO to take call answering queries complaints, where are we in current state of affairs- 
                                                what system look like when working- its behavior.
                        Generally devp and testing given to different companies to avoid bias and standard for devp be shared- so model comes in picture.
                                in DB, data model- how logically data is stored. Introduces abstraction.
                                Tells how data is processed and stored in system.
            </EnterpriseArchitecture>
            <GIS>
                Street maps along with their depths helps a lot
                        in identifying the areas affected in Nepal for relief help.
                        in identifying water-logged areas in Chennai and informing people.
                Transportation
                        Fleet Scheduling
                                Eg- FLight, trains, KSRTC (8K), BMTC(7K)
                                        Money when they run- people pay for it.
                                        Means maximize their profits->increase their runtime.
                                        2 types of customers-
                                                Commuters- Daily travel at fixed time from home to office,workplace,school
                                                        Huge demand at peak hours; idle otherwise.
                                                        not all at one place and times also variation- meaning wait for the first person be minimized.
                                                Floating population
                                                        Scheduling of public transport seeing the demand pattern betw A and B.
                        Ride Sharing
                                Ola, Uber- increase their rates in peak hours and encourage sharing by giving 50% off.
                                For different clients, different routes may be reqd.
                                Wait of first person be minimized.
                                Cost depends on route- if longer route to my home giving me 50% off is costly, better alone.
                                Amazon, flipkart have to plan the route of their courier servers- maximum distance in limited hours and maximum delivery.
                                        They even ask time of their preference and cost a little more- this complicates a little.
                        Keyhole Mapping
                                KML extension of Google Maps- company acquired by Google.
                                We navigate thru landmarks in India- that temple take left.
                                If finding a route- all junctions are nodes and edge emanate with restrictions- Dijkstra has directional edge support.
                        Traffic
                                Duration of ride- Uber claims to produce results 33% better than google.
                                Predicting bus arrivals correct to 5 mins. Turbulence changes.
                                How to track Traffic?
                                        1. Average speed of BMTC buses fitted with GPS- send their data for time slots betw A to B.
                                                Map Unity- Bang Traffic Information System- 
                                        2. Previous data- year/month,time,etc.
                                        3. Tracking mobile phones' concentration- set of colors for it- reasonable way- for cricket match it fails.
                                Limitations of system- Google maps API disregard bus stops, traffic analysis need improvement.
                Using map DB
                        Address Geocoding
                                Where these addresses are. located 3000 airtel addresses on googlemap in 25 mins.
                                1st recording case of using maps- an epidemic in Europe, plague/cholera- Doctor tracks where they were coming from
                                        sewage pipe had leaked into water pipe due to corrosion.
                                        In BC, Nile maps still available
                        Data Visualization on polygon or networks
                                Wheat distribution on Atlas. 
                                        Dot Dense city maps.
                                        Pie Chart.
                                NO boundary by google maps of city,etc.
                                        cuz detect roads from satellite not boundaries.
                                Identifying and plotting areas with low literacy- eg- south orissa and plan prgm there.
                                Overused ATMs- Bring in data and put on maps.
                                Our cutomer density from pincodes maybe.
                                Represent states- thru polygons with many vertices.
                                Crime density in states
                                Density of buses thru a bus stop. (2.6K in BLR)
                                        Reprensentation- thick line for frequency.

                                Infering from a map is much easier than Database entries.
                        India Income Calculator on Indian map
                        Volunteer Locations- Approach them when blood needed- blood banks empty.
                                Get locations thru mobile No- you approve apps to access you- Ola,uber!
                Image Processing
                        Extract Road changes over time
                                Extractions of road- adding streets, roads, change, extract roads from images.
                        Land Use Delineation
                                Road, park, forest, lake, moutain, valley, home, office, market- for better decisions.
                                Take a look of 5 years trends and conclude sth.

                GIS consists of
                        Softwares
                        Hardware
                                Volumnous data and display- devices should support these.
                        Digital Maps
                                Data- Map data, attribute data.
                                        Map data- all represented thru polygon, dot and line.
                                                how to represent a police station on map- polygon. Also, dams- aerial view.
                                                How to represent it on map of India- a dot- polygon that small.
                                                How to represent river,road on India map- line.
                                                Define a standard basis for the coordinates/ reference.
                                                        Accuracy depends on scale. Acuracy of distance here and Delhi vs here and Gate.
                                                        maps of different scales are made.
                                        Attribute Data- What are attribs of this polygon
                                                Eg- IIITB- educational institute, 1k studs, 100 faculties, open office hrs.
                                                Eg- ATM- ppl/day use, avg money withdrawn, highest money withdrawn.
                                                vary from orgs needs.
                                        MapId for every object on map. FK MapId also for every attribute data tuple.
                                        Combine them and visualize on map with different colors.
                        Related statistical information
                        Rules for decision making

                        Where to open bank? Mobile company? company? 
                        Mobile company regularly pings you so as to route your packets to apt tower.
                Mapping Organizations-
                        Survey of India- Zoological Survey of India, Forests ~, Geological, water.
                Data Gathering organizations- Rainfall data, polution data- we want 7 to take responsibility of data.
                        Eg- Acuweather gets i/f from Indian Metereological dept.
                        Why orgs? data seems static
                                More historical importance of data- seeing trends.
                                some subtle changes, finer measurements, increased technology for more insights.
                                Ground water content changes with time and need to be supervised.
                        Data is updated not transancted
                                Update by not many ppl but authorized.
                                Planned maps of Chandigarh,Amaravati.
                                Which train to run is not transaction. getting tickets is.
                                Courses offered is not transaction, enrollment is. Actors to the system.
                4 stages of GIS
                        Information dissemination
                                Temperature, rainfall, heights- isoheights- equal in term of temperature- same color.
                                Monsoon, cold air this year- really!
                                Pollution of Delhi- Odd/Even.
                        Spatial MIS
                                 Answers query to the system. Leads to decision making. 
                                 NoSql, etc used.
                                 Dashboard- Page with all i/f pertinent to cust- show me the health of org.
                                        structuring data on easy display.
                                 Good day for fishing, weather.
                                 Flipkart uses filters to narrow your search- why email? More to your interests.
                        DSS- Spatial Decision Support System
                                Lottery vs Criteria.
                                Decision means choosing from multiple alternatives by setting criteria.
                                Eg- Why Choose M.Tech? Father said, Cousin here, ALumni, Placement, Profs, BLR city.
                                In MIS query now, criteria later. Here both together.
                                WHere my location? Farther from water body but not Rajasthan.
                                        This is like DB with procedural power.
                        Integrating SDSS with automated Business Process
                                Mess into auditorium- permit from 8 agencies.
                                Airport clearance, fire service (sprinklers on all floor), inspector for lifts,pollution, Municipalty.
                                Height- depends on plot, area, proximity.
                                Standardized process of a system is put into practice as a tool.
                                        Routine steps.
                                Routes for milk collection, routes for flipkart.
                                Bank- take 3 ppl for security means wasting money- many vans,etc.


                #GPS using for locate people lost in woods! thru coordinates.

            </GIS>
            <OLTPSystems>
            OLTP Systems- automates some business processes of a usecase thru computational operations of a system.
                defn - software programs capable of supporting transaction-oriented applications on the Internet
                Eg - order entry, financial transactions, customer relationship management (CRM) and retail sales.
                business processes change the states of a system.
                <DataArchitectureOLTP>
                    <EvolutionInTiers>
                        Tier1 - Monolithic systems
                            access to data was difficult- not hope for data- monolithic system
                            Tools- Mainframe servers, dumb terminals
                            Technology- COBOL, IMS
                        Tier2 - DB Servers (Client and DB Server)
                            DB in separate server- hope to have data (open systems)
                            If RDB, work on data is minimized.
                            Clients fire query using ODBC 
                            Tools- Desktop PCs, Windows GUI, ODBC
                            Technology- Visual Basic.
                            servers accept it. have remained same all throughout.
                        Tier2_5 - DB Servers with PLSQL (Client and Smart DB Server)
                            data logic (PL/SQL) comes on server side.
                            Form query on client end and fire it using ODBC to server (DB Server)
                            Data logic (1/2 layer)
                                Tools- Stored Procedures
                                Technology- PL/SQL
                        Tier3 - ?Presentation Tier (Client, Presentation Tier, DB Tier)
                            Impacts on top- Software side.
                            Data Tier has remained thru the years.
                            Client and Presentation tiers were merged before.
                        Tier4 - ?Business Tier
                            Client Tier- 
                                Tools- Javascripts, AJAX, Browser
                                Technology- Flash, HTML5, Firefox
                            Presentation Tier- 
                                Tools- Web Server
                                Technology- Apache Tomcat
                            Business Tier
                                Tools- Application Servers
                                Technology- IBM Websphere, Oracle 10gAS
                            Data Tier
                                Tools- Unix servers, Relational Databases
                                Technology- HP-UX, Solaris, Oracle / DB2
                    </EvolutionInTiers>
                    <Characteristics>
                        Original Source of data- no derivation (copied/ moved)
                        Fast updates (CRUD)
                        Standard Queries (hardcoded across application for JDBC)
                            Query plan, optimization, execution (standardized=>speed)
                        Don't retain historical data (archive for analysis)- else prescription for disaster.
                            Probs with using table with 1000 rows
                                Difficulty in CRUD- Join query
                                what if table gets corrupt - error message
                                Concurrent Processing- span of lock increases.
                        Remembering Trick - CRUD - Fast Create and update, Retrievals standard, Delete necessary.
                    </Characteristics>
                </DataArchitectureOLTP>
            </OLTPSystems>
            <TemporalDBDemo>
                Postgresql temporal data- v9.6 (must have > 9.2)
                        Reference- http://clarkdave.net/2015/02/historical-records-with-postgresql-and-temporal-tables-and-sql-2011/
                CREATE TABLE subscriptions
                (
                  id SERIAL PRIMARY KEY,
                  state text NOT NULL CHECK (state IN ('trial', 'expired', 'active', 'cancelled')),
                  created_at timestamptz NOT NULL DEFAULT current_timestamp
                );
                ALTER TABLE subscriptions
                        ADD COLUMN sys_period tstzrange NOT NULL DEFAULT tstzrange(current_timestamp, null);
                CREATE TABLE subscriptions_history (LIKE subscriptions);

                /*good idea to lock your history tables down so that they cant be written to except by the trigger.
                CREATE TRIGGER versioning_trigger
                BEFORE INSERT OR UPDATE OR DELETE ON subscriptions
                FOR EACH ROW EXECUTE PROCEDURE versioning(
                  'sys_period', 'subscriptions_history', true
                );*/

                INSERT INTO subscriptions (state, created_at) VALUES ('cancelled', '2015-01-05 12:00:00');
                INSERT INTO subscriptions (state, created_at) VALUES ('active', '2015-01-10 12:00:00');

                INSERT INTO subscriptions_history (id, state, created_at, sys_period)
                  VALUES (1, 'trial', '2015-01-05 12:00:00',
                        tstzrange('2015-01-05 12:00:00', '2015-01-15 15:00:00')
                  );
                INSERT INTO subscriptions_history (id, state, created_at, sys_period)
                  VALUES (1, 'active', '2015-01-05 12:00:00',
                        tstzrange('2015-01-15 15:00:00', (SELECT lower(sys_period) FROM subscriptions WHERE id = 1))
                  );
                INSERT INTO subscriptions_history (id, state, created_at, sys_period)
                  VALUES (2, 'trial', '2015-01-10 12:00:00',
                        tstzrange('2015-01-10 15:00:00', (SELECT lower(sys_period) FROM subscriptions WHERE id = 2))
                  );
                select * from subscription_history;

                id | state  |       created_at       |                         sys_period
                ----+--------+------------------------+------------------------------------------------------------
                  1 | trial  | 2015-01-05 12:00:00+00 | "2015-01-05 12:00:00+00","2015-01-15 15:00:00+00"
                  1 | active | 2015-01-05 12:00:00+00 | "2015-01-15 15:00:00+00","2015-02-19 18:21:22.548028+00"
                  2 | trial  | 2015-01-10 12:00:00+00 | "2015-01-10 15:00:00+00","2015-02-19 18:21:22.992536+00"
                in above, the first sys_period data value is inclusive of the value and 2nd term is exclusive remember in primary school maths (2,3) vs [2,3]
                What state was a particular subscription on X date
                        SELECT * FROM subscriptions AS OF SYSTEM TIME '2014-01-10' WHERE id = 1;	//not supported at least till 9.6
                        SELECT id, state FROM subscriptions
                                        WHERE id = 1 AND sys_period @> '2015-01-10'::timestamptz
                                UNION ALL
                                  SELECT id, state from subscriptions_history
                                        WHERE id = 1 AND sys_period @> '2015-01-10'::timestamptz;
                        //@> is the 'containment operator which will find records whose sys_period contain the given timestamp.
                        //sys_periods have an inclusive lower bound and exclusive upper bound- so datetime on boundary is taken care- most recent is returned.
                        SELECT * from subscriptions_history
                                WHERE id = 1 AND sys_period @> '2015-01-15 15:00:00'::timestamptz;
                Creating and using a view
                        CREATE VIEW subscriptions_with_history AS
                                SELECT * FROM subscriptions
                          UNION ALL
                                SELECT * FROM subscriptions_history;
                        Usage- 
                        SELECT * FROM subscriptions_with_history
                                WHERE id = 1 AND sys_period @> '2015-01-10'::timestamptz
                How many subscriptions were in each state on X date
                        SELECT state, count(*) FROM subscriptions_with_history
                                  WHERE sys_period @> '2015-01-10'::timestamptz
                                  GROUP BY state;
                                Across a date range
                                        WITH dates AS (
                          SELECT *
                          FROM generate_series('2015-01-10'::timestamptz, '2015-01-20', '1 day') date
                        ),
                        trial_subscriptions AS (
                          SELECT * FROM subscriptions_with_history s WHERE s.state = 'trial'
                        ),
                        active_subscriptions AS (
                          SELECT * FROM subscriptions_with_history s WHERE s.state = 'active'
                        ),
                        cancelled_subscriptions AS (
                          SELECT * FROM subscriptions_with_history s WHERE s.state = 'cancelled'
                        )
                        SELECT date,
                          ( SELECT count(*) FROM trial_subscriptions s
                                WHERE s.sys_period @> date ) as trial,
                          ( SELECT count(*) FROM active_subscriptions s
                                WHERE s.sys_period @> date ) as active,
                          ( SELECT count(*) FROM cancelled_subscriptions s
                                WHERE s.sys_period @> date ) as cancelled
                        FROM dates
                        GROUP BY date
                        ORDER BY date;
                        //query works on days- so 2015-01-10 03:00:00 - they wont be included for 2015-01-10, but may be included for 2015-01-11
                        && operator- gives problem of duplicate readings- better stick to @>
                        WITH dates AS (
                                  SELECT start, lead(start, 1, '2015-01-20') OVER (ORDER BY start) AS end
                                  FROM generate_series('2015-01-10'::timestamptz, '2015-01-19', '1 day') start
                                )
                                SELECT dates.start, dates.end, ( SELECT count(*) FROM trial_subscriptions s,
                                  WHERE tstzrange(dates.start, dates.end) & s.sys_period ) as trials
                On what date did a subscription change from state X -> Y
                        //This can be done using a self join, with the join constraint being the upper bound of one record and the lower bound of the next.
                        SELECT upper(s1.sys_period)::date AS date
                                FROM subscriptions_with_history s1, subscriptions_with_history s2
                                WHERE
                                  s1.id = 1 AND s2.id = 1 AND
                                  date_trunc('day', upper(s1.sys_period)::date) =
                                        date_trunc('day', lower(s2.sys_period)::date) AND
                                  s1.state = 'trial' AND s2.state = 'active';
                        //if the same state change happened multiple times, youll get multiple dates back with this query - so if only care about the first or last time it happened, adjust the order and set a limit accordingly.
                Views to return multiple subscriptions
                        CREATE VIEW subscription_conversion_dates AS
                          SELECT s1.id AS subscription_id, upper(s1.sys_period)::date AS date
                          FROM subscriptions_with_history s1, subscriptions_with_history s2
                          WHERE
                                s1.id = s2.id AND
                                date_trunc('day', upper(s1.sys_period)::date) =
                                  date_trunc('day', lower(s2.sys_period)::date) AND
                                s1.state = 'trial' AND s2.state = 'active';
                        This would allow for some elegant queries, such as show me which subscriptions converted in Jan 2015:	
                                SELECT s.id
                                        FROM subscriptions s
                                        INNER JOIN subscription_conversion_dates sd
                                          ON s.id = sd.subscription_id
                                        WHERE date_trunc('month', sd.date) = '2015-01-01';
                                //how many converted- count(*)
                count how many conversions in a month
                        WITH dates AS (
                                SELECT * FROM generate_series('2014-11-01'::date, '2015-02-01', '1 month') month
                                )
                                SELECT month, count(s.*)
                                FROM dates
                                LEFT JOIN subscription_conversion_dates scd
                                  ON month = date_trunc('month', scd.date)
                                LEFT JOIN subscriptions s
                                  ON scd.subscription_id = s.id
                                GROUP BY month
                                ORDER BY month;
                                //can replace month by millenium
                1 way of recording history
                        CREATE TABLE subscriptions_history () INHERITS (subscriptions);
                                You no longer need to worry about keeping both tables in sync. DDL updates to the parent table will be applied automatically to the history table
                                Because both tables are linked, new columns with default values will be propagated to the history table
                                Problems- queries also apply on children- like updation would be catastrophic
                                        sql_inheritance = false- deprecated
                                        SELECT * FROM ONLY ... or UPDATE ONLY ...
                        Creating triggers
                <UncatNotesFromYoutube>
                    important to not just capture new information but full history of it
                    name and address- when moved and start and end date
                            where i lived in 2005
                            new pricing starts at midnight- solution 1 run the script at 12pm vs have an entry inside db for that cuz queries are time conscious.\
                            recall slowly changing dimensions in DWH.
                            Eg- star schema with dimension tables for when, why, when, whose
                                    find sales in the month of january- we need to refer price on the dates on which items were sold.
                                    identify active rows by using magic dates or find null in ToDate.
                                    we could have written query with lots of inequalities and have scope of mistakes- so make DB time conscious- smaller queries.
                                    Transaction time- what time I was informed of a change- eg- change of address from jan to mar informed on 5th feb.

                    Sound
                            Check control panel- hardware- sound- if headphone can detect anything.
                            check files on HDD, across browsers/sites to pinpoint problem
                    Need
                            financial changes over the past
                            checking medical history of a person in lawsuit
                            who all took car on rent during their travel to city.
                            not more than 1 discount should be given.
                            OLTP
                                    previous sal of employee
                                    his salary on 2nd feb
                                    his average salary last year
                                    budgest of dept over past 5 years
                                    how budget changed over years.
                            Temporal data- data anchored with time
                                    long data (not big)- data with massive historical sweep.
                                    it's ubiquitous- everywhere- all data is qualified with a time instant/period.
                                            Eg- RDF Stores, Medical records, transport data.
                                            essential for analysis- predict- plan- accountability.
                                    measure time by clock ticking- different granularities as per needs.
                                            dates/time can very well map to integers.
                                            Representation
                                                    Time points- (t1,e1)- at t1, e1 happened.
                                                    Intervals- ((t2,t4),e)- betw t2 and t4 event was taking place.
                                                    temporal elements
                                    Types of data
                                            discrete- ladder style- eg- salary
                                            points- Eg- sales happened
                                            continuous- eg- voice frequency
                                    Imagine a relational table going deeper into the sheet with its history for the entities.
                                            there is a 3-D to it.
                                    Mistakes can happen in updating the values wrt time- wef jan his sal is 17K not 15K.
                                    Types of time in DB	
                                            Valid time/ application time/ business time- tenure of validity of data
                                            transaction time/ system time- instant when data was captured
                                            user defined time- time as interpreted by user.
                                            others
                                    Types of Temporal DB
                                            Snapshot DB- no time dimension- snapshot of how data looks at some point in time.
                                            Valid Time/ historical DB- past snapshots also remembered.
                                            Transaction Time DB- rollback DB, immutable.
                                            BiTemporal DB- valid and transaction time support.
                                    Query types
                                            snapshot query- no ref to time
                                            temporal- evaluated at different times
                                            Time Travel- go back to previous time instant.
                                    Temporal integrity constraints
                                            but b4- what's data model say on RDBMS
                                            where do you add time- 
                                                    tuples or attributes.
                                                    attach timestamps/range to data in particular not row.
                                                    attach a set of history of data as a column.
                                                    separate the temporal part to referenced table from the relation.
                                            SQL 2011- create table emp 
                                            (emp_no integer,
                                            start DATE, 
                                            end DATE,
                                            dept integer,
                                            period for Eperiod (start, end)
                                            )
                                            ALTER TABLE Emp ADD
                                            (Eno, Eperiod WITHOUT OVERLAPS)
                                            constructs- as of, from..to.., between ..and ..


                    You can open notepad++ from run command by typing- notepad++
                            eclipse- just paste text on a package and you get a java class copied to namespace.

                    SELECT * FROM customers AS OF SYSTEM TIME '2015-01-22 15:45:00' WHERE id = 3 
                    SELECT * FROM customers AS OF SYSTEM TIME current_timestamp ;
                    SELECT * FROM customers AS OF SYSTEM TIME current_date - 3 days ;
                    SELECT * FROM customers AS OF SYSTEM TIME current_timestamp - 1 min ;
                    SELECT * FROM customers AS OF SYSTEM TIME TIMESTAMP '2015-01-22' ;
                    SELECT * FROM customers AS OF SYSTEM TIME :hv ;
                    DB2: ... FROM (table) FOR BUSINESS_TIME AS OF (timestamp)
                    Oracle: ... FROM (table) AS OF PERIOD FOR (business_time) (timestamp)

                </UncatNotesFromYoutube>
            </TemporalDBDemo>
            <StreamDBPipeline>
                pipelinedb- referencer docs pipelinedb- working
                sudo dpkg -i pipelinedb-(version).deb
                This will install PipelineDB at /usr/lib/pipelinedb.
                Initilize- pipeline-init -D (data_directory)
                running- pipeline-ctl -D (data_directory) -l pipelinedb.log start 
                        or pipelinedb -D (data_directory)
                        pipeline-ctl -D (data_directory) stop
                client shell
                        pipeline pipeline
                        psql -p 5432 -h localhost pipeline
                configuration
                        allow incoming connections from remote hosts- 
                                pipelinedb.conf- listen_addresses = '*'
                                pg_hba.conf- add following line to allow incoming connections
                                        host    all             all             (ip address)/(subnet)            md5
                CRUD
                        continuous view is much like a regular view, except that it selects from a combination of streams and tables as its inputs and is incrementally updated in realtime as new data is written to those inputs.
                        As soon as a stream row has been read by the continuous views that must read it, it is discarded. It is not stored anywhere.
                        create stream ~ tables
                                CREATE STREAM stream_name ( [
                                        { column_name data_type [ COLLATE collation ] | LIKE parent_stream } [, ... ]
                                ] )
                                Eg- CREATE STREAM astream (x integer, y integer);
                                        CREATE CONTINUOUS VIEW v AS SELECT sum(x + y) FROM stream;
                                        INSERT INTO stream (x, y, z) VALUES (0, 1, 2);
                                                INSERT INTO json_stream (payload) VALUES (
                                                  '{"key": "value", "arr": [92, 12, 100, 200], "obj": { "nested": "value" } }'
                                                );
                                        BATCHED INSERT-
                                                INSERT INTO stream (x, y, z) VALUES (0, 1, 2), (3, 4, 5), (6, 7, 8)
                                                        (9, 10, 11), (12, 13, 14), (15, 16, 17), (18, 19, 20), (21, 22, 23), (24, 25, 26);
                                                INSERT INTO geo_stream (id, coords) VALUES (42, ST_MakePoint(-72.09, 41.40));
                                                INSERT INTO ss_stream (x) SELECT generate_series(1, 10) AS x;
                                                INSERT INTO tab_stream (x) SELECT x FROM some_table;
                                                PREPARE write_to_stream AS INSERT INTO stream (x, y, z) VALUES ($1, $2, $3);
                                                Eg- 
                                                        EXECUTE write_to_stream(0, 1, 2);
                                                        EXECUTE write_to_stream(3, 4, 5);
                                                        EXECUTE write_to_stream(6, 7, 8);
                                                File to STREAM
                                                        COPY stream (data) FROM '/some/file.csv'
                                        old and new
                                                CREATE CONTINUOUS VIEW v_deltas AS SELECT abs((new).sum - (old).sum) AS delta
                                                          FROM output_of('v_sum')
                                                          WHERE abs((new).sum - (old).sum) > 10;
                                ALTER STREAM stream ADD COLUMN x integer;
                        only persistence is of SELECT * FROM view
                        CREATE CONTINUOUS VIEW name AS query
                                Query can include windowing as follows:
                                        [ existing_window_name ]
                                        [ PARTITION BY expression [, ...] ]
                                        [ ORDER BY expression ] [ NULLS { FIRST | LAST } ] [, ...] ]
                                        [ frame_clause ]
                                CREATE CONTINUOUS VIEW imps AS
                                        SELECT COUNT(*) FROM imps_stream
                                        WHERE (arrival_timestamp > clock_timestamp() - interval '5 minutes');
                                CREATE CONTINUOUS VIEW avg_of_forever AS SELECT AVG(x) FROM one_trillion_events_stream
                                linear regression of stream bucketed by minutes
                                        CREATE CONTINUOUS VIEW lreg AS
                                                SELECT date_trunc('minute', arrival_timestamp) AS minute,
                                                  regr_slope(y, x) AS mx,
                                                  regr_intercept(y, x) AS b
                                                FROM datapoints_stream GROUP BY minute;
                        Row expiration- using ttl
                                CREATE CONTINUOUS VIEW v_ttl WITH (ttl = '1 month', ttl_column = 'minute') AS
                                        SELECT minute(arrival_timestamp), COUNT(*) FROM some_stream GROUP BY minute;
                                modify it- set_ttl ( cv_name, ttl, ttl_column )
                        DROP CONTINUOUS VIEW name
                        TRUNCATE CONTINUOUS VIEW name
                        SELECT * FROM pipeline_views();
                        any select statement is valid on views
                                SELECT t.name, sum(v.value) + sum(t.table_value) AS total
                                        FROM some_continuous_view v JOIN some_table t ON v.id = t.id GROUP BY t.name
                        Activation
                                start and stop  continuously processing input streams without stopping pipeline service
                                ACTIVATE | DEACTIVATE [ continuous view or transform name ]
                        Transform data without storing
                                CREATE CONTINUOUS TRANSFORM name AS query [ THEN EXECUTE PROCEDURE function_name ( arguments ) ]
                                CREATE CONTINUOUS TRANSFORM t AS
                                        SELECT t.y FROM some_stream s JOIN some_table t ON s.x = t.x;
                                        CREATE CONTINUOUS VIEW v AS
                                                SELECT sum(y) FROM output_of('t');
                        CREATE CONTINUOUS VIEW recent_users WITH (sw = '1 minute') AS
                                SELECT user_id::integer FROM stream;
                        It is compatible with postgres- so we use postgres jdbc only to connect to pipeline db (refer to the java code)- but fire pipeline queries only.
            </StreamDBPipeline>
        </Usecases>
        <Misc>Blockchain</Misc>
    </DBMS>
	</Persistence>
	<Utils>
		<GIS></GIS>
		<ComputerVision_DIP>OpenCV</ComputerVision_DIP>
		<Multimedia></Multimedia>
	</Utils>
	<AsyncEfforts>
		<ML>
			<Intro></Intro>
			<DeepLearning></DeepLearning>
		</ML>
	</AsyncEfforts>
    <Misc>Paint (Painter JAVA API), editors (word, text, pdf), compiler, IDE, Hadoop, etc.</Misc>
</UtilsForAppDev>