<?xml version="1.0" encoding="UTF-8"?>
<CSLearningOnTheGo>
<Pandas>
df["a"] = pd.to_numeric(df["a"])
df.loc[0, 'new'] = df.iloc[:N, df.columns.get_loc('col2')].mean()
from scipy.stats import norm
# generate random numbers from N(0,1)
import seaborn as sns
data_normal = norm.rvs(size=10000,loc=0,scale=1)
ax = sns.distplot(data_normal,
                  bins=100,
                  kde=True,
                  color='skyblue',
                  hist_kws={"linewidth": 15,'alpha':1})
ax.set(xlabel='Normal Distribution', ylabel='Frequency')
ax.set_xlim(50, 70);
===
import pandas
from collections import Counter
a = ['a', 'a', 'a', 'a', 'b', 'b', 'c', 'c', 'c', 'd', 'e', 'e', 'e', 'e', 'e']
letter_counts = Counter(a)
df = pandas.DataFrame.from_dict(letter_counts, orient='index')
df.plot(kind='bar')
===
Numpy Arrays - print(precip_2002_2013[0:1, 0:2])
==
df = df.drop_duplicates()
    drop_duplicates(subset = ["b"])
===
df.loc[:, 'C':'E']
    df.loc[:, ['C', 'D', 'E']]
df[['C', 'D', 'E']]
dataframe.iloc[:,[1,2]]
===
df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',
                              'Parrot', 'Parrot'],
                   'Max Speed': [380., 370., 24., 26.]})
===
df.loc[df['column_name'] == some_value]
To select rows whose column value is in an iterable, some_values, use isin:

df.loc[df['column_name'].isin(some_values)]
Combine multiple conditions with &:

df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]
===
df.filter(df("state") === "OH" && df("gender") === "M")
    .show(false)

df.filter("gender == 'M'").show(false)
df.where("gender == 'M'").show(false)
df.filter($state === "OH").show(false)
df.filter(col("state") === "OH").show(false)
df.where(df("state") === "OH").show(false)
===

df.dropna(subset = ['column1_name', 'column2_name', 'column3_name'])

trip_data["rate_code"] = pd.to_numeric(trip_data["rate_code"])
trip_data[['trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']] = trip_data[['trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].astype(float)

trip_data.columns = trip_data.columns.str.replace(' ', '')
df['New_Sample'] = df.Sample.str[:1]
df['New_Sample'] = df.Sample.apply(lambda x: x[:1])
===
df['Value'].round(decimals=3)
np.round(df['DataFrame column'], decimals=number of decimal places needed)
 df[list("ABCD")] = df[list("ABCD")].fillna(0.0).astype(int)
>>> df[list("ABCD")] = df[list("ABCD")].astype(int)
df = df.astype({'col1': 'int', 'col2': 'int', 'col3': 'int'})
df.nlargest(3, 'a')
df['sum_values_A'] = df.groupby('A')['values'].transform(np.sum)
===
No column space in names
from pyspark.sql import functions as F
renamed_df = df.select([F.col(col).alias(col.replace(' ', '_')) for col in df.columns])
===
idea given a problem make a pipeline of what all we need to do to get to end result - like column name change, data nan removal, data frame creation, table creation, join, etc.
---
df['col'] = df['col'].str[:9]
Or str.slice:

df['col'] = df['col'].str.slice(0, 9)
df
    concat - union. append - any times.
pd.datetime.now()

pd.Timestamp('2017-03-01')
print pd.Timestamp(1587687255,unit='s')
print pd.date_range("11:00", "13:30", freq="30min").time
====
trip_merged["date"] = trip_merged['pickup_datetime'].str[0:10].astype(str)
trip_merged["day"] = pd.to_datetime(trip_merged["date"]).dt.day_name() #Or dayofweek
trip_merged["day"] #could be number as well but categorical variable so let it be.
===
trip_merged["pickup_time"] = trip_merged['pickup_datetime'].str[11:13].astype(str) \
    + "." \
    + (trip_merged['pickup_datetime'].str[14:16].astype(int)/60*100).astype(int).apply(lambda x: "0" + str(x) if len(str(x)) == 1 else str(x))
trip_merged["pickup_time"] = pd.to_numeric(trip_merged["pickup_time"])
trip_merged["pickup_time"]
======
df = df.apply(lambda x: np.square(x) if x.name == 'd' else x, axis=1)
</Pandas>
<Spark>
UI Facade - http://localhost:4040/
Kibana Logs - another.
createDataFrame(df, schema)
Schema or metadata or descriptive
    printSchema
    columns
    describe().show()
        describe('uniform', 'normal').show()
Read
    spark.read.option("header", "true").option("inferSchema", "true").csv("trip_data_4.csv")
		.schema(flightSchemaStruct)
		//where
			flightSchemaStruct = StructType([
				StructField("FL_DATE", DateType()),
				StructField("OP_CARRIER", StringType()),
				StructField("OP_CARRIER_FL_NUM", IntegerType()),
				StructField("ORIGIN", StringType())
			])
show, head,
    limit(10) or take(10) or head(10) or show(10)
	join_df.collect()
Write
    dataFrame.write.format("com.databricks.spark.csv").save("myFile.csv")
    xyz.repartition(1).write.format("com.databricks.spark.csv").option("header","true").save("trip_merged_4.csv")
		mode("overwrite"), option("path", path), partitionBy, option(maxRecordsPerFile, max)
Clean
    df.na.drop("all").show(false)
	.dropDuplicates(["name", "dob"])
Manipulate
	filter("Age = 40")
    groupBy("C1")
        groupBy("C1").count()
	select([mean('uniform'), min('uniform'), max('uniform')])
    Add column
        abc2=abc1.withColumn('pick up_hour', substring('` pickup_datetime`', 12,2))
		file_df.select(regexp_extract('value', log_reg, 1).alias('ip'),
                             regexp_extract('value', log_reg, 4).alias('date'))
		drop("day", "month")
    Using functions
        abc2=abc1.withColumn('pickup_hour', substring('` pickup_datetime`', 12,2))
        df.withColumn("dayofweek", date_format(to_date($"date", "MM/dd/yyyy"), "EEEE"))
        select date_format(my_timestamp, 'EEEE') from
        date_format(from_unixtime(created_utc), 'EEEE') from testTable
	Using UDF
		def to_date_df(df, fmt, fld):
    		return df.withColumn(fld, to_date(fld, fmt))
		new_df = to_date_df(my_df, "M/d/y", "EventDate")
		OR
		def parse_gender(gender):
			female_pattern = r"^f$|f.m|w.m"
			male_pattern = r"^m$|ma|m.l"
			if re.search(female_pattern, gender.lower()):
				return "Female"
			elif re.search(male_pattern, gender.lower()):
				return "Male"
			else:
				return "Unknown"
		parse_gender_udf = udf(parse_gender, returnType=StringType())
		survey_df2 = survey_df.withColumn("Gender", parse_gender_udf("Gender"))
		OR
		spark.udf.register("parse_gender_udf", parse_gender, StringType())
		survey_df3 = survey_df.withColumn("Gender", expr("parse_gender_udf(Gender)"))
	aggregate
		Mean, sum, count, Max, min, Variance, Stdev, sort.
	Group by and multiple aggregates
		summary_df = invoice_df \
        .groupBy("Country", "InvoiceNo") \
        .agg(f.sum("Quantity").alias("TotalQuantity"),
             f.round(f.sum(f.expr("Quantity * UnitPrice")), 2).alias("InvoiceValue"),
             f.expr("round(sum(Quantity * UnitPrice),2) as InvoiceValueExpr")
             )
	JOIN
		join_expr = order_df.prod_id == product_df.prod_id
		product_renamed_df = product_df.withColumnRenamed("qty", "reorder_qty")
		order2_df = order_df.join(product_renamed_df, join_expr, "inner")
			.drop(product_renamed_df.prod_id)
		order_df.join(product_renamed_df, join_expr, "left")
	Running Window
		running_total_window = Window.partitionBy("Country") \
			.orderBy("WeekNumber") \
			.rowsBetween(-2, Window.currentRow)
		summary_df.withColumn("RunningTotal",
                          f.sum("InvoiceValue").over(running_total_window)) \
        				.show()
	Ranking with partition
		rank_window = Window.partitionBy("Country") \
			.orderBy(f.col("InvoiceValue").desc()) \
			.rowsBetween(Window.unboundedPreceding, Window.currentRow)
		df = summary_df.withColumn("Rank", f.dense_rank().over(rank_window)) \
			.where(f.col("Rank") == 1) \
			.sort("Country", "WeekNumber") \
			.show()
RDD
	linesRDD = sc.textFile(sys.argv[1])
	linesRDD.repartition(2)
	colsRDD = partitionedRDD.map(lambda line: line.replace('"', '').split(","))
    selectRDD = colsRDD.map(lambda cols: SurveyRecord(int(cols[1]), cols[2], cols[3], cols[4]))
		//SurveyRecord = namedtuple("SurveyRecord", ["Age", "Gender", "Country", "State"])
    filteredRDD = selectRDD.filter(lambda r: r.Age = 40)
    kvRDD = filteredRDD.map(lambda r: (r.Country, 1))
    countRDD = kvRDD.reduceByKey(lambda v1, v2: v1 + v2)
	colsList = countRDD.collect()
transformations
	map, filter, flatMap,
	mapPartitions, sample
	union, intersection, distinct, groupByKey,
	reduceByKey, aggregateByKey, sortByKey
	join, cartesian, pipe, coalesce, repartition
Actions
    count()
	reduce, collect, first, take, takeSample
	countByKey, foreach
Powerful
	cache(), unpersist
	val broadcastVar = sc.broadcast(Array(1, 2, 3))
		should be used instead of the value v; v is not shipped to the nodes more than once
		object v should not be modified after its broadcast
		to ensure that all nodes get the same value of the broadcast variable.
	val accum = sc.accumulator(0)
	sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
SQL
    createOrReplaceTempView("test")
    re=spark.sql("select max_seq from test")
    SELECT user, stddev(duration) FROM df GROUP BY user
Pandas
    pandasDF=sparkDF.toPandas()
Eg
    matchesDf
    .join(playersDf, $"matches.player1" === $"players.player")
    .select($"matches.matchId" as "matchId", $"matches.player1" as "player1", $"matches.player2" as "player2", $"players.birthYear" as "player1BirthYear")
    .join(playersDf, $"player2" === $"players.player")
    .select($"matchId" as "MatchID", $"player1" as "Player1", $"player2" as "Player2", $"player1BirthYear" as "BYear_P1", $"players.birthYear" as "BYear_P2")
    .withColumn("Diff", abs('BYear_P2.minus('BYear_P1)))
    .show()
Eg - Wordcount
	val input_file = sc.textFile("input.txt")
	val counts = input_file.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
		idea - for each line, get array of string - for every string in the array, create a tuple (word,1) - we gotta group keys and aggregate the sum / count.
	counts.saveAsTextFile("output")
Deployment
	Compile the code with dependencies on classpath
		scalac -classpath "spark-core_2.10-1.3.0.jar:/usr/local/spark/lib/spark-assembly-1.4.0-hadoop2.6.0.jar" SparkPi.scala
	Create Wordcount Jar along with dependencies
		jar -cvf wordcount.jar SparkWordCount*.class spark-core_2.10-1.3.0.jar /usr/local/spark/lib/spark-assembly-1.4.0-hadoop2.6.0.jar
	Submit Jar
		spark-submit --class SparkWordCount --master local wordcount.jar
</Spark>
<Hive>
	ALTER TABLE employee PARTITION (year=’1203’) RENAME TO PARTITION (Yoj=’1203’);
	supported - && and AND both.
	A[n], M[key], S.x
	SELECT round(2.6) from temp;
	CREATE VIEW emp_30000 AS (query)
		drop view emp_30000
	CREATE INDEX index_salary ON TABLE employee(salary) AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';
		drop index index_salary ON employee
	Extra - [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]]
	Rownum, latest, etc.
		Get latest records
			select count(*) from test_table where to_Date(datetime) in (select max(to_Date(datetime)) from test_table);
	Windowing
		SELECT t.id
			,t.name
			,t.age
			,t.modified
		FROM (
			SELECT id
				,name
				,age
				,modified
				,ROW_NUMBER() OVER (
					PARTITION BY id ORDER BY unix_timestamp(modified,'yyyy-MM-dd hh:mm:ss') DESC
					) AS ROW_NUMBER
			FROM test
			) t
		WHERE t.ROW_NUMBER <= 1;
		//similarly for top n.
	table_reference JOIN table_factor [join_condition]
	   | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference
	   join_condition
	   | table_reference LEFT SEMI JOIN table_reference join_condition
	   | table_reference CROSS JOIN table_reference [join_condition]

</Hive>
<SKLearn>
sklearn
====
from sklearn.preprocessing import PolynomialFeatures
model_name = 'Poly'
polynomial_features = PolynomialFeatures(degree=2)
plRegressor = LinearRegression()
plr_model = Pipeline(steps=[('polyFeature', polynomial_features),('regressor', plRegressor)])
plr_model.fit(X_train,y_train)
y_pred_plr = plr_model.predict(X_test)
plrMetrics = model_metrics(plRegressor, y_test, y_pred_plr)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                    random_state=1)
reg = linear_model.LinearRegression()
reg.fit(X_train, y_train)
# variance score: 1 means perfect prediction
print('Variance score: {}'.format(reg.score(X_test, y_test)))

import matplotlib.pyplot as plt
## setting plot style
plt.style.use('fivethirtyeight')

## plotting residual errors in training data
plt.scatter(reg.predict(X_train), reg.predict(X_train) - y_train,
            color = "green", s = 10, label = 'Train data')

## plotting residual errors in test data
plt.scatter(reg.predict(X_test), reg.predict(X_test) - y_test,
            color = "blue", s = 10, label = 'Test data')

## plotting line for zero residual error

plt.hlines(y = 0, xmin = 0, xmax = 3000, linewidth = 2)

## plotting legend
plt.legend(loc = 'upper right')

## plot title
plt.title("Residual errors")

## method call for showing the plot
plt.show()

from sklearn.tree import DecisionTreeRegressor
model_name = "Decision Tree Regressor"
decisionTreeRegressor = DecisionTreeRegressor(random_state=0,max_features=30)
dtr_model = Pipeline(steps=[('preprocessorAll',preprocessorForAllColumns),('regressor', decisionTreeRegressor)])
dtr_model.fit(X_train, y_train)
y_pred_dtr = dtr_model.predict(X_test)
print(decisionTreeRegressor)
<Misc>Internally uses sciPy which we can use if we want to perform tracing and using maths with low level APIs - recall Octave programming</Misc>
</SKLearn>
<Pytorch>
	Google: python pytorch regression
		https://www.analyticsvidhya.com/blog/2020/10/perform-regression-analysis-with-pytorch-seamlessly/
		https://www.guru99.com/pytorch-tutorial.html
		https://www.kaggle.com/aakashns/pytorch-basics-linear-regression-from-scratch
		https://medium.com/@benjamin.phillips22/simple-regression-with-neural-networks-in-pytorch-313f06910379
</Pytorch>
<TensorFlow>
	Google: Python Tensorflow regression
		https://www.geeksforgeeks.org/linear-regression-using-tensorflow/
</TensorFlow>
<Keras>
	Google: Python keras regression
</Keras>
</CSLearningOnTheGo>